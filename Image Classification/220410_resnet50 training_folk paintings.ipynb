{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet50_folk_paintings_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sooo19/A-study-of-the-data-generation-model-of-the-Folk-Painting-source-data/blob/main/resnet50_folk_paintings_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## albumentation을 라이브러리를 이용해 dataloader를 정의한 Resnet50 model training\n",
        "- Tiger, Flower, Letter 분류\n"
      ],
      "metadata": {
        "id": "lpEOgdy5Ewbw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5SyFDlRs8ze"
      },
      "source": [
        "# 이미지 데이터 파이토치 커스터마이징하기"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDo2MaUPRqz4",
        "outputId": "b6a3097c-4ad0-4d02-a839-f3fabcdea591"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th61J-4wtknJ"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms # 이미지 데이터 transform\n",
        "from torch.utils.data import DataLoader # 이미지 데이터 로더\n",
        "import os\n",
        "import glob"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0An2e3Nrq8H"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "  torch.cuda.manual_seed_all(777)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I77GUH6Qrq3T"
      },
      "source": [
        "# 경로 지정\n",
        "\n",
        "Dataset_path = '/content/drive/MyDrive/민화 AI인턴/민화 AI인턴 (2)/220403/ImageData'\n",
        "\n",
        "# train_path = '/content/drive/MyDrive/민화 AI인턴/민화 AI인턴 (2)/220403/ImageData/train'\n",
        "# test_path = '/content/drive/MyDrive/민화 AI인턴/민화 AI인턴 (2)/220403/ImageData/test'\n",
        "\n",
        "# tiger_dir = '/tiger/'\n",
        "# flower_dir = '/flower/'\n",
        "# letter_dir = '/letter/'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN6uBbo5Iqlm"
      },
      "source": [
        "# 이미지 데이터 전처리   \n",
        "> 파이토치 라이브러리를 이용한 쉬운 방법 or albumentation 라이브러리를 이용한 데이터 커스터마이징 2가지 방식으로 진행합니다.\n",
        "\n",
        " 1. 이미지를 resize합니다.   \n",
        "    \n",
        " 2. resize한 이미지에 대해서 스케일링을 준비합니다.   \n",
        "    \n",
        " 3. 전처리한 이미지들을 augmentation해줍니다.   \n",
        "   \n",
        " 4. augmentation transform을 진행했으면, data loader로 학습을 준비합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# albumentation을 라이브러리를 이용한 dataloader 정의"
      ],
      "metadata": {
        "id": "qmzXzjOpzkFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader # 데이터 커스터마이징\n",
        "from PIL import Image # PIL = Python Image Library\n",
        "import cv2 # albumentation transform을 쓰려면 꼭 이 라이브러리를 이용\n",
        "import tensorflow as tf\n",
        "\n",
        "class inhovation_Dataset(Dataset):\n",
        "\n",
        "  def __init__(self, file_path, mode, transform=None):\n",
        "    self.all_data = sorted(glob.glob(os.path.join(file_path, mode, '*', '*')))\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    if torch.is_tensor(index):        # 인덱스가 tensor 형태일 수 있으니 리스트 형태로 바꿔준다.\n",
        "       index = index.tolist()\n",
        "\n",
        "    data_path = self.all_data[index]\n",
        "    #img = np.array(Image.open(data_path).convert(\"RGB\")) # albumenatation transform을 쓰려면 cv2 라이브러리로 이미지를 읽어야 함\n",
        "    image=cv2.imread(data_path)\n",
        "    image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # BGR -> RGB 변환\n",
        "\n",
        "    # transform 적용\n",
        "    if self.transform is not None:    \n",
        "       augmented = self.transform(image=image)\n",
        "       image = augmented['image'] \n",
        "\n",
        "    # 이미지 이름을 활용해 label 부여\n",
        "    label=[]                                \n",
        "    if os.path.basename(data_path).startswith(\"tiger\") == True:\n",
        "        label = 0\n",
        "    elif os.path.basename(data_path).startswith(\"flower\") == True: \n",
        "        label = 1\n",
        "    else :\n",
        "        label = 2\n",
        "    return image, label\n",
        "\n",
        "  def __len__(self):\n",
        "    length = len(self.all_data)\n",
        "    return length"
      ],
      "metadata": {
        "id": "d9Hh2rjLybTh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations\n",
        "import albumentations.pytorch\n",
        "from torchvision import transforms # 이미지 데이터 transform\n",
        "from torch.utils.data import DataLoader # 이미지 데이터 로더\n",
        "\n",
        "batch_size = 16\n",
        "albumentations_train = albumentations.Compose([\n",
        "                                                \n",
        "    albumentations.Resize(224, 224),   \n",
        "    # albumentations.Resize(512, 512),   # 이미지 size를 512*512로 resize하려고 하였더니, 메모리 초과 사용 문제 발생\n",
        "    albumentations.OneOf([\n",
        "                          albumentations.HorizontalFlip(p=0.8), # p확률로 이미지 좌우 반전\n",
        "                          albumentations.RandomRotate90(p=0.8), # p확률로 90도 회전\n",
        "                          albumentations.VerticalFlip(p=0.8) # p확률로 이미지 상하 반전\n",
        "    ], p=1),\n",
        "\n",
        "    albumentations.OneOf([\n",
        "                          albumentations.MotionBlur(p=0.8), # p확률로 이미지를 흐리게(?) 만들어 줌\n",
        "                          albumentations.OpticalDistortion(p=0.8), # p확률로 이미지 왜곡\n",
        "                          albumentations.GaussNoise(p=0.8) # 임의의 noise를 삽입          \n",
        "    ], p=1),\n",
        "    # albumentations.Normalize(mean = resize_train_mean, std = resize_train_std),\n",
        "    albumentations.pytorch.ToTensor()\n",
        "    \n",
        "])\n",
        "\n",
        "albumentations_test = albumentations.Compose([\n",
        "                                                \n",
        "    albumentations.Resize(224, 224),\n",
        "    # albumentations.Resize(512, 512),   \n",
        "    albumentations.pytorch.ToTensor()\n",
        "    \n",
        "])\n",
        "\n",
        "\n",
        "trainset=inhovation_Dataset(Dataset_path, 'train', transform=albumentations_train)\n",
        "testset=inhovation_Dataset(Dataset_path, 'test', transform=albumentations_test)\n",
        "\n",
        "albumentations_train_loader = torch.utils.data.DataLoader(trainset, batch_size=16,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "\n",
        "albumentations_test_loader = torch.utils.data.DataLoader(testset, batch_size=16,\n",
        "                                         shuffle=False, num_workers=0)\n",
        "\n",
        "classes = ('tiger', 'flower', 'letter')   # class명 추가"
      ],
      "metadata": {
        "id": "ah9G1jm2zf2R"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 메모리 초과 사용 에러 : CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 11.17 GiB total capacity; 10.49 GiB already allocated; 1.81 MiB free; 10.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
        "\n",
        "* 따라서 이미지 사이즈를 224*224로 resize"
      ],
      "metadata": {
        "id": "VuLKRsbAVBN-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTc559LqKV6z"
      },
      "source": [
        "## 파이토치 라이브러리를 이용해 쉽게 진행\n",
        "1. 이미지 resize & 스케일링\n",
        "> 꼭 resize를 진행한 이미지들을 스케일링 해줍니다.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FItktxnhh30j"
      },
      "source": [
        "# resize_trans = transforms.Compose([\n",
        "#                                    transforms.Resize((128,128)),\n",
        "#                                    transforms.ToTensor()\n",
        "# ])\n",
        "\n",
        "# resize_train = torchvision.datasets.ImageFolder(root=train_path, transform=resize_trans)\n",
        "# resize_test = torchvision.datasets.ImageFolder(root=test_path, transform=resize_trans)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u48iThG_oD7q"
      },
      "source": [
        "# resize_train[0][0].shape # ToTensor를 진행했기 때문에 데이터가 torch(C,H,W) 형태로 바뀜"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXASAt6-9hXv"
      },
      "source": [
        "# resize_train[0][0]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMiOxJFtn0OX"
      },
      "source": [
        "# import numpy as np\n",
        "# np.mean(resize_train[0][0].numpy(),axis=(1,2)) # numpy로 바꾸고, axis = 1,2 mean으로 RGB mean/std 뽑기"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGYovru-h3w-"
      },
      "source": [
        "# def get_mean_std(dataset):\n",
        "#   meanRGB = [np.mean(image.numpy(), axis=(1,2)) for image,_ in dataset]\n",
        "#   stdRGB = [np.std(image.numpy(), axis=(1,2)) for image,_ in dataset]\n",
        "\n",
        "#   meanR = np.mean([m[0] for m in meanRGB])\n",
        "#   meanG = np.mean([m[1] for m in meanRGB])\n",
        "#   meanB = np.mean([m[2] for m in meanRGB])\n",
        "\n",
        "#   stdR = np.mean([s[0] for s in stdRGB])\n",
        "#   stdG = np.mean([s[1] for s in stdRGB])\n",
        "#   stdB = np.mean([s[2] for s in stdRGB])\n",
        "\n",
        "#   print(meanR, meanG, meanB)\n",
        "#   print(stdR, stdG, stdB)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRpbrXwmDU3A"
      },
      "source": [
        "# get_mean_std(resize_train)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzRTUme7h3uN"
      },
      "source": [
        "# get_mean_std(resize_test)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqJqtWrZC1lZ"
      },
      "source": [
        "# # normalization 준비\n",
        "\n",
        "# resize_train_mean=[0.58988106, 0.5013303, 0.39416644]\n",
        "# resize_train_std=[0.18581268, 0.16816284, 0.14526728]\n",
        "\n",
        "# resize_test_mean=[0.5859482, 0.49771816, 0.38617086]\n",
        "# resize_test_std=[0.1768823, 0.16044693, 0.13925213]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji1tlhr1g5wL"
      },
      "source": [
        "3. transform을 이용해 data augmentation하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnQd3WypHkF0"
      },
      "source": [
        "# transform_train = transforms.Compose([\n",
        "#     transforms.Resize((128, 128)), # 이미지 resize\n",
        "#     transforms.RandomCrop(124), # 이미지를 랜덤으로 크롭\n",
        "#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2), # 이미지 지터링(밝기, 대조, 채비, 색조)\n",
        "#     transforms.RandomHorizontalFlip(p = 1), # p확률로 이미지 좌우반전\n",
        "#     transforms.RandomVerticalFlip(p = 1), # p확률로 상하반전\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(resize_train_mean, resize_train_std)\n",
        "# ])\n",
        "\n",
        "# transform_test = transforms.Compose([\n",
        "#     transforms.Resize((128, 128)), \n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(resize_test_mean, resize_test_std)\n",
        "# ])\n",
        "\n",
        "# trainset = torchvision.datasets.ImageFolder(root=train_path, transform=transform_train)\n",
        "# testset = torchvision.datasets.ImageFolder(root=test_path, transform=transform_test)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RrWjRB6dJ1z"
      },
      "source": [
        "augmentation을 적용한 이미지 확인하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MoU7WDZbEAw"
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# %matplotlib inline\n",
        "\n",
        "# def imshow(img, mean, std):\n",
        "#     npimg = img.numpy()\n",
        "#     img = np.transpose( npimg, (1, 2, 0) )\n",
        "#     img = img * std + mean # renormalize\n",
        "#     img = img.clip(0, 1)\n",
        "#     plt.imshow(img)\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "# none_trans = torchvision.datasets.ImageFolder(root=train_path, transform=None)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCzW6PvFcuPI"
      },
      "source": [
        "# # 원본 이미지\n",
        "# none_trans[300][0]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-f4tWSLTWPc"
      },
      "source": [
        "# # augmentation을 적용한 이미지\n",
        "# imshow((trainset[300][0]),resize_train_mean,resize_train_std)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5RaH7MIhELb"
      },
      "source": [
        "data loader까지 정의했으니 이제 학습을 위한 데이터 준비는 끝났습니다!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkHpOyhxUcm7"
      },
      "source": [
        "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
        "#                                           shuffle=True, num_workers=0)\n",
        "\n",
        "# testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "#                                          shuffle=False, num_workers=0)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq0evsP8ke_2"
      },
      "source": [
        "# resnet 50 아키텍쳐 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaYnM5VRUcdc"
      },
      "source": [
        "# import resnet\n",
        "import torchvision.models.resnet as resnet\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 미리 정의\n",
        "conv1x1=resnet.conv1x1\n",
        "Bottleneck = resnet.Bottleneck\n",
        "BasicBlock= resnet.BasicBlock"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6i-bHlXUcbD"
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=True):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inplanes = 32 # conv1에서 나올 채널의 차원 -> 이미지넷보다 작은 데이터이므로 32로 조정\n",
        "\n",
        "        # inputs = 3x224x224 -> 3x128x128로 바뀜\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False) # 마찬가지로 전부 사이즈 조정\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        \n",
        "        self.layer1 = self._make_layer(block, 32, layers[0], stride=1) # 3 반복\n",
        "        self.layer2 = self._make_layer(block, 64, layers[1], stride=2) # 4 반복\n",
        "        self.layer3 = self._make_layer(block, 128, layers[2], stride=2) # 6 반복\n",
        "        self.layer4 = self._make_layer(block, 256, layers[3], stride=2) # 3 반복\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1): # planes -> 입력되는 채널 수\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion: \n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input [32, 128, 128] -> [C ,H, W]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        #x.shape =[32, 64, 64]\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        #x.shape =[128, 64, 64]\n",
        "        x = self.layer2(x)\n",
        "        #x.shape =[256, 32, 32]\n",
        "        x = self.layer3(x)\n",
        "        #x.shape =[512, 16, 16]\n",
        "        x = self.layer4(x)\n",
        "        #x.shape =[1024, 8, 8]\n",
        "        \n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSApkvKbUcYQ"
      },
      "source": [
        "resnet50 = ResNet(resnet.Bottleneck, [3, 4, 6, 3], 3, True).to(device) \n",
        "# resnet50\n",
        "# 1(conv1) + 9(layer1) + 12(layer2) + 18(layer3) + 9(layer4) +1(fc)= ResNet50"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7yTW3RIOQI5"
      },
      "source": [
        "## 구성한 아키텍쳐 확인하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOiTbuw_rIuL",
        "outputId": "fb42202c-4d99-42e4-bd24-7b2f4c8ba8fe"
      },
      "source": [
        "# 출력 tensor가 맞는지 확인해보자\n",
        "\n",
        "from torchsummary import summary\n",
        "summary(resnet50, input_size=(3, 128, 128), device=device)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 128, 128]             864\n",
            "       BatchNorm2d-2         [-1, 32, 128, 128]              64\n",
            "              ReLU-3         [-1, 32, 128, 128]               0\n",
            "         MaxPool2d-4           [-1, 32, 64, 64]               0\n",
            "            Conv2d-5           [-1, 32, 64, 64]           1,024\n",
            "       BatchNorm2d-6           [-1, 32, 64, 64]              64\n",
            "              ReLU-7           [-1, 32, 64, 64]               0\n",
            "            Conv2d-8           [-1, 32, 64, 64]           9,216\n",
            "       BatchNorm2d-9           [-1, 32, 64, 64]              64\n",
            "             ReLU-10           [-1, 32, 64, 64]               0\n",
            "           Conv2d-11          [-1, 128, 64, 64]           4,096\n",
            "      BatchNorm2d-12          [-1, 128, 64, 64]             256\n",
            "           Conv2d-13          [-1, 128, 64, 64]           4,096\n",
            "      BatchNorm2d-14          [-1, 128, 64, 64]             256\n",
            "             ReLU-15          [-1, 128, 64, 64]               0\n",
            "       Bottleneck-16          [-1, 128, 64, 64]               0\n",
            "           Conv2d-17           [-1, 32, 64, 64]           4,096\n",
            "      BatchNorm2d-18           [-1, 32, 64, 64]              64\n",
            "             ReLU-19           [-1, 32, 64, 64]               0\n",
            "           Conv2d-20           [-1, 32, 64, 64]           9,216\n",
            "      BatchNorm2d-21           [-1, 32, 64, 64]              64\n",
            "             ReLU-22           [-1, 32, 64, 64]               0\n",
            "           Conv2d-23          [-1, 128, 64, 64]           4,096\n",
            "      BatchNorm2d-24          [-1, 128, 64, 64]             256\n",
            "             ReLU-25          [-1, 128, 64, 64]               0\n",
            "       Bottleneck-26          [-1, 128, 64, 64]               0\n",
            "           Conv2d-27           [-1, 32, 64, 64]           4,096\n",
            "      BatchNorm2d-28           [-1, 32, 64, 64]              64\n",
            "             ReLU-29           [-1, 32, 64, 64]               0\n",
            "           Conv2d-30           [-1, 32, 64, 64]           9,216\n",
            "      BatchNorm2d-31           [-1, 32, 64, 64]              64\n",
            "             ReLU-32           [-1, 32, 64, 64]               0\n",
            "           Conv2d-33          [-1, 128, 64, 64]           4,096\n",
            "      BatchNorm2d-34          [-1, 128, 64, 64]             256\n",
            "             ReLU-35          [-1, 128, 64, 64]               0\n",
            "       Bottleneck-36          [-1, 128, 64, 64]               0\n",
            "           Conv2d-37           [-1, 64, 64, 64]           8,192\n",
            "      BatchNorm2d-38           [-1, 64, 64, 64]             128\n",
            "             ReLU-39           [-1, 64, 64, 64]               0\n",
            "           Conv2d-40           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-41           [-1, 64, 32, 32]             128\n",
            "             ReLU-42           [-1, 64, 32, 32]               0\n",
            "           Conv2d-43          [-1, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-44          [-1, 256, 32, 32]             512\n",
            "           Conv2d-45          [-1, 256, 32, 32]          32,768\n",
            "      BatchNorm2d-46          [-1, 256, 32, 32]             512\n",
            "             ReLU-47          [-1, 256, 32, 32]               0\n",
            "       Bottleneck-48          [-1, 256, 32, 32]               0\n",
            "           Conv2d-49           [-1, 64, 32, 32]          16,384\n",
            "      BatchNorm2d-50           [-1, 64, 32, 32]             128\n",
            "             ReLU-51           [-1, 64, 32, 32]               0\n",
            "           Conv2d-52           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-53           [-1, 64, 32, 32]             128\n",
            "             ReLU-54           [-1, 64, 32, 32]               0\n",
            "           Conv2d-55          [-1, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-56          [-1, 256, 32, 32]             512\n",
            "             ReLU-57          [-1, 256, 32, 32]               0\n",
            "       Bottleneck-58          [-1, 256, 32, 32]               0\n",
            "           Conv2d-59           [-1, 64, 32, 32]          16,384\n",
            "      BatchNorm2d-60           [-1, 64, 32, 32]             128\n",
            "             ReLU-61           [-1, 64, 32, 32]               0\n",
            "           Conv2d-62           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-63           [-1, 64, 32, 32]             128\n",
            "             ReLU-64           [-1, 64, 32, 32]               0\n",
            "           Conv2d-65          [-1, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-66          [-1, 256, 32, 32]             512\n",
            "             ReLU-67          [-1, 256, 32, 32]               0\n",
            "       Bottleneck-68          [-1, 256, 32, 32]               0\n",
            "           Conv2d-69           [-1, 64, 32, 32]          16,384\n",
            "      BatchNorm2d-70           [-1, 64, 32, 32]             128\n",
            "             ReLU-71           [-1, 64, 32, 32]               0\n",
            "           Conv2d-72           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-73           [-1, 64, 32, 32]             128\n",
            "             ReLU-74           [-1, 64, 32, 32]               0\n",
            "           Conv2d-75          [-1, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-76          [-1, 256, 32, 32]             512\n",
            "             ReLU-77          [-1, 256, 32, 32]               0\n",
            "       Bottleneck-78          [-1, 256, 32, 32]               0\n",
            "           Conv2d-79          [-1, 128, 32, 32]          32,768\n",
            "      BatchNorm2d-80          [-1, 128, 32, 32]             256\n",
            "             ReLU-81          [-1, 128, 32, 32]               0\n",
            "           Conv2d-82          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-83          [-1, 128, 16, 16]             256\n",
            "             ReLU-84          [-1, 128, 16, 16]               0\n",
            "           Conv2d-85          [-1, 512, 16, 16]          65,536\n",
            "      BatchNorm2d-86          [-1, 512, 16, 16]           1,024\n",
            "           Conv2d-87          [-1, 512, 16, 16]         131,072\n",
            "      BatchNorm2d-88          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-89          [-1, 512, 16, 16]               0\n",
            "       Bottleneck-90          [-1, 512, 16, 16]               0\n",
            "           Conv2d-91          [-1, 128, 16, 16]          65,536\n",
            "      BatchNorm2d-92          [-1, 128, 16, 16]             256\n",
            "             ReLU-93          [-1, 128, 16, 16]               0\n",
            "           Conv2d-94          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-95          [-1, 128, 16, 16]             256\n",
            "             ReLU-96          [-1, 128, 16, 16]               0\n",
            "           Conv2d-97          [-1, 512, 16, 16]          65,536\n",
            "      BatchNorm2d-98          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-99          [-1, 512, 16, 16]               0\n",
            "      Bottleneck-100          [-1, 512, 16, 16]               0\n",
            "          Conv2d-101          [-1, 128, 16, 16]          65,536\n",
            "     BatchNorm2d-102          [-1, 128, 16, 16]             256\n",
            "            ReLU-103          [-1, 128, 16, 16]               0\n",
            "          Conv2d-104          [-1, 128, 16, 16]         147,456\n",
            "     BatchNorm2d-105          [-1, 128, 16, 16]             256\n",
            "            ReLU-106          [-1, 128, 16, 16]               0\n",
            "          Conv2d-107          [-1, 512, 16, 16]          65,536\n",
            "     BatchNorm2d-108          [-1, 512, 16, 16]           1,024\n",
            "            ReLU-109          [-1, 512, 16, 16]               0\n",
            "      Bottleneck-110          [-1, 512, 16, 16]               0\n",
            "          Conv2d-111          [-1, 128, 16, 16]          65,536\n",
            "     BatchNorm2d-112          [-1, 128, 16, 16]             256\n",
            "            ReLU-113          [-1, 128, 16, 16]               0\n",
            "          Conv2d-114          [-1, 128, 16, 16]         147,456\n",
            "     BatchNorm2d-115          [-1, 128, 16, 16]             256\n",
            "            ReLU-116          [-1, 128, 16, 16]               0\n",
            "          Conv2d-117          [-1, 512, 16, 16]          65,536\n",
            "     BatchNorm2d-118          [-1, 512, 16, 16]           1,024\n",
            "            ReLU-119          [-1, 512, 16, 16]               0\n",
            "      Bottleneck-120          [-1, 512, 16, 16]               0\n",
            "          Conv2d-121          [-1, 128, 16, 16]          65,536\n",
            "     BatchNorm2d-122          [-1, 128, 16, 16]             256\n",
            "            ReLU-123          [-1, 128, 16, 16]               0\n",
            "          Conv2d-124          [-1, 128, 16, 16]         147,456\n",
            "     BatchNorm2d-125          [-1, 128, 16, 16]             256\n",
            "            ReLU-126          [-1, 128, 16, 16]               0\n",
            "          Conv2d-127          [-1, 512, 16, 16]          65,536\n",
            "     BatchNorm2d-128          [-1, 512, 16, 16]           1,024\n",
            "            ReLU-129          [-1, 512, 16, 16]               0\n",
            "      Bottleneck-130          [-1, 512, 16, 16]               0\n",
            "          Conv2d-131          [-1, 128, 16, 16]          65,536\n",
            "     BatchNorm2d-132          [-1, 128, 16, 16]             256\n",
            "            ReLU-133          [-1, 128, 16, 16]               0\n",
            "          Conv2d-134          [-1, 128, 16, 16]         147,456\n",
            "     BatchNorm2d-135          [-1, 128, 16, 16]             256\n",
            "            ReLU-136          [-1, 128, 16, 16]               0\n",
            "          Conv2d-137          [-1, 512, 16, 16]          65,536\n",
            "     BatchNorm2d-138          [-1, 512, 16, 16]           1,024\n",
            "            ReLU-139          [-1, 512, 16, 16]               0\n",
            "      Bottleneck-140          [-1, 512, 16, 16]               0\n",
            "          Conv2d-141          [-1, 256, 16, 16]         131,072\n",
            "     BatchNorm2d-142          [-1, 256, 16, 16]             512\n",
            "            ReLU-143          [-1, 256, 16, 16]               0\n",
            "          Conv2d-144            [-1, 256, 8, 8]         589,824\n",
            "     BatchNorm2d-145            [-1, 256, 8, 8]             512\n",
            "            ReLU-146            [-1, 256, 8, 8]               0\n",
            "          Conv2d-147           [-1, 1024, 8, 8]         262,144\n",
            "     BatchNorm2d-148           [-1, 1024, 8, 8]           2,048\n",
            "          Conv2d-149           [-1, 1024, 8, 8]         524,288\n",
            "     BatchNorm2d-150           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-151           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-152           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-153            [-1, 256, 8, 8]         262,144\n",
            "     BatchNorm2d-154            [-1, 256, 8, 8]             512\n",
            "            ReLU-155            [-1, 256, 8, 8]               0\n",
            "          Conv2d-156            [-1, 256, 8, 8]         589,824\n",
            "     BatchNorm2d-157            [-1, 256, 8, 8]             512\n",
            "            ReLU-158            [-1, 256, 8, 8]               0\n",
            "          Conv2d-159           [-1, 1024, 8, 8]         262,144\n",
            "     BatchNorm2d-160           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-161           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-162           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-163            [-1, 256, 8, 8]         262,144\n",
            "     BatchNorm2d-164            [-1, 256, 8, 8]             512\n",
            "            ReLU-165            [-1, 256, 8, 8]               0\n",
            "          Conv2d-166            [-1, 256, 8, 8]         589,824\n",
            "     BatchNorm2d-167            [-1, 256, 8, 8]             512\n",
            "            ReLU-168            [-1, 256, 8, 8]               0\n",
            "          Conv2d-169           [-1, 1024, 8, 8]         262,144\n",
            "     BatchNorm2d-170           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-171           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-172           [-1, 1024, 8, 8]               0\n",
            "AdaptiveAvgPool2d-173           [-1, 1024, 1, 1]               0\n",
            "          Linear-174                    [-1, 3]           3,075\n",
            "================================================================\n",
            "Total params: 5,891,875\n",
            "Trainable params: 5,891,875\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.19\n",
            "Forward/backward pass size (MB): 187.13\n",
            "Params size (MB): 22.48\n",
            "Estimated Total Size (MB): 209.80\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ50nGrT-zBa"
      },
      "source": [
        "#from torchvision import models\n",
        "#import torch\n",
        "\n",
        "#resnet50_pretrained = models.resnet50(pretrained=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0nDL-BJ-XBE"
      },
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6mSsmoJUcWB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "outputId": "dbe5fe5d-e61c-4b2b-bb02-5b097f731c45"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# 첫 번째 layer의 filter를 확인해보자 (=가중치 확인) -> 나중에 학습을 완료한 후의 filter도 확인하기\n",
        "for w in resnet50.parameters():\n",
        "    w = w.data.cpu()\n",
        "    print(w.shape)\n",
        "    break\n",
        "\n",
        "# 가중치 renormalization\n",
        "min_w = torch.min(w)\n",
        "w1 = (-1/(2 * min_w)) * w + 0.5\n",
        "\n",
        "# make grid to display it\n",
        "grid_size = len(w1)\n",
        "x_grid = [w1[i] for i in range(grid_size)]\n",
        "x_grid = torchvision.utils.make_grid(x_grid, nrow=6, padding=1)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "imshow(x_grid)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 3, 3])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgl5q-Qoz7zB"
      },
      "source": [
        "## 파라미터 설정하기"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장 함수 정의\n",
        "def save_model(model, saved_dir):\n",
        "  os.makedirs(saved_dir, exist_ok=True)  # 폴더가 존재하지 않으면, 디렉토리를 생성함.\n",
        "  check_point = {\n",
        "  'net': model.state_dict()\n",
        "  #'optim' : optimizer.state_dict()\n",
        "  #'loss' : loss.state_dict()\n",
        "  #'epoch' : epoch.state_dict()\n",
        "  }\n",
        "  #output_path = os.path.join(saved_dir) # 옵션들을 합쳐 경로 지정\n",
        "  torch.save(check_point, saved_dir+'/best_model_weight.pt') # 인수로 '모델의 매개 변수, 경로'를 넣어주면 된다.\n"
      ],
      "metadata": {
        "id": "7c1gniM_3N5M"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lkZGLamUcKL"
      },
      "source": [
        "# config 모델 파라미터 인자를 만들기위한 클래스\n",
        "class Config:\n",
        "  def __init__(self, **kwargs):\n",
        "    for key, value in kwargs.items():\n",
        "      setattr(self, key, value)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjiuQhZ03gY5"
      },
      "source": [
        "lr = 0.0008\n",
        "epochs = 30\n",
        "optimizer = 'Adam'"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsctNJSGUcHc"
      },
      "source": [
        "# # 파라미터 클래스 => tranfer.compose 함수를 사용해서 데이터 전처리 진행하는 경우\n",
        "# config = Config(\n",
        "#     trainloader = trainloader,\n",
        "#     testloader = testloader,\n",
        "#     model = resnet50,\n",
        "#     device = device,\n",
        "#     optimizer = torch.optim.Adam(resnet50.parameters(), lr=lr),\n",
        "#     criterion= nn.CrossEntropyLoss().to(device),\n",
        "#     globaliter = 0\n",
        "# )"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# albumentations 사용해서 데이터 전처리 하는 경우\n",
        "# 파라미터 클래스\n",
        "config = Config(\n",
        "    trainloader = albumentations_train_loader,\n",
        "    testloader = albumentations_test_loader,\n",
        "    model = resnet50,\n",
        "    device = device,\n",
        "    optimizer = torch.optim.Adam(resnet50.parameters(), lr=lr),\n",
        "    criterion= nn.CrossEntropyLoss().to(device),\n",
        "    globaliter = 0\n",
        ")"
      ],
      "metadata": {
        "id": "UMfSmyjX3cLL"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSWibVMIUcFY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "238dc374-76c4-4e47-c77a-7ca7191c48be"
      },
      "source": [
        "class train_test():\n",
        "      def __init__(self, config):\n",
        "        # 파라미터 인자\n",
        "        self.trainloader = config.trainloader\n",
        "        self.testloader = config.testloader\n",
        "        self.model = config.model\n",
        "        self.device = config.device\n",
        "        self.optimizer = config.optimizer\n",
        "        self.criterion = config.criterion\n",
        "        self.globaliter = config.globaliter\n",
        "        print(len(self.trainloader))\n",
        "\n",
        "      def train(self, epochs, log_interval):\n",
        "          self.model.train()\n",
        "          for epoch in range(1, epochs + 1 ):  # epochs 루프\n",
        "              running_loss = 0.0\n",
        "              lr_sche.step()\n",
        "              # scheduler.step()\n",
        "              # optimizer.step()\n",
        "              for i, data in enumerate(self.trainloader, 0): # batch 루프\n",
        "                  # get the inputs\n",
        "                  self.globaliter += 1\n",
        "                  inputs, labels = data # input data, label 분리\n",
        "                  inputs = inputs.to(self.device)\n",
        "                  labels = labels.to(self.device)\n",
        "\n",
        "                  # 가중치 초기화 -> 이전 batch에서 계산되었던 가중치를 0으로 만들고 최적화 진행\n",
        "                  self.optimizer.zero_grad() \n",
        "\n",
        "                  # forward + backward + optimize\n",
        "                  outputs = self.model(inputs)\n",
        "                  loss = self.criterion(outputs, labels)\n",
        "                  loss.backward()\n",
        "                  self.optimizer.step()\n",
        "                  running_loss += loss.item()\n",
        "\n",
        "                  print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tlearningLoss: {:.6f}\\twhole_loss: {:.6f} '.format(\n",
        "                        epoch, i*len(inputs), len(self.trainloader.dataset),\n",
        "                        100. * i*len(inputs) / len(self.trainloader.dataset), \n",
        "                        running_loss / log_interval,\n",
        "                        loss.item()))\n",
        "                  \n",
        "                  # 30 iteration마다 acc & loss 출력\n",
        "                  if i % log_interval == log_interval -1 : # i는 1에포크의 iteration\n",
        "                    # print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tlearningLoss: {:.6f}\\twhole_loss: {:.6f} '.format(\n",
        "                    #     epoch, i*len(inputs), len(self.trainloader.dataset),\n",
        "                    #     100. * i*len(inputs) / len(self.trainloader.dataset), \n",
        "                    #     running_loss / log_interval,\n",
        "                    #     loss.item()))\n",
        "                    running_loss = 0.0\n",
        "\n",
        "                    #with train_summary_writer.as_default():\n",
        "                    #    summary.scalar('loss', loss.item() , step = self.globaliter)\n",
        "\n",
        "              with torch.no_grad():\n",
        "                  self.model.eval()\n",
        "                  correct = 0\n",
        "                  total = 0\n",
        "                  test_loss = 0\n",
        "                  acc = []\n",
        "                  for k, data in enumerate(self.testloader, 0):\n",
        "                    images, labels = data\n",
        "                    images = images.to(self.device)\n",
        "                    labels = labels.to(self.device)\n",
        "                    outputs = self.model(images)      # tensor 값으로 출력됨\n",
        "\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()     # correct: 잘 분류된 이미지 개수\n",
        "                    test_loss += self.criterion(outputs, labels).item()\n",
        "                    acc.append(100 * correct/total)\n",
        "                    print(\"Real: \", labels)       #0(tiger), 1(flower), 2(letter) 값으로 출력됨\n",
        "                    print(\"Predicted: \", predicted)\n",
        "\n",
        "                  print('\\nTest set : Average loss:{:.4f}, Accuracy: {}/{}({:.0f}%)\\n'.format(\n",
        "                      test_loss, correct, total, 100 * correct/total\n",
        "                  ))\n",
        "                  # print(\"Test 결과:\\n\")\n",
        "                  # print('Real labels: ', ' '.join('%5s' % classes[labels[j]] \n",
        "                  #              for j in range(75)))   # batch size = 16\n",
        "                  # print('Predicted labels: ', ' '.join('%5s' % classes[predicted[j]] \n",
        "                  #                             for j in range(75)))\n",
        "\n",
        "                  \n",
        "                  # * 모델 저장 시 아래 주석 풀고 진행하기!!\n",
        "\n",
        "                  # with test_summary_writer.as_default():\n",
        "                  #    summary.scalar('loss', test_loss , step = self.globaliter)\n",
        "                  #    summary.scalar('accuracy', 100 * correct/total , step = self.globaliter)  \n",
        "                  #    if acc [k] > 60 and acc[k] > acc[k-1]:\n",
        "                  #       torch.save({\n",
        "                  #                   'epoch': epoch,\n",
        "                  #                   'model_state_dict': self.model.state_dict(),\n",
        "                  #                   'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                  #                   'loss': test_loss\n",
        "                  #                   }, PATH)\n",
        "                         \n",
        "      print('Finished Training')\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classes)\n",
        "\n",
        "# 0: tiger, 1: flower, 2: letter"
      ],
      "metadata": {
        "id": "ajYRt_60AkQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cc04e79-017a-4679-ec16-d500fb991163"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('tiger', 'flower', 'letter')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2fjPoqKUcDb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af065220-a95b-4005-e7cc-54243f7174c1"
      },
      "source": [
        "ready_to_train=train_test(config)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD2D-0YHUcAk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a4d5eee-debb-4d41-c3fd-e52e4fedbb5b"
      },
      "source": [
        "lr_sche = optim.lr_scheduler.StepLR(config.optimizer, step_size=10000, gamma=0.5) # 20 step마다 lr조정\n",
        "epochs = 200\n",
        "log_interval = 175\n",
        "\n",
        "model = ready_to_train.train(epochs, log_interval)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "Train Epoch: 87 [256/486 (53%)]\tlearningLoss: 0.007546\twhole_loss: 0.008646 \n",
            "Train Epoch: 87 [272/486 (56%)]\tlearningLoss: 0.007653\twhole_loss: 0.018726 \n",
            "Train Epoch: 87 [288/486 (59%)]\tlearningLoss: 0.007714\twhole_loss: 0.010775 \n",
            "Train Epoch: 87 [304/486 (63%)]\tlearningLoss: 0.007739\twhole_loss: 0.004406 \n",
            "Train Epoch: 87 [320/486 (66%)]\tlearningLoss: 0.007926\twhole_loss: 0.032632 \n",
            "Train Epoch: 87 [336/486 (69%)]\tlearningLoss: 0.008783\twhole_loss: 0.150001 \n",
            "Train Epoch: 87 [352/486 (72%)]\tlearningLoss: 0.009612\twhole_loss: 0.145152 \n",
            "Train Epoch: 87 [368/486 (76%)]\tlearningLoss: 0.009802\twhole_loss: 0.033242 \n",
            "Train Epoch: 87 [384/486 (79%)]\tlearningLoss: 0.010747\twhole_loss: 0.165330 \n",
            "Train Epoch: 87 [400/486 (82%)]\tlearningLoss: 0.010846\twhole_loss: 0.017346 \n",
            "Train Epoch: 87 [416/486 (86%)]\tlearningLoss: 0.011574\twhole_loss: 0.127416 \n",
            "Train Epoch: 87 [432/486 (89%)]\tlearningLoss: 0.013085\twhole_loss: 0.264398 \n",
            "Train Epoch: 87 [448/486 (92%)]\tlearningLoss: 0.013162\twhole_loss: 0.013486 \n",
            "Train Epoch: 87 [464/486 (95%)]\tlearningLoss: 0.013474\twhole_loss: 0.054648 \n",
            "Train Epoch: 87 [180/486 (37%)]\tlearningLoss: 0.013666\twhole_loss: 0.033576 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 2, 1, 1, 1, 1, 1, 2, 0, 2, 2, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.4243, Accuracy: 63/75(84%)\n",
            "\n",
            "Train Epoch: 88 [0/486 (0%)]\tlearningLoss: 0.001152\twhole_loss: 0.201686 \n",
            "Train Epoch: 88 [16/486 (3%)]\tlearningLoss: 0.001272\twhole_loss: 0.020876 \n",
            "Train Epoch: 88 [32/486 (7%)]\tlearningLoss: 0.001282\twhole_loss: 0.001803 \n",
            "Train Epoch: 88 [48/486 (10%)]\tlearningLoss: 0.001479\twhole_loss: 0.034434 \n",
            "Train Epoch: 88 [64/486 (13%)]\tlearningLoss: 0.001664\twhole_loss: 0.032470 \n",
            "Train Epoch: 88 [80/486 (16%)]\tlearningLoss: 0.001954\twhole_loss: 0.050690 \n",
            "Train Epoch: 88 [96/486 (20%)]\tlearningLoss: 0.002185\twhole_loss: 0.040394 \n",
            "Train Epoch: 88 [112/486 (23%)]\tlearningLoss: 0.002678\twhole_loss: 0.086278 \n",
            "Train Epoch: 88 [128/486 (26%)]\tlearningLoss: 0.003153\twhole_loss: 0.083198 \n",
            "Train Epoch: 88 [144/486 (30%)]\tlearningLoss: 0.003781\twhole_loss: 0.109848 \n",
            "Train Epoch: 88 [160/486 (33%)]\tlearningLoss: 0.004135\twhole_loss: 0.061949 \n",
            "Train Epoch: 88 [176/486 (36%)]\tlearningLoss: 0.004536\twhole_loss: 0.070160 \n",
            "Train Epoch: 88 [192/486 (40%)]\tlearningLoss: 0.004988\twhole_loss: 0.079046 \n",
            "Train Epoch: 88 [208/486 (43%)]\tlearningLoss: 0.005314\twhole_loss: 0.057169 \n",
            "Train Epoch: 88 [224/486 (46%)]\tlearningLoss: 0.005430\twhole_loss: 0.020289 \n",
            "Train Epoch: 88 [240/486 (49%)]\tlearningLoss: 0.006011\twhole_loss: 0.101607 \n",
            "Train Epoch: 88 [256/486 (53%)]\tlearningLoss: 0.006345\twhole_loss: 0.058544 \n",
            "Train Epoch: 88 [272/486 (56%)]\tlearningLoss: 0.006491\twhole_loss: 0.025422 \n",
            "Train Epoch: 88 [288/486 (59%)]\tlearningLoss: 0.007094\twhole_loss: 0.105659 \n",
            "Train Epoch: 88 [304/486 (63%)]\tlearningLoss: 0.007584\twhole_loss: 0.085734 \n",
            "Train Epoch: 88 [320/486 (66%)]\tlearningLoss: 0.007668\twhole_loss: 0.014557 \n",
            "Train Epoch: 88 [336/486 (69%)]\tlearningLoss: 0.008380\twhole_loss: 0.124749 \n",
            "Train Epoch: 88 [352/486 (72%)]\tlearningLoss: 0.008739\twhole_loss: 0.062849 \n",
            "Train Epoch: 88 [368/486 (76%)]\tlearningLoss: 0.009573\twhole_loss: 0.145861 \n",
            "Train Epoch: 88 [384/486 (79%)]\tlearningLoss: 0.009691\twhole_loss: 0.020648 \n",
            "Train Epoch: 88 [400/486 (82%)]\tlearningLoss: 0.010107\twhole_loss: 0.072850 \n",
            "Train Epoch: 88 [416/486 (86%)]\tlearningLoss: 0.010181\twhole_loss: 0.012979 \n",
            "Train Epoch: 88 [432/486 (89%)]\tlearningLoss: 0.010246\twhole_loss: 0.011222 \n",
            "Train Epoch: 88 [448/486 (92%)]\tlearningLoss: 0.010313\twhole_loss: 0.011761 \n",
            "Train Epoch: 88 [464/486 (95%)]\tlearningLoss: 0.010383\twhole_loss: 0.012356 \n",
            "Train Epoch: 88 [180/486 (37%)]\tlearningLoss: 0.010487\twhole_loss: 0.018183 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 2, 2, 1, 2, 2, 1, 2, 0, 2, 2, 2, 1, 2, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:6.1080, Accuracy: 56/75(75%)\n",
            "\n",
            "Train Epoch: 89 [0/486 (0%)]\tlearningLoss: 0.000009\twhole_loss: 0.001655 \n",
            "Train Epoch: 89 [16/486 (3%)]\tlearningLoss: 0.000380\twhole_loss: 0.064804 \n",
            "Train Epoch: 89 [32/486 (7%)]\tlearningLoss: 0.002582\twhole_loss: 0.385315 \n",
            "Train Epoch: 89 [48/486 (10%)]\tlearningLoss: 0.002846\twhole_loss: 0.046350 \n",
            "Train Epoch: 89 [64/486 (13%)]\tlearningLoss: 0.003191\twhole_loss: 0.060299 \n",
            "Train Epoch: 89 [80/486 (16%)]\tlearningLoss: 0.003386\twhole_loss: 0.034085 \n",
            "Train Epoch: 89 [96/486 (20%)]\tlearningLoss: 0.003582\twhole_loss: 0.034349 \n",
            "Train Epoch: 89 [112/486 (23%)]\tlearningLoss: 0.004203\twhole_loss: 0.108682 \n",
            "Train Epoch: 89 [128/486 (26%)]\tlearningLoss: 0.004465\twhole_loss: 0.045779 \n",
            "Train Epoch: 89 [144/486 (30%)]\tlearningLoss: 0.004784\twhole_loss: 0.055808 \n",
            "Train Epoch: 89 [160/486 (33%)]\tlearningLoss: 0.005484\twhole_loss: 0.122587 \n",
            "Train Epoch: 89 [176/486 (36%)]\tlearningLoss: 0.006419\twhole_loss: 0.163644 \n",
            "Train Epoch: 89 [192/486 (40%)]\tlearningLoss: 0.006484\twhole_loss: 0.011377 \n",
            "Train Epoch: 89 [208/486 (43%)]\tlearningLoss: 0.006576\twhole_loss: 0.016100 \n",
            "Train Epoch: 89 [224/486 (46%)]\tlearningLoss: 0.006776\twhole_loss: 0.035049 \n",
            "Train Epoch: 89 [240/486 (49%)]\tlearningLoss: 0.010779\twhole_loss: 0.700440 \n",
            "Train Epoch: 89 [256/486 (53%)]\tlearningLoss: 0.010847\twhole_loss: 0.011910 \n",
            "Train Epoch: 89 [272/486 (56%)]\tlearningLoss: 0.011481\twhole_loss: 0.110862 \n",
            "Train Epoch: 89 [288/486 (59%)]\tlearningLoss: 0.012930\twhole_loss: 0.253734 \n",
            "Train Epoch: 89 [304/486 (63%)]\tlearningLoss: 0.014786\twhole_loss: 0.324768 \n",
            "Train Epoch: 89 [320/486 (66%)]\tlearningLoss: 0.015282\twhole_loss: 0.086787 \n",
            "Train Epoch: 89 [336/486 (69%)]\tlearningLoss: 0.015538\twhole_loss: 0.044785 \n",
            "Train Epoch: 89 [352/486 (72%)]\tlearningLoss: 0.015974\twhole_loss: 0.076233 \n",
            "Train Epoch: 89 [368/486 (76%)]\tlearningLoss: 0.016614\twhole_loss: 0.112063 \n",
            "Train Epoch: 89 [384/486 (79%)]\tlearningLoss: 0.016659\twhole_loss: 0.007938 \n",
            "Train Epoch: 89 [400/486 (82%)]\tlearningLoss: 0.016691\twhole_loss: 0.005443 \n",
            "Train Epoch: 89 [416/486 (86%)]\tlearningLoss: 0.016722\twhole_loss: 0.005473 \n",
            "Train Epoch: 89 [432/486 (89%)]\tlearningLoss: 0.016929\twhole_loss: 0.036267 \n",
            "Train Epoch: 89 [448/486 (92%)]\tlearningLoss: 0.017219\twhole_loss: 0.050791 \n",
            "Train Epoch: 89 [464/486 (95%)]\tlearningLoss: 0.018395\twhole_loss: 0.205769 \n",
            "Train Epoch: 89 [180/486 (37%)]\tlearningLoss: 0.018913\twhole_loss: 0.090648 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.4471, Accuracy: 67/75(89%)\n",
            "\n",
            "Train Epoch: 90 [0/486 (0%)]\tlearningLoss: 0.000136\twhole_loss: 0.023869 \n",
            "Train Epoch: 90 [16/486 (3%)]\tlearningLoss: 0.000162\twhole_loss: 0.004435 \n",
            "Train Epoch: 90 [32/486 (7%)]\tlearningLoss: 0.000723\twhole_loss: 0.098223 \n",
            "Train Epoch: 90 [48/486 (10%)]\tlearningLoss: 0.000754\twhole_loss: 0.005433 \n",
            "Train Epoch: 90 [64/486 (13%)]\tlearningLoss: 0.001490\twhole_loss: 0.128846 \n",
            "Train Epoch: 90 [80/486 (16%)]\tlearningLoss: 0.001934\twhole_loss: 0.077701 \n",
            "Train Epoch: 90 [96/486 (20%)]\tlearningLoss: 0.002923\twhole_loss: 0.173027 \n",
            "Train Epoch: 90 [112/486 (23%)]\tlearningLoss: 0.002929\twhole_loss: 0.001054 \n",
            "Train Epoch: 90 [128/486 (26%)]\tlearningLoss: 0.003275\twhole_loss: 0.060578 \n",
            "Train Epoch: 90 [144/486 (30%)]\tlearningLoss: 0.003319\twhole_loss: 0.007609 \n",
            "Train Epoch: 90 [160/486 (33%)]\tlearningLoss: 0.003897\twhole_loss: 0.101265 \n",
            "Train Epoch: 90 [176/486 (36%)]\tlearningLoss: 0.003981\twhole_loss: 0.014593 \n",
            "Train Epoch: 90 [192/486 (40%)]\tlearningLoss: 0.004422\twhole_loss: 0.077205 \n",
            "Train Epoch: 90 [208/486 (43%)]\tlearningLoss: 0.004790\twhole_loss: 0.064496 \n",
            "Train Epoch: 90 [224/486 (46%)]\tlearningLoss: 0.006125\twhole_loss: 0.233592 \n",
            "Train Epoch: 90 [240/486 (49%)]\tlearningLoss: 0.006188\twhole_loss: 0.010919 \n",
            "Train Epoch: 90 [256/486 (53%)]\tlearningLoss: 0.006301\twhole_loss: 0.019782 \n",
            "Train Epoch: 90 [272/486 (56%)]\tlearningLoss: 0.006346\twhole_loss: 0.007937 \n",
            "Train Epoch: 90 [288/486 (59%)]\tlearningLoss: 0.006831\twhole_loss: 0.084920 \n",
            "Train Epoch: 90 [304/486 (63%)]\tlearningLoss: 0.009636\twhole_loss: 0.490820 \n",
            "Train Epoch: 90 [320/486 (66%)]\tlearningLoss: 0.010074\twhole_loss: 0.076570 \n",
            "Train Epoch: 90 [336/486 (69%)]\tlearningLoss: 0.010884\twhole_loss: 0.141827 \n",
            "Train Epoch: 90 [352/486 (72%)]\tlearningLoss: 0.011056\twhole_loss: 0.030121 \n",
            "Train Epoch: 90 [368/486 (76%)]\tlearningLoss: 0.011108\twhole_loss: 0.008994 \n",
            "Train Epoch: 90 [384/486 (79%)]\tlearningLoss: 0.011223\twhole_loss: 0.020292 \n",
            "Train Epoch: 90 [400/486 (82%)]\tlearningLoss: 0.011280\twhole_loss: 0.009901 \n",
            "Train Epoch: 90 [416/486 (86%)]\tlearningLoss: 0.011436\twhole_loss: 0.027246 \n",
            "Train Epoch: 90 [432/486 (89%)]\tlearningLoss: 0.011850\twhole_loss: 0.072468 \n",
            "Train Epoch: 90 [448/486 (92%)]\tlearningLoss: 0.011851\twhole_loss: 0.000136 \n",
            "Train Epoch: 90 [464/486 (95%)]\tlearningLoss: 0.012785\twhole_loss: 0.163491 \n",
            "Train Epoch: 90 [180/486 (37%)]\tlearningLoss: 0.012796\twhole_loss: 0.001966 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 1, 2, 0, 2], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.4590, Accuracy: 66/75(88%)\n",
            "\n",
            "Train Epoch: 91 [0/486 (0%)]\tlearningLoss: 0.001247\twhole_loss: 0.218210 \n",
            "Train Epoch: 91 [16/486 (3%)]\tlearningLoss: 0.001778\twhole_loss: 0.092961 \n",
            "Train Epoch: 91 [32/486 (7%)]\tlearningLoss: 0.002474\twhole_loss: 0.121799 \n",
            "Train Epoch: 91 [48/486 (10%)]\tlearningLoss: 0.002533\twhole_loss: 0.010277 \n",
            "Train Epoch: 91 [64/486 (13%)]\tlearningLoss: 0.002873\twhole_loss: 0.059605 \n",
            "Train Epoch: 91 [80/486 (16%)]\tlearningLoss: 0.002931\twhole_loss: 0.010107 \n",
            "Train Epoch: 91 [96/486 (20%)]\tlearningLoss: 0.003509\twhole_loss: 0.101177 \n",
            "Train Epoch: 91 [112/486 (23%)]\tlearningLoss: 0.007520\twhole_loss: 0.701824 \n",
            "Train Epoch: 91 [128/486 (26%)]\tlearningLoss: 0.008086\twhole_loss: 0.099164 \n",
            "Train Epoch: 91 [144/486 (30%)]\tlearningLoss: 0.009651\twhole_loss: 0.273867 \n",
            "Train Epoch: 91 [160/486 (33%)]\tlearningLoss: 0.010924\twhole_loss: 0.222684 \n",
            "Train Epoch: 91 [176/486 (36%)]\tlearningLoss: 0.011313\twhole_loss: 0.068143 \n",
            "Train Epoch: 91 [192/486 (40%)]\tlearningLoss: 0.012704\twhole_loss: 0.243434 \n",
            "Train Epoch: 91 [208/486 (43%)]\tlearningLoss: 0.013267\twhole_loss: 0.098435 \n",
            "Train Epoch: 91 [224/486 (46%)]\tlearningLoss: 0.013500\twhole_loss: 0.040816 \n",
            "Train Epoch: 91 [240/486 (49%)]\tlearningLoss: 0.013828\twhole_loss: 0.057459 \n",
            "Train Epoch: 91 [256/486 (53%)]\tlearningLoss: 0.014074\twhole_loss: 0.043054 \n",
            "Train Epoch: 91 [272/486 (56%)]\tlearningLoss: 0.014219\twhole_loss: 0.025314 \n",
            "Train Epoch: 91 [288/486 (59%)]\tlearningLoss: 0.014983\twhole_loss: 0.133774 \n",
            "Train Epoch: 91 [304/486 (63%)]\tlearningLoss: 0.015639\twhole_loss: 0.114643 \n",
            "Train Epoch: 91 [320/486 (66%)]\tlearningLoss: 0.017501\twhole_loss: 0.325959 \n",
            "Train Epoch: 91 [336/486 (69%)]\tlearningLoss: 0.017906\twhole_loss: 0.070760 \n",
            "Train Epoch: 91 [352/486 (72%)]\tlearningLoss: 0.029013\twhole_loss: 1.943835 \n",
            "Train Epoch: 91 [368/486 (76%)]\tlearningLoss: 0.029810\twhole_loss: 0.139512 \n",
            "Train Epoch: 91 [384/486 (79%)]\tlearningLoss: 0.030276\twhole_loss: 0.081566 \n",
            "Train Epoch: 91 [400/486 (82%)]\tlearningLoss: 0.030533\twhole_loss: 0.044934 \n",
            "Train Epoch: 91 [416/486 (86%)]\tlearningLoss: 0.032073\twhole_loss: 0.269377 \n",
            "Train Epoch: 91 [432/486 (89%)]\tlearningLoss: 0.032303\twhole_loss: 0.040378 \n",
            "Train Epoch: 91 [448/486 (92%)]\tlearningLoss: 0.032670\twhole_loss: 0.064095 \n",
            "Train Epoch: 91 [464/486 (95%)]\tlearningLoss: 0.035021\twhole_loss: 0.411483 \n",
            "Train Epoch: 91 [180/486 (37%)]\tlearningLoss: 0.038676\twhole_loss: 0.639651 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.6369, Accuracy: 61/75(81%)\n",
            "\n",
            "Train Epoch: 92 [0/486 (0%)]\tlearningLoss: 0.003110\twhole_loss: 0.544325 \n",
            "Train Epoch: 92 [16/486 (3%)]\tlearningLoss: 0.003180\twhole_loss: 0.012100 \n",
            "Train Epoch: 92 [32/486 (7%)]\tlearningLoss: 0.003530\twhole_loss: 0.061251 \n",
            "Train Epoch: 92 [48/486 (10%)]\tlearningLoss: 0.004670\twhole_loss: 0.199551 \n",
            "Train Epoch: 92 [64/486 (13%)]\tlearningLoss: 0.008542\twhole_loss: 0.677614 \n",
            "Train Epoch: 92 [80/486 (16%)]\tlearningLoss: 0.009044\twhole_loss: 0.087835 \n",
            "Train Epoch: 92 [96/486 (20%)]\tlearningLoss: 0.009732\twhole_loss: 0.120345 \n",
            "Train Epoch: 92 [112/486 (23%)]\tlearningLoss: 0.010044\twhole_loss: 0.054625 \n",
            "Train Epoch: 92 [128/486 (26%)]\tlearningLoss: 0.012455\twhole_loss: 0.422029 \n",
            "Train Epoch: 92 [144/486 (30%)]\tlearningLoss: 0.015540\twhole_loss: 0.539882 \n",
            "Train Epoch: 92 [160/486 (33%)]\tlearningLoss: 0.016454\twhole_loss: 0.159928 \n",
            "Train Epoch: 92 [176/486 (36%)]\tlearningLoss: 0.017078\twhole_loss: 0.109171 \n",
            "Train Epoch: 92 [192/486 (40%)]\tlearningLoss: 0.017396\twhole_loss: 0.055633 \n",
            "Train Epoch: 92 [208/486 (43%)]\tlearningLoss: 0.017494\twhole_loss: 0.017151 \n",
            "Train Epoch: 92 [224/486 (46%)]\tlearningLoss: 0.018987\twhole_loss: 0.261268 \n",
            "Train Epoch: 92 [240/486 (49%)]\tlearningLoss: 0.019244\twhole_loss: 0.045007 \n",
            "Train Epoch: 92 [256/486 (53%)]\tlearningLoss: 0.021460\twhole_loss: 0.387708 \n",
            "Train Epoch: 92 [272/486 (56%)]\tlearningLoss: 0.024190\twhole_loss: 0.477866 \n",
            "Train Epoch: 92 [288/486 (59%)]\tlearningLoss: 0.025204\twhole_loss: 0.177380 \n",
            "Train Epoch: 92 [304/486 (63%)]\tlearningLoss: 0.027293\twhole_loss: 0.365551 \n",
            "Train Epoch: 92 [320/486 (66%)]\tlearningLoss: 0.027705\twhole_loss: 0.072160 \n",
            "Train Epoch: 92 [336/486 (69%)]\tlearningLoss: 0.028074\twhole_loss: 0.064602 \n",
            "Train Epoch: 92 [352/486 (72%)]\tlearningLoss: 0.028735\twhole_loss: 0.115558 \n",
            "Train Epoch: 92 [368/486 (76%)]\tlearningLoss: 0.029394\twhole_loss: 0.115441 \n",
            "Train Epoch: 92 [384/486 (79%)]\tlearningLoss: 0.030356\twhole_loss: 0.168260 \n",
            "Train Epoch: 92 [400/486 (82%)]\tlearningLoss: 0.031049\twhole_loss: 0.121258 \n",
            "Train Epoch: 92 [416/486 (86%)]\tlearningLoss: 0.031315\twhole_loss: 0.046627 \n",
            "Train Epoch: 92 [432/486 (89%)]\tlearningLoss: 0.031956\twhole_loss: 0.112166 \n",
            "Train Epoch: 92 [448/486 (92%)]\tlearningLoss: 0.032694\twhole_loss: 0.129225 \n",
            "Train Epoch: 92 [464/486 (95%)]\tlearningLoss: 0.033120\twhole_loss: 0.074485 \n",
            "Train Epoch: 92 [180/486 (37%)]\tlearningLoss: 0.033363\twhole_loss: 0.042520 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 2, 1, 1, 1, 1, 2, 0, 2, 2, 2, 1, 2, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 2, 2, 2], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.5269, Accuracy: 57/75(76%)\n",
            "\n",
            "Train Epoch: 93 [0/486 (0%)]\tlearningLoss: 0.000081\twhole_loss: 0.014186 \n",
            "Train Epoch: 93 [16/486 (3%)]\tlearningLoss: 0.001059\twhole_loss: 0.171124 \n",
            "Train Epoch: 93 [32/486 (7%)]\tlearningLoss: 0.001292\twhole_loss: 0.040795 \n",
            "Train Epoch: 93 [48/486 (10%)]\tlearningLoss: 0.001822\twhole_loss: 0.092735 \n",
            "Train Epoch: 93 [64/486 (13%)]\tlearningLoss: 0.002607\twhole_loss: 0.137323 \n",
            "Train Epoch: 93 [80/486 (16%)]\tlearningLoss: 0.002800\twhole_loss: 0.033801 \n",
            "Train Epoch: 93 [96/486 (20%)]\tlearningLoss: 0.004120\twhole_loss: 0.231081 \n",
            "Train Epoch: 93 [112/486 (23%)]\tlearningLoss: 0.005399\twhole_loss: 0.223857 \n",
            "Train Epoch: 93 [128/486 (26%)]\tlearningLoss: 0.005843\twhole_loss: 0.077639 \n",
            "Train Epoch: 93 [144/486 (30%)]\tlearningLoss: 0.006444\twhole_loss: 0.105153 \n",
            "Train Epoch: 93 [160/486 (33%)]\tlearningLoss: 0.006900\twhole_loss: 0.079724 \n",
            "Train Epoch: 93 [176/486 (36%)]\tlearningLoss: 0.009434\twhole_loss: 0.443618 \n",
            "Train Epoch: 93 [192/486 (40%)]\tlearningLoss: 0.010718\twhole_loss: 0.224629 \n",
            "Train Epoch: 93 [208/486 (43%)]\tlearningLoss: 0.011926\twhole_loss: 0.211385 \n",
            "Train Epoch: 93 [224/486 (46%)]\tlearningLoss: 0.012534\twhole_loss: 0.106433 \n",
            "Train Epoch: 93 [240/486 (49%)]\tlearningLoss: 0.012718\twhole_loss: 0.032224 \n",
            "Train Epoch: 93 [256/486 (53%)]\tlearningLoss: 0.013232\twhole_loss: 0.089934 \n",
            "Train Epoch: 93 [272/486 (56%)]\tlearningLoss: 0.013659\twhole_loss: 0.074621 \n",
            "Train Epoch: 93 [288/486 (59%)]\tlearningLoss: 0.013895\twhole_loss: 0.041329 \n",
            "Train Epoch: 93 [304/486 (63%)]\tlearningLoss: 0.014054\twhole_loss: 0.027860 \n",
            "Train Epoch: 93 [320/486 (66%)]\tlearningLoss: 0.014406\twhole_loss: 0.061535 \n",
            "Train Epoch: 93 [336/486 (69%)]\tlearningLoss: 0.014552\twhole_loss: 0.025552 \n",
            "Train Epoch: 93 [352/486 (72%)]\tlearningLoss: 0.014923\twhole_loss: 0.064938 \n",
            "Train Epoch: 93 [368/486 (76%)]\tlearningLoss: 0.015247\twhole_loss: 0.056806 \n",
            "Train Epoch: 93 [384/486 (79%)]\tlearningLoss: 0.015356\twhole_loss: 0.019045 \n",
            "Train Epoch: 93 [400/486 (82%)]\tlearningLoss: 0.015812\twhole_loss: 0.079854 \n",
            "Train Epoch: 93 [416/486 (86%)]\tlearningLoss: 0.015903\twhole_loss: 0.015802 \n",
            "Train Epoch: 93 [432/486 (89%)]\tlearningLoss: 0.016230\twhole_loss: 0.057342 \n",
            "Train Epoch: 93 [448/486 (92%)]\tlearningLoss: 0.016690\twhole_loss: 0.080461 \n",
            "Train Epoch: 93 [464/486 (95%)]\tlearningLoss: 0.016909\twhole_loss: 0.038349 \n",
            "Train Epoch: 93 [180/486 (37%)]\tlearningLoss: 0.016989\twhole_loss: 0.013924 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.5226, Accuracy: 70/75(93%)\n",
            "\n",
            "Train Epoch: 94 [0/486 (0%)]\tlearningLoss: 0.000380\twhole_loss: 0.066434 \n",
            "Train Epoch: 94 [16/486 (3%)]\tlearningLoss: 0.000550\twhole_loss: 0.029826 \n",
            "Train Epoch: 94 [32/486 (7%)]\tlearningLoss: 0.000823\twhole_loss: 0.047824 \n",
            "Train Epoch: 94 [48/486 (10%)]\tlearningLoss: 0.001124\twhole_loss: 0.052648 \n",
            "Train Epoch: 94 [64/486 (13%)]\tlearningLoss: 0.001649\twhole_loss: 0.091778 \n",
            "Train Epoch: 94 [80/486 (16%)]\tlearningLoss: 0.002050\twhole_loss: 0.070166 \n",
            "Train Epoch: 94 [96/486 (20%)]\tlearningLoss: 0.002145\twhole_loss: 0.016663 \n",
            "Train Epoch: 94 [112/486 (23%)]\tlearningLoss: 0.002458\twhole_loss: 0.054737 \n",
            "Train Epoch: 94 [128/486 (26%)]\tlearningLoss: 0.002508\twhole_loss: 0.008821 \n",
            "Train Epoch: 94 [144/486 (30%)]\tlearningLoss: 0.002605\twhole_loss: 0.017055 \n",
            "Train Epoch: 94 [160/486 (33%)]\tlearningLoss: 0.002684\twhole_loss: 0.013805 \n",
            "Train Epoch: 94 [176/486 (36%)]\tlearningLoss: 0.002803\twhole_loss: 0.020841 \n",
            "Train Epoch: 94 [192/486 (40%)]\tlearningLoss: 0.002881\twhole_loss: 0.013518 \n",
            "Train Epoch: 94 [208/486 (43%)]\tlearningLoss: 0.003887\twhole_loss: 0.176084 \n",
            "Train Epoch: 94 [224/486 (46%)]\tlearningLoss: 0.004106\twhole_loss: 0.038342 \n",
            "Train Epoch: 94 [240/486 (49%)]\tlearningLoss: 0.004210\twhole_loss: 0.018196 \n",
            "Train Epoch: 94 [256/486 (53%)]\tlearningLoss: 0.004329\twhole_loss: 0.020854 \n",
            "Train Epoch: 94 [272/486 (56%)]\tlearningLoss: 0.004748\twhole_loss: 0.073385 \n",
            "Train Epoch: 94 [288/486 (59%)]\tlearningLoss: 0.004773\twhole_loss: 0.004279 \n",
            "Train Epoch: 94 [304/486 (63%)]\tlearningLoss: 0.004907\twhole_loss: 0.023465 \n",
            "Train Epoch: 94 [320/486 (66%)]\tlearningLoss: 0.005155\twhole_loss: 0.043445 \n",
            "Train Epoch: 94 [336/486 (69%)]\tlearningLoss: 0.005529\twhole_loss: 0.065321 \n",
            "Train Epoch: 94 [352/486 (72%)]\tlearningLoss: 0.006004\twhole_loss: 0.083245 \n",
            "Train Epoch: 94 [368/486 (76%)]\tlearningLoss: 0.006846\twhole_loss: 0.147284 \n",
            "Train Epoch: 94 [384/486 (79%)]\tlearningLoss: 0.006913\twhole_loss: 0.011755 \n",
            "Train Epoch: 94 [400/486 (82%)]\tlearningLoss: 0.007007\twhole_loss: 0.016460 \n",
            "Train Epoch: 94 [416/486 (86%)]\tlearningLoss: 0.007266\twhole_loss: 0.045363 \n",
            "Train Epoch: 94 [432/486 (89%)]\tlearningLoss: 0.007351\twhole_loss: 0.014836 \n",
            "Train Epoch: 94 [448/486 (92%)]\tlearningLoss: 0.007658\twhole_loss: 0.053724 \n",
            "Train Epoch: 94 [464/486 (95%)]\tlearningLoss: 0.008801\twhole_loss: 0.200102 \n",
            "Train Epoch: 94 [180/486 (37%)]\tlearningLoss: 0.008820\twhole_loss: 0.003269 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.6393, Accuracy: 68/75(91%)\n",
            "\n",
            "Train Epoch: 95 [0/486 (0%)]\tlearningLoss: 0.000056\twhole_loss: 0.009771 \n",
            "Train Epoch: 95 [16/486 (3%)]\tlearningLoss: 0.000190\twhole_loss: 0.023426 \n",
            "Train Epoch: 95 [32/486 (7%)]\tlearningLoss: 0.000204\twhole_loss: 0.002530 \n",
            "Train Epoch: 95 [48/486 (10%)]\tlearningLoss: 0.000499\twhole_loss: 0.051568 \n",
            "Train Epoch: 95 [64/486 (13%)]\tlearningLoss: 0.000655\twhole_loss: 0.027397 \n",
            "Train Epoch: 95 [80/486 (16%)]\tlearningLoss: 0.000851\twhole_loss: 0.034244 \n",
            "Train Epoch: 95 [96/486 (20%)]\tlearningLoss: 0.000984\twhole_loss: 0.023241 \n",
            "Train Epoch: 95 [112/486 (23%)]\tlearningLoss: 0.002343\twhole_loss: 0.237919 \n",
            "Train Epoch: 95 [128/486 (26%)]\tlearningLoss: 0.002420\twhole_loss: 0.013320 \n",
            "Train Epoch: 95 [144/486 (30%)]\tlearningLoss: 0.002426\twhole_loss: 0.001199 \n",
            "Train Epoch: 95 [160/486 (33%)]\tlearningLoss: 0.002483\twhole_loss: 0.009980 \n",
            "Train Epoch: 95 [176/486 (36%)]\tlearningLoss: 0.002512\twhole_loss: 0.005003 \n",
            "Train Epoch: 95 [192/486 (40%)]\tlearningLoss: 0.002741\twhole_loss: 0.040014 \n",
            "Train Epoch: 95 [208/486 (43%)]\tlearningLoss: 0.003487\twhole_loss: 0.130556 \n",
            "Train Epoch: 95 [224/486 (46%)]\tlearningLoss: 0.003969\twhole_loss: 0.084491 \n",
            "Train Epoch: 95 [240/486 (49%)]\tlearningLoss: 0.004046\twhole_loss: 0.013305 \n",
            "Train Epoch: 95 [256/486 (53%)]\tlearningLoss: 0.004118\twhole_loss: 0.012712 \n",
            "Train Epoch: 95 [272/486 (56%)]\tlearningLoss: 0.004253\twhole_loss: 0.023613 \n",
            "Train Epoch: 95 [288/486 (59%)]\tlearningLoss: 0.004411\twhole_loss: 0.027705 \n",
            "Train Epoch: 95 [304/486 (63%)]\tlearningLoss: 0.004610\twhole_loss: 0.034687 \n",
            "Train Epoch: 95 [320/486 (66%)]\tlearningLoss: 0.004690\twhole_loss: 0.014022 \n",
            "Train Epoch: 95 [336/486 (69%)]\tlearningLoss: 0.005094\twhole_loss: 0.070750 \n",
            "Train Epoch: 95 [352/486 (72%)]\tlearningLoss: 0.005327\twhole_loss: 0.040727 \n",
            "Train Epoch: 95 [368/486 (76%)]\tlearningLoss: 0.006009\twhole_loss: 0.119415 \n",
            "Train Epoch: 95 [384/486 (79%)]\tlearningLoss: 0.006154\twhole_loss: 0.025288 \n",
            "Train Epoch: 95 [400/486 (82%)]\tlearningLoss: 0.006275\twhole_loss: 0.021235 \n",
            "Train Epoch: 95 [416/486 (86%)]\tlearningLoss: 0.006689\twhole_loss: 0.072372 \n",
            "Train Epoch: 95 [432/486 (89%)]\tlearningLoss: 0.006993\twhole_loss: 0.053323 \n",
            "Train Epoch: 95 [448/486 (92%)]\tlearningLoss: 0.007339\twhole_loss: 0.060528 \n",
            "Train Epoch: 95 [464/486 (95%)]\tlearningLoss: 0.007434\twhole_loss: 0.016564 \n",
            "Train Epoch: 95 [180/486 (37%)]\tlearningLoss: 0.007617\twhole_loss: 0.032154 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 2, 2, 0, 1, 0, 0, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.9461, Accuracy: 65/75(87%)\n",
            "\n",
            "Train Epoch: 96 [0/486 (0%)]\tlearningLoss: 0.000517\twhole_loss: 0.090441 \n",
            "Train Epoch: 96 [16/486 (3%)]\tlearningLoss: 0.000575\twhole_loss: 0.010123 \n",
            "Train Epoch: 96 [32/486 (7%)]\tlearningLoss: 0.000859\twhole_loss: 0.049752 \n",
            "Train Epoch: 96 [48/486 (10%)]\tlearningLoss: 0.000962\twhole_loss: 0.018095 \n",
            "Train Epoch: 96 [64/486 (13%)]\tlearningLoss: 0.001027\twhole_loss: 0.011229 \n",
            "Train Epoch: 96 [80/486 (16%)]\tlearningLoss: 0.001049\twhole_loss: 0.003869 \n",
            "Train Epoch: 96 [96/486 (20%)]\tlearningLoss: 0.001647\twhole_loss: 0.104703 \n",
            "Train Epoch: 96 [112/486 (23%)]\tlearningLoss: 0.002751\twhole_loss: 0.193266 \n",
            "Train Epoch: 96 [128/486 (26%)]\tlearningLoss: 0.002880\twhole_loss: 0.022460 \n",
            "Train Epoch: 96 [144/486 (30%)]\tlearningLoss: 0.003423\twhole_loss: 0.095081 \n",
            "Train Epoch: 96 [160/486 (33%)]\tlearningLoss: 0.003548\twhole_loss: 0.021848 \n",
            "Train Epoch: 96 [176/486 (36%)]\tlearningLoss: 0.003627\twhole_loss: 0.013800 \n",
            "Train Epoch: 96 [192/486 (40%)]\tlearningLoss: 0.003859\twhole_loss: 0.040685 \n",
            "Train Epoch: 96 [208/486 (43%)]\tlearningLoss: 0.003971\twhole_loss: 0.019652 \n",
            "Train Epoch: 96 [224/486 (46%)]\tlearningLoss: 0.004220\twhole_loss: 0.043438 \n",
            "Train Epoch: 96 [240/486 (49%)]\tlearningLoss: 0.004241\twhole_loss: 0.003708 \n",
            "Train Epoch: 96 [256/486 (53%)]\tlearningLoss: 0.004958\twhole_loss: 0.125562 \n",
            "Train Epoch: 96 [272/486 (56%)]\tlearningLoss: 0.005417\twhole_loss: 0.080296 \n",
            "Train Epoch: 96 [288/486 (59%)]\tlearningLoss: 0.007832\twhole_loss: 0.422666 \n",
            "Train Epoch: 96 [304/486 (63%)]\tlearningLoss: 0.007836\twhole_loss: 0.000572 \n",
            "Train Epoch: 96 [320/486 (66%)]\tlearningLoss: 0.008647\twhole_loss: 0.141985 \n",
            "Train Epoch: 96 [336/486 (69%)]\tlearningLoss: 0.008651\twhole_loss: 0.000738 \n",
            "Train Epoch: 96 [352/486 (72%)]\tlearningLoss: 0.008664\twhole_loss: 0.002193 \n",
            "Train Epoch: 96 [368/486 (76%)]\tlearningLoss: 0.008736\twhole_loss: 0.012628 \n",
            "Train Epoch: 96 [384/486 (79%)]\tlearningLoss: 0.010362\twhole_loss: 0.284488 \n",
            "Train Epoch: 96 [400/486 (82%)]\tlearningLoss: 0.010845\twhole_loss: 0.084608 \n",
            "Train Epoch: 96 [416/486 (86%)]\tlearningLoss: 0.014962\twhole_loss: 0.720405 \n",
            "Train Epoch: 96 [432/486 (89%)]\tlearningLoss: 0.014980\twhole_loss: 0.003170 \n",
            "Train Epoch: 96 [448/486 (92%)]\tlearningLoss: 0.015655\twhole_loss: 0.118173 \n",
            "Train Epoch: 96 [464/486 (95%)]\tlearningLoss: 0.016006\twhole_loss: 0.061461 \n",
            "Train Epoch: 96 [180/486 (37%)]\tlearningLoss: 0.023521\twhole_loss: 1.315125 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 2, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 2, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.7574, Accuracy: 65/75(87%)\n",
            "\n",
            "Train Epoch: 97 [0/486 (0%)]\tlearningLoss: 0.000905\twhole_loss: 0.158452 \n",
            "Train Epoch: 97 [16/486 (3%)]\tlearningLoss: 0.000982\twhole_loss: 0.013401 \n",
            "Train Epoch: 97 [32/486 (7%)]\tlearningLoss: 0.001396\twhole_loss: 0.072386 \n",
            "Train Epoch: 97 [48/486 (10%)]\tlearningLoss: 0.001783\twhole_loss: 0.067768 \n",
            "Train Epoch: 97 [64/486 (13%)]\tlearningLoss: 0.002083\twhole_loss: 0.052580 \n",
            "Train Epoch: 97 [80/486 (16%)]\tlearningLoss: 0.002117\twhole_loss: 0.005966 \n",
            "Train Epoch: 97 [96/486 (20%)]\tlearningLoss: 0.003222\twhole_loss: 0.193342 \n",
            "Train Epoch: 97 [112/486 (23%)]\tlearningLoss: 0.003974\twhole_loss: 0.131491 \n",
            "Train Epoch: 97 [128/486 (26%)]\tlearningLoss: 0.004333\twhole_loss: 0.062838 \n",
            "Train Epoch: 97 [144/486 (30%)]\tlearningLoss: 0.006177\twhole_loss: 0.322821 \n",
            "Train Epoch: 97 [160/486 (33%)]\tlearningLoss: 0.010473\twhole_loss: 0.751724 \n",
            "Train Epoch: 97 [176/486 (36%)]\tlearningLoss: 0.012322\twhole_loss: 0.323551 \n",
            "Train Epoch: 97 [192/486 (40%)]\tlearningLoss: 0.014234\twhole_loss: 0.334710 \n",
            "Train Epoch: 97 [208/486 (43%)]\tlearningLoss: 0.015544\twhole_loss: 0.229164 \n",
            "Train Epoch: 97 [224/486 (46%)]\tlearningLoss: 0.018485\twhole_loss: 0.514722 \n",
            "Train Epoch: 97 [240/486 (49%)]\tlearningLoss: 0.019193\twhole_loss: 0.123804 \n",
            "Train Epoch: 97 [256/486 (53%)]\tlearningLoss: 0.020944\twhole_loss: 0.306451 \n",
            "Train Epoch: 97 [272/486 (56%)]\tlearningLoss: 0.021351\twhole_loss: 0.071324 \n",
            "Train Epoch: 97 [288/486 (59%)]\tlearningLoss: 0.023579\twhole_loss: 0.389881 \n",
            "Train Epoch: 97 [304/486 (63%)]\tlearningLoss: 0.025302\twhole_loss: 0.301472 \n",
            "Train Epoch: 97 [320/486 (66%)]\tlearningLoss: 0.027116\twhole_loss: 0.317457 \n",
            "Train Epoch: 97 [336/486 (69%)]\tlearningLoss: 0.031576\twhole_loss: 0.780529 \n",
            "Train Epoch: 97 [352/486 (72%)]\tlearningLoss: 0.040832\twhole_loss: 1.619708 \n",
            "Train Epoch: 97 [368/486 (76%)]\tlearningLoss: 0.041575\twhole_loss: 0.130153 \n",
            "Train Epoch: 97 [384/486 (79%)]\tlearningLoss: 0.042010\twhole_loss: 0.075986 \n",
            "Train Epoch: 97 [400/486 (82%)]\tlearningLoss: 0.043780\twhole_loss: 0.309880 \n",
            "Train Epoch: 97 [416/486 (86%)]\tlearningLoss: 0.045045\twhole_loss: 0.221359 \n",
            "Train Epoch: 97 [432/486 (89%)]\tlearningLoss: 0.046117\twhole_loss: 0.187553 \n",
            "Train Epoch: 97 [448/486 (92%)]\tlearningLoss: 0.046377\twhole_loss: 0.045585 \n",
            "Train Epoch: 97 [464/486 (95%)]\tlearningLoss: 0.046621\twhole_loss: 0.042546 \n",
            "Train Epoch: 97 [180/486 (37%)]\tlearningLoss: 0.046700\twhole_loss: 0.013970 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 2, 1, 2, 1, 1, 2, 0, 2, 2, 2, 1, 1, 2, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 2], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.5832, Accuracy: 55/75(73%)\n",
            "\n",
            "Train Epoch: 98 [0/486 (0%)]\tlearningLoss: 0.000115\twhole_loss: 0.020070 \n",
            "Train Epoch: 98 [16/486 (3%)]\tlearningLoss: 0.000853\twhole_loss: 0.129251 \n",
            "Train Epoch: 98 [32/486 (7%)]\tlearningLoss: 0.001561\twhole_loss: 0.123788 \n",
            "Train Epoch: 98 [48/486 (10%)]\tlearningLoss: 0.002694\twhole_loss: 0.198301 \n",
            "Train Epoch: 98 [64/486 (13%)]\tlearningLoss: 0.004426\twhole_loss: 0.303094 \n",
            "Train Epoch: 98 [80/486 (16%)]\tlearningLoss: 0.004625\twhole_loss: 0.034887 \n",
            "Train Epoch: 98 [96/486 (20%)]\tlearningLoss: 0.006301\twhole_loss: 0.293244 \n",
            "Train Epoch: 98 [112/486 (23%)]\tlearningLoss: 0.007701\twhole_loss: 0.245026 \n",
            "Train Epoch: 98 [128/486 (26%)]\tlearningLoss: 0.009170\twhole_loss: 0.257154 \n",
            "Train Epoch: 98 [144/486 (30%)]\tlearningLoss: 0.010558\twhole_loss: 0.242892 \n",
            "Train Epoch: 98 [160/486 (33%)]\tlearningLoss: 0.010778\twhole_loss: 0.038455 \n",
            "Train Epoch: 98 [176/486 (36%)]\tlearningLoss: 0.012230\twhole_loss: 0.254160 \n",
            "Train Epoch: 98 [192/486 (40%)]\tlearningLoss: 0.013905\twhole_loss: 0.293055 \n",
            "Train Epoch: 98 [208/486 (43%)]\tlearningLoss: 0.016566\twhole_loss: 0.465635 \n",
            "Train Epoch: 98 [224/486 (46%)]\tlearningLoss: 0.016782\twhole_loss: 0.037856 \n",
            "Train Epoch: 98 [240/486 (49%)]\tlearningLoss: 0.018107\twhole_loss: 0.231830 \n",
            "Train Epoch: 98 [256/486 (53%)]\tlearningLoss: 0.018501\twhole_loss: 0.068922 \n",
            "Train Epoch: 98 [272/486 (56%)]\tlearningLoss: 0.020467\twhole_loss: 0.344095 \n",
            "Train Epoch: 98 [288/486 (59%)]\tlearningLoss: 0.020473\twhole_loss: 0.000975 \n",
            "Train Epoch: 98 [304/486 (63%)]\tlearningLoss: 0.022563\twhole_loss: 0.365867 \n",
            "Train Epoch: 98 [320/486 (66%)]\tlearningLoss: 0.022609\twhole_loss: 0.008047 \n",
            "Train Epoch: 98 [336/486 (69%)]\tlearningLoss: 0.024535\twhole_loss: 0.336964 \n",
            "Train Epoch: 98 [352/486 (72%)]\tlearningLoss: 0.024796\twhole_loss: 0.045813 \n",
            "Train Epoch: 98 [368/486 (76%)]\tlearningLoss: 0.025502\twhole_loss: 0.123394 \n",
            "Train Epoch: 98 [384/486 (79%)]\tlearningLoss: 0.026612\twhole_loss: 0.194324 \n",
            "Train Epoch: 98 [400/486 (82%)]\tlearningLoss: 0.028799\twhole_loss: 0.382673 \n",
            "Train Epoch: 98 [416/486 (86%)]\tlearningLoss: 0.029314\twhole_loss: 0.090127 \n",
            "Train Epoch: 98 [432/486 (89%)]\tlearningLoss: 0.029652\twhole_loss: 0.059128 \n",
            "Train Epoch: 98 [448/486 (92%)]\tlearningLoss: 0.030341\twhole_loss: 0.120688 \n",
            "Train Epoch: 98 [464/486 (95%)]\tlearningLoss: 0.030827\twhole_loss: 0.084924 \n",
            "Train Epoch: 98 [180/486 (37%)]\tlearningLoss: 0.031552\twhole_loss: 0.126909 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 2, 1, 1, 1, 1, 1, 2, 0, 2, 2, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:1.9326, Accuracy: 65/75(87%)\n",
            "\n",
            "Train Epoch: 99 [0/486 (0%)]\tlearningLoss: 0.000567\twhole_loss: 0.099293 \n",
            "Train Epoch: 99 [16/486 (3%)]\tlearningLoss: 0.000935\twhole_loss: 0.064409 \n",
            "Train Epoch: 99 [32/486 (7%)]\tlearningLoss: 0.001501\twhole_loss: 0.098985 \n",
            "Train Epoch: 99 [48/486 (10%)]\tlearningLoss: 0.002187\twhole_loss: 0.120002 \n",
            "Train Epoch: 99 [64/486 (13%)]\tlearningLoss: 0.003796\twhole_loss: 0.281684 \n",
            "Train Epoch: 99 [80/486 (16%)]\tlearningLoss: 0.004228\twhole_loss: 0.075484 \n",
            "Train Epoch: 99 [96/486 (20%)]\tlearningLoss: 0.005029\twhole_loss: 0.140172 \n",
            "Train Epoch: 99 [112/486 (23%)]\tlearningLoss: 0.005241\twhole_loss: 0.037166 \n",
            "Train Epoch: 99 [128/486 (26%)]\tlearningLoss: 0.005616\twhole_loss: 0.065583 \n",
            "Train Epoch: 99 [144/486 (30%)]\tlearningLoss: 0.006064\twhole_loss: 0.078382 \n",
            "Train Epoch: 99 [160/486 (33%)]\tlearningLoss: 0.006210\twhole_loss: 0.025580 \n",
            "Train Epoch: 99 [176/486 (36%)]\tlearningLoss: 0.007194\twhole_loss: 0.172184 \n",
            "Train Epoch: 99 [192/486 (40%)]\tlearningLoss: 0.007979\twhole_loss: 0.137430 \n",
            "Train Epoch: 99 [208/486 (43%)]\tlearningLoss: 0.008283\twhole_loss: 0.053242 \n",
            "Train Epoch: 99 [224/486 (46%)]\tlearningLoss: 0.008431\twhole_loss: 0.025790 \n",
            "Train Epoch: 99 [240/486 (49%)]\tlearningLoss: 0.008795\twhole_loss: 0.063673 \n",
            "Train Epoch: 99 [256/486 (53%)]\tlearningLoss: 0.008960\twhole_loss: 0.029010 \n",
            "Train Epoch: 99 [272/486 (56%)]\tlearningLoss: 0.009057\twhole_loss: 0.016826 \n",
            "Train Epoch: 99 [288/486 (59%)]\tlearningLoss: 0.009417\twhole_loss: 0.063005 \n",
            "Train Epoch: 99 [304/486 (63%)]\tlearningLoss: 0.009615\twhole_loss: 0.034780 \n",
            "Train Epoch: 99 [320/486 (66%)]\tlearningLoss: 0.010151\twhole_loss: 0.093728 \n",
            "Train Epoch: 99 [336/486 (69%)]\tlearningLoss: 0.011118\twhole_loss: 0.169298 \n",
            "Train Epoch: 99 [352/486 (72%)]\tlearningLoss: 0.011667\twhole_loss: 0.095935 \n",
            "Train Epoch: 99 [368/486 (76%)]\tlearningLoss: 0.012354\twhole_loss: 0.120360 \n",
            "Train Epoch: 99 [384/486 (79%)]\tlearningLoss: 0.012406\twhole_loss: 0.009106 \n",
            "Train Epoch: 99 [400/486 (82%)]\tlearningLoss: 0.012588\twhole_loss: 0.031843 \n",
            "Train Epoch: 99 [416/486 (86%)]\tlearningLoss: 0.013410\twhole_loss: 0.143717 \n",
            "Train Epoch: 99 [432/486 (89%)]\tlearningLoss: 0.015110\twhole_loss: 0.297556 \n",
            "Train Epoch: 99 [448/486 (92%)]\tlearningLoss: 0.015318\twhole_loss: 0.036429 \n",
            "Train Epoch: 99 [464/486 (95%)]\tlearningLoss: 0.015363\twhole_loss: 0.007930 \n",
            "Train Epoch: 99 [180/486 (37%)]\tlearningLoss: 0.015370\twhole_loss: 0.001254 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 2, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.0969, Accuracy: 68/75(91%)\n",
            "\n",
            "Train Epoch: 100 [0/486 (0%)]\tlearningLoss: 0.001056\twhole_loss: 0.184786 \n",
            "Train Epoch: 100 [16/486 (3%)]\tlearningLoss: 0.001363\twhole_loss: 0.053763 \n",
            "Train Epoch: 100 [32/486 (7%)]\tlearningLoss: 0.001657\twhole_loss: 0.051379 \n",
            "Train Epoch: 100 [48/486 (10%)]\tlearningLoss: 0.002077\twhole_loss: 0.073588 \n",
            "Train Epoch: 100 [64/486 (13%)]\tlearningLoss: 0.002191\twhole_loss: 0.019825 \n",
            "Train Epoch: 100 [80/486 (16%)]\tlearningLoss: 0.003814\twhole_loss: 0.284166 \n",
            "Train Epoch: 100 [96/486 (20%)]\tlearningLoss: 0.004420\twhole_loss: 0.105919 \n",
            "Train Epoch: 100 [112/486 (23%)]\tlearningLoss: 0.004501\twhole_loss: 0.014248 \n",
            "Train Epoch: 100 [128/486 (26%)]\tlearningLoss: 0.004779\twhole_loss: 0.048622 \n",
            "Train Epoch: 100 [144/486 (30%)]\tlearningLoss: 0.005283\twhole_loss: 0.088259 \n",
            "Train Epoch: 100 [160/486 (33%)]\tlearningLoss: 0.005420\twhole_loss: 0.023974 \n",
            "Train Epoch: 100 [176/486 (36%)]\tlearningLoss: 0.005649\twhole_loss: 0.040105 \n",
            "Train Epoch: 100 [192/486 (40%)]\tlearningLoss: 0.005891\twhole_loss: 0.042255 \n",
            "Train Epoch: 100 [208/486 (43%)]\tlearningLoss: 0.007810\twhole_loss: 0.335797 \n",
            "Train Epoch: 100 [224/486 (46%)]\tlearningLoss: 0.008015\twhole_loss: 0.035989 \n",
            "Train Epoch: 100 [240/486 (49%)]\tlearningLoss: 0.008088\twhole_loss: 0.012802 \n",
            "Train Epoch: 100 [256/486 (53%)]\tlearningLoss: 0.009421\twhole_loss: 0.233136 \n",
            "Train Epoch: 100 [272/486 (56%)]\tlearningLoss: 0.009480\twhole_loss: 0.010368 \n",
            "Train Epoch: 100 [288/486 (59%)]\tlearningLoss: 0.009644\twhole_loss: 0.028646 \n",
            "Train Epoch: 100 [304/486 (63%)]\tlearningLoss: 0.009991\twhole_loss: 0.060797 \n",
            "Train Epoch: 100 [320/486 (66%)]\tlearningLoss: 0.010086\twhole_loss: 0.016562 \n",
            "Train Epoch: 100 [336/486 (69%)]\tlearningLoss: 0.010266\twhole_loss: 0.031531 \n",
            "Train Epoch: 100 [352/486 (72%)]\tlearningLoss: 0.010995\twhole_loss: 0.127532 \n",
            "Train Epoch: 100 [368/486 (76%)]\tlearningLoss: 0.011478\twhole_loss: 0.084532 \n",
            "Train Epoch: 100 [384/486 (79%)]\tlearningLoss: 0.012039\twhole_loss: 0.098265 \n",
            "Train Epoch: 100 [400/486 (82%)]\tlearningLoss: 0.012259\twhole_loss: 0.038475 \n",
            "Train Epoch: 100 [416/486 (86%)]\tlearningLoss: 0.012690\twhole_loss: 0.075470 \n",
            "Train Epoch: 100 [432/486 (89%)]\tlearningLoss: 0.013057\twhole_loss: 0.064128 \n",
            "Train Epoch: 100 [448/486 (92%)]\tlearningLoss: 0.013255\twhole_loss: 0.034649 \n",
            "Train Epoch: 100 [464/486 (95%)]\tlearningLoss: 0.013378\twhole_loss: 0.021514 \n",
            "Train Epoch: 100 [180/486 (37%)]\tlearningLoss: 0.013693\twhole_loss: 0.055172 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 2, 1, 1, 1, 1, 2, 0, 2, 2, 1, 1, 2, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.4005, Accuracy: 62/75(83%)\n",
            "\n",
            "Train Epoch: 101 [0/486 (0%)]\tlearningLoss: 0.000014\twhole_loss: 0.002385 \n",
            "Train Epoch: 101 [16/486 (3%)]\tlearningLoss: 0.000716\twhole_loss: 0.122925 \n",
            "Train Epoch: 101 [32/486 (7%)]\tlearningLoss: 0.000743\twhole_loss: 0.004671 \n",
            "Train Epoch: 101 [48/486 (10%)]\tlearningLoss: 0.000939\twhole_loss: 0.034272 \n",
            "Train Epoch: 101 [64/486 (13%)]\tlearningLoss: 0.000990\twhole_loss: 0.009077 \n",
            "Train Epoch: 101 [80/486 (16%)]\tlearningLoss: 0.001451\twhole_loss: 0.080631 \n",
            "Train Epoch: 101 [96/486 (20%)]\tlearningLoss: 0.001932\twhole_loss: 0.084142 \n",
            "Train Epoch: 101 [112/486 (23%)]\tlearningLoss: 0.002190\twhole_loss: 0.045185 \n",
            "Train Epoch: 101 [128/486 (26%)]\tlearningLoss: 0.002310\twhole_loss: 0.021030 \n",
            "Train Epoch: 101 [144/486 (30%)]\tlearningLoss: 0.002385\twhole_loss: 0.013097 \n",
            "Train Epoch: 101 [160/486 (33%)]\tlearningLoss: 0.002606\twhole_loss: 0.038708 \n",
            "Train Epoch: 101 [176/486 (36%)]\tlearningLoss: 0.002750\twhole_loss: 0.025070 \n",
            "Train Epoch: 101 [192/486 (40%)]\tlearningLoss: 0.002762\twhole_loss: 0.002226 \n",
            "Train Epoch: 101 [208/486 (43%)]\tlearningLoss: 0.003210\twhole_loss: 0.078277 \n",
            "Train Epoch: 101 [224/486 (46%)]\tlearningLoss: 0.003468\twhole_loss: 0.045186 \n",
            "Train Epoch: 101 [240/486 (49%)]\tlearningLoss: 0.003518\twhole_loss: 0.008691 \n",
            "Train Epoch: 101 [256/486 (53%)]\tlearningLoss: 0.003642\twhole_loss: 0.021708 \n",
            "Train Epoch: 101 [272/486 (56%)]\tlearningLoss: 0.003725\twhole_loss: 0.014528 \n",
            "Train Epoch: 101 [288/486 (59%)]\tlearningLoss: 0.004239\twhole_loss: 0.089942 \n",
            "Train Epoch: 101 [304/486 (63%)]\tlearningLoss: 0.004761\twhole_loss: 0.091472 \n",
            "Train Epoch: 101 [320/486 (66%)]\tlearningLoss: 0.004973\twhole_loss: 0.037027 \n",
            "Train Epoch: 101 [336/486 (69%)]\tlearningLoss: 0.005947\twhole_loss: 0.170549 \n",
            "Train Epoch: 101 [352/486 (72%)]\tlearningLoss: 0.006166\twhole_loss: 0.038201 \n",
            "Train Epoch: 101 [368/486 (76%)]\tlearningLoss: 0.006240\twhole_loss: 0.013008 \n",
            "Train Epoch: 101 [384/486 (79%)]\tlearningLoss: 0.006290\twhole_loss: 0.008716 \n",
            "Train Epoch: 101 [400/486 (82%)]\tlearningLoss: 0.006807\twhole_loss: 0.090570 \n",
            "Train Epoch: 101 [416/486 (86%)]\tlearningLoss: 0.006916\twhole_loss: 0.018985 \n",
            "Train Epoch: 101 [432/486 (89%)]\tlearningLoss: 0.006950\twhole_loss: 0.006014 \n",
            "Train Epoch: 101 [448/486 (92%)]\tlearningLoss: 0.007489\twhole_loss: 0.094236 \n",
            "Train Epoch: 101 [464/486 (95%)]\tlearningLoss: 0.007571\twhole_loss: 0.014388 \n",
            "Train Epoch: 101 [180/486 (37%)]\tlearningLoss: 0.007579\twhole_loss: 0.001396 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 2, 1, 1, 1, 1, 2, 0, 2, 2, 2, 1, 0, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.7302, Accuracy: 63/75(84%)\n",
            "\n",
            "Train Epoch: 102 [0/486 (0%)]\tlearningLoss: 0.000262\twhole_loss: 0.045832 \n",
            "Train Epoch: 102 [16/486 (3%)]\tlearningLoss: 0.000394\twhole_loss: 0.023067 \n",
            "Train Epoch: 102 [32/486 (7%)]\tlearningLoss: 0.000465\twhole_loss: 0.012470 \n",
            "Train Epoch: 102 [48/486 (10%)]\tlearningLoss: 0.002177\twhole_loss: 0.299685 \n",
            "Train Epoch: 102 [64/486 (13%)]\tlearningLoss: 0.002725\twhole_loss: 0.095871 \n",
            "Train Epoch: 102 [80/486 (16%)]\tlearningLoss: 0.002980\twhole_loss: 0.044495 \n",
            "Train Epoch: 102 [96/486 (20%)]\tlearningLoss: 0.003171\twhole_loss: 0.033500 \n",
            "Train Epoch: 102 [112/486 (23%)]\tlearningLoss: 0.003367\twhole_loss: 0.034380 \n",
            "Train Epoch: 102 [128/486 (26%)]\tlearningLoss: 0.003380\twhole_loss: 0.002269 \n",
            "Train Epoch: 102 [144/486 (30%)]\tlearningLoss: 0.003386\twhole_loss: 0.001009 \n",
            "Train Epoch: 102 [160/486 (33%)]\tlearningLoss: 0.004825\twhole_loss: 0.251832 \n",
            "Train Epoch: 102 [176/486 (36%)]\tlearningLoss: 0.005265\twhole_loss: 0.076901 \n",
            "Train Epoch: 102 [192/486 (40%)]\tlearningLoss: 0.005413\twhole_loss: 0.025950 \n",
            "Train Epoch: 102 [208/486 (43%)]\tlearningLoss: 0.005834\twhole_loss: 0.073768 \n",
            "Train Epoch: 102 [224/486 (46%)]\tlearningLoss: 0.005838\twhole_loss: 0.000633 \n",
            "Train Epoch: 102 [240/486 (49%)]\tlearningLoss: 0.005885\twhole_loss: 0.008163 \n",
            "Train Epoch: 102 [256/486 (53%)]\tlearningLoss: 0.007187\twhole_loss: 0.227822 \n",
            "Train Epoch: 102 [272/486 (56%)]\tlearningLoss: 0.007222\twhole_loss: 0.006271 \n",
            "Train Epoch: 102 [288/486 (59%)]\tlearningLoss: 0.007237\twhole_loss: 0.002584 \n",
            "Train Epoch: 102 [304/486 (63%)]\tlearningLoss: 0.007460\twhole_loss: 0.039017 \n",
            "Train Epoch: 102 [320/486 (66%)]\tlearningLoss: 0.007528\twhole_loss: 0.011896 \n",
            "Train Epoch: 102 [336/486 (69%)]\tlearningLoss: 0.008216\twhole_loss: 0.120311 \n",
            "Train Epoch: 102 [352/486 (72%)]\tlearningLoss: 0.009615\twhole_loss: 0.244967 \n",
            "Train Epoch: 102 [368/486 (76%)]\tlearningLoss: 0.009857\twhole_loss: 0.042231 \n",
            "Train Epoch: 102 [384/486 (79%)]\tlearningLoss: 0.010126\twhole_loss: 0.047103 \n",
            "Train Epoch: 102 [400/486 (82%)]\tlearningLoss: 0.010192\twhole_loss: 0.011549 \n",
            "Train Epoch: 102 [416/486 (86%)]\tlearningLoss: 0.010225\twhole_loss: 0.005731 \n",
            "Train Epoch: 102 [432/486 (89%)]\tlearningLoss: 0.010241\twhole_loss: 0.002868 \n",
            "Train Epoch: 102 [448/486 (92%)]\tlearningLoss: 0.010756\twhole_loss: 0.090118 \n",
            "Train Epoch: 102 [464/486 (95%)]\tlearningLoss: 0.011117\twhole_loss: 0.063230 \n",
            "Train Epoch: 102 [180/486 (37%)]\tlearningLoss: 0.011209\twhole_loss: 0.016049 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 2, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.7371, Accuracy: 67/75(89%)\n",
            "\n",
            "Train Epoch: 103 [0/486 (0%)]\tlearningLoss: 0.000006\twhole_loss: 0.000968 \n",
            "Train Epoch: 103 [16/486 (3%)]\tlearningLoss: 0.000471\twhole_loss: 0.081495 \n",
            "Train Epoch: 103 [32/486 (7%)]\tlearningLoss: 0.000756\twhole_loss: 0.049853 \n",
            "Train Epoch: 103 [48/486 (10%)]\tlearningLoss: 0.000895\twhole_loss: 0.024256 \n",
            "Train Epoch: 103 [64/486 (13%)]\tlearningLoss: 0.001173\twhole_loss: 0.048637 \n",
            "Train Epoch: 103 [80/486 (16%)]\tlearningLoss: 0.001376\twhole_loss: 0.035628 \n",
            "Train Epoch: 103 [96/486 (20%)]\tlearningLoss: 0.001458\twhole_loss: 0.014260 \n",
            "Train Epoch: 103 [112/486 (23%)]\tlearningLoss: 0.001970\twhole_loss: 0.089615 \n",
            "Train Epoch: 103 [128/486 (26%)]\tlearningLoss: 0.002243\twhole_loss: 0.047834 \n",
            "Train Epoch: 103 [144/486 (30%)]\tlearningLoss: 0.002879\twhole_loss: 0.111222 \n",
            "Train Epoch: 103 [160/486 (33%)]\tlearningLoss: 0.005842\twhole_loss: 0.518570 \n",
            "Train Epoch: 103 [176/486 (36%)]\tlearningLoss: 0.006920\twhole_loss: 0.188746 \n",
            "Train Epoch: 103 [192/486 (40%)]\tlearningLoss: 0.007075\twhole_loss: 0.027086 \n",
            "Train Epoch: 103 [208/486 (43%)]\tlearningLoss: 0.007349\twhole_loss: 0.047954 \n",
            "Train Epoch: 103 [224/486 (46%)]\tlearningLoss: 0.007724\twhole_loss: 0.065517 \n",
            "Train Epoch: 103 [240/486 (49%)]\tlearningLoss: 0.008115\twhole_loss: 0.068532 \n",
            "Train Epoch: 103 [256/486 (53%)]\tlearningLoss: 0.008311\twhole_loss: 0.034236 \n",
            "Train Epoch: 103 [272/486 (56%)]\tlearningLoss: 0.008823\twhole_loss: 0.089644 \n",
            "Train Epoch: 103 [288/486 (59%)]\tlearningLoss: 0.009166\twhole_loss: 0.059954 \n",
            "Train Epoch: 103 [304/486 (63%)]\tlearningLoss: 0.009226\twhole_loss: 0.010487 \n",
            "Train Epoch: 103 [320/486 (66%)]\tlearningLoss: 0.009385\twhole_loss: 0.027948 \n",
            "Train Epoch: 103 [336/486 (69%)]\tlearningLoss: 0.009531\twhole_loss: 0.025515 \n",
            "Train Epoch: 103 [352/486 (72%)]\tlearningLoss: 0.009921\twhole_loss: 0.068242 \n",
            "Train Epoch: 103 [368/486 (76%)]\tlearningLoss: 0.009990\twhole_loss: 0.012019 \n",
            "Train Epoch: 103 [384/486 (79%)]\tlearningLoss: 0.010027\twhole_loss: 0.006478 \n",
            "Train Epoch: 103 [400/486 (82%)]\tlearningLoss: 0.011139\twhole_loss: 0.194601 \n",
            "Train Epoch: 103 [416/486 (86%)]\tlearningLoss: 0.011604\twhole_loss: 0.081398 \n",
            "Train Epoch: 103 [432/486 (89%)]\tlearningLoss: 0.011973\twhole_loss: 0.064545 \n",
            "Train Epoch: 103 [448/486 (92%)]\tlearningLoss: 0.011974\twhole_loss: 0.000253 \n",
            "Train Epoch: 103 [464/486 (95%)]\tlearningLoss: 0.012139\twhole_loss: 0.028798 \n",
            "Train Epoch: 103 [180/486 (37%)]\tlearningLoss: 0.012241\twhole_loss: 0.017932 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 2, 2, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:5.4563, Accuracy: 62/75(83%)\n",
            "\n",
            "Train Epoch: 104 [0/486 (0%)]\tlearningLoss: 0.001538\twhole_loss: 0.269104 \n",
            "Train Epoch: 104 [16/486 (3%)]\tlearningLoss: 0.001567\twhole_loss: 0.005149 \n",
            "Train Epoch: 104 [32/486 (7%)]\tlearningLoss: 0.002080\twhole_loss: 0.089714 \n",
            "Train Epoch: 104 [48/486 (10%)]\tlearningLoss: 0.002247\twhole_loss: 0.029218 \n",
            "Train Epoch: 104 [64/486 (13%)]\tlearningLoss: 0.002336\twhole_loss: 0.015530 \n",
            "Train Epoch: 104 [80/486 (16%)]\tlearningLoss: 0.002386\twhole_loss: 0.008758 \n",
            "Train Epoch: 104 [96/486 (20%)]\tlearningLoss: 0.003027\twhole_loss: 0.112253 \n",
            "Train Epoch: 104 [112/486 (23%)]\tlearningLoss: 0.003584\twhole_loss: 0.097560 \n",
            "Train Epoch: 104 [128/486 (26%)]\tlearningLoss: 0.004821\twhole_loss: 0.216449 \n",
            "Train Epoch: 104 [144/486 (30%)]\tlearningLoss: 0.004952\twhole_loss: 0.022813 \n",
            "Train Epoch: 104 [160/486 (33%)]\tlearningLoss: 0.004994\twhole_loss: 0.007357 \n",
            "Train Epoch: 104 [176/486 (36%)]\tlearningLoss: 0.005041\twhole_loss: 0.008247 \n",
            "Train Epoch: 104 [192/486 (40%)]\tlearningLoss: 0.005222\twhole_loss: 0.031711 \n",
            "Train Epoch: 104 [208/486 (43%)]\tlearningLoss: 0.005251\twhole_loss: 0.005083 \n",
            "Train Epoch: 104 [224/486 (46%)]\tlearningLoss: 0.006205\twhole_loss: 0.166890 \n",
            "Train Epoch: 104 [240/486 (49%)]\tlearningLoss: 0.006483\twhole_loss: 0.048631 \n",
            "Train Epoch: 104 [256/486 (53%)]\tlearningLoss: 0.007317\twhole_loss: 0.145924 \n",
            "Train Epoch: 104 [272/486 (56%)]\tlearningLoss: 0.011904\twhole_loss: 0.802757 \n",
            "Train Epoch: 104 [288/486 (59%)]\tlearningLoss: 0.012074\twhole_loss: 0.029856 \n",
            "Train Epoch: 104 [304/486 (63%)]\tlearningLoss: 0.012952\twhole_loss: 0.153546 \n",
            "Train Epoch: 104 [320/486 (66%)]\tlearningLoss: 0.012968\twhole_loss: 0.002909 \n",
            "Train Epoch: 104 [336/486 (69%)]\tlearningLoss: 0.013392\twhole_loss: 0.074191 \n",
            "Train Epoch: 104 [352/486 (72%)]\tlearningLoss: 0.014258\twhole_loss: 0.151454 \n",
            "Train Epoch: 104 [368/486 (76%)]\tlearningLoss: 0.016281\twhole_loss: 0.354102 \n",
            "Train Epoch: 104 [384/486 (79%)]\tlearningLoss: 0.017328\twhole_loss: 0.183171 \n",
            "Train Epoch: 104 [400/486 (82%)]\tlearningLoss: 0.017458\twhole_loss: 0.022698 \n",
            "Train Epoch: 104 [416/486 (86%)]\tlearningLoss: 0.017461\twhole_loss: 0.000683 \n",
            "Train Epoch: 104 [432/486 (89%)]\tlearningLoss: 0.017712\twhole_loss: 0.043807 \n",
            "Train Epoch: 104 [448/486 (92%)]\tlearningLoss: 0.017965\twhole_loss: 0.044373 \n",
            "Train Epoch: 104 [464/486 (95%)]\tlearningLoss: 0.018144\twhole_loss: 0.031305 \n",
            "Train Epoch: 104 [180/486 (37%)]\tlearningLoss: 0.018898\twhole_loss: 0.131981 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 2, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.0239, Accuracy: 65/75(87%)\n",
            "\n",
            "Train Epoch: 105 [0/486 (0%)]\tlearningLoss: 0.000143\twhole_loss: 0.024983 \n",
            "Train Epoch: 105 [16/486 (3%)]\tlearningLoss: 0.000249\twhole_loss: 0.018660 \n",
            "Train Epoch: 105 [32/486 (7%)]\tlearningLoss: 0.000352\twhole_loss: 0.018035 \n",
            "Train Epoch: 105 [48/486 (10%)]\tlearningLoss: 0.000940\twhole_loss: 0.102868 \n",
            "Train Epoch: 105 [64/486 (13%)]\tlearningLoss: 0.000956\twhole_loss: 0.002768 \n",
            "Train Epoch: 105 [80/486 (16%)]\tlearningLoss: 0.001091\twhole_loss: 0.023546 \n",
            "Train Epoch: 105 [96/486 (20%)]\tlearningLoss: 0.001429\twhole_loss: 0.059142 \n",
            "Train Epoch: 105 [112/486 (23%)]\tlearningLoss: 0.001555\twhole_loss: 0.022177 \n",
            "Train Epoch: 105 [128/486 (26%)]\tlearningLoss: 0.001558\twhole_loss: 0.000515 \n",
            "Train Epoch: 105 [144/486 (30%)]\tlearningLoss: 0.001842\twhole_loss: 0.049715 \n",
            "Train Epoch: 105 [160/486 (33%)]\tlearningLoss: 0.002390\twhole_loss: 0.095883 \n",
            "Train Epoch: 105 [176/486 (36%)]\tlearningLoss: 0.002455\twhole_loss: 0.011363 \n",
            "Train Epoch: 105 [192/486 (40%)]\tlearningLoss: 0.002664\twhole_loss: 0.036530 \n",
            "Train Epoch: 105 [208/486 (43%)]\tlearningLoss: 0.002700\twhole_loss: 0.006376 \n",
            "Train Epoch: 105 [224/486 (46%)]\tlearningLoss: 0.002836\twhole_loss: 0.023681 \n",
            "Train Epoch: 105 [240/486 (49%)]\tlearningLoss: 0.002840\twhole_loss: 0.000743 \n",
            "Train Epoch: 105 [256/486 (53%)]\tlearningLoss: 0.002924\twhole_loss: 0.014790 \n",
            "Train Epoch: 105 [272/486 (56%)]\tlearningLoss: 0.004102\twhole_loss: 0.206150 \n",
            "Train Epoch: 105 [288/486 (59%)]\tlearningLoss: 0.004118\twhole_loss: 0.002775 \n",
            "Train Epoch: 105 [304/486 (63%)]\tlearningLoss: 0.004265\twhole_loss: 0.025737 \n",
            "Train Epoch: 105 [320/486 (66%)]\tlearningLoss: 0.004369\twhole_loss: 0.018053 \n",
            "Train Epoch: 105 [336/486 (69%)]\tlearningLoss: 0.004515\twhole_loss: 0.025617 \n",
            "Train Epoch: 105 [352/486 (72%)]\tlearningLoss: 0.004519\twhole_loss: 0.000710 \n",
            "Train Epoch: 105 [368/486 (76%)]\tlearningLoss: 0.004776\twhole_loss: 0.044905 \n",
            "Train Epoch: 105 [384/486 (79%)]\tlearningLoss: 0.004784\twhole_loss: 0.001512 \n",
            "Train Epoch: 105 [400/486 (82%)]\tlearningLoss: 0.004815\twhole_loss: 0.005346 \n",
            "Train Epoch: 105 [416/486 (86%)]\tlearningLoss: 0.004874\twhole_loss: 0.010359 \n",
            "Train Epoch: 105 [432/486 (89%)]\tlearningLoss: 0.005030\twhole_loss: 0.027286 \n",
            "Train Epoch: 105 [448/486 (92%)]\tlearningLoss: 0.005562\twhole_loss: 0.093116 \n",
            "Train Epoch: 105 [464/486 (95%)]\tlearningLoss: 0.005641\twhole_loss: 0.013889 \n",
            "Train Epoch: 105 [180/486 (37%)]\tlearningLoss: 0.006114\twhole_loss: 0.082687 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 2, 1, 1, 1, 1, 1, 2, 0, 1, 2, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.4407, Accuracy: 65/75(87%)\n",
            "\n",
            "Train Epoch: 106 [0/486 (0%)]\tlearningLoss: 0.000058\twhole_loss: 0.010150 \n",
            "Train Epoch: 106 [16/486 (3%)]\tlearningLoss: 0.000083\twhole_loss: 0.004376 \n",
            "Train Epoch: 106 [32/486 (7%)]\tlearningLoss: 0.000171\twhole_loss: 0.015434 \n",
            "Train Epoch: 106 [48/486 (10%)]\tlearningLoss: 0.000378\twhole_loss: 0.036112 \n",
            "Train Epoch: 106 [64/486 (13%)]\tlearningLoss: 0.000494\twhole_loss: 0.020423 \n",
            "Train Epoch: 106 [80/486 (16%)]\tlearningLoss: 0.000544\twhole_loss: 0.008787 \n",
            "Train Epoch: 106 [96/486 (20%)]\tlearningLoss: 0.000583\twhole_loss: 0.006678 \n",
            "Train Epoch: 106 [112/486 (23%)]\tlearningLoss: 0.001248\twhole_loss: 0.116394 \n",
            "Train Epoch: 106 [128/486 (26%)]\tlearningLoss: 0.001534\twhole_loss: 0.050147 \n",
            "Train Epoch: 106 [144/486 (30%)]\tlearningLoss: 0.001619\twhole_loss: 0.014817 \n",
            "Train Epoch: 106 [160/486 (33%)]\tlearningLoss: 0.001643\twhole_loss: 0.004289 \n",
            "Train Epoch: 106 [176/486 (36%)]\tlearningLoss: 0.001919\twhole_loss: 0.048214 \n",
            "Train Epoch: 106 [192/486 (40%)]\tlearningLoss: 0.001957\twhole_loss: 0.006703 \n",
            "Train Epoch: 106 [208/486 (43%)]\tlearningLoss: 0.001976\twhole_loss: 0.003211 \n",
            "Train Epoch: 106 [224/486 (46%)]\tlearningLoss: 0.002455\twhole_loss: 0.083924 \n",
            "Train Epoch: 106 [240/486 (49%)]\tlearningLoss: 0.002890\twhole_loss: 0.076115 \n",
            "Train Epoch: 106 [256/486 (53%)]\tlearningLoss: 0.003172\twhole_loss: 0.049264 \n",
            "Train Epoch: 106 [272/486 (56%)]\tlearningLoss: 0.003421\twhole_loss: 0.043592 \n",
            "Train Epoch: 106 [288/486 (59%)]\tlearningLoss: 0.003456\twhole_loss: 0.006146 \n",
            "Train Epoch: 106 [304/486 (63%)]\tlearningLoss: 0.003481\twhole_loss: 0.004379 \n",
            "Train Epoch: 106 [320/486 (66%)]\tlearningLoss: 0.003641\twhole_loss: 0.028072 \n",
            "Train Epoch: 106 [336/486 (69%)]\tlearningLoss: 0.003778\twhole_loss: 0.023952 \n",
            "Train Epoch: 106 [352/486 (72%)]\tlearningLoss: 0.005588\twhole_loss: 0.316647 \n",
            "Train Epoch: 106 [368/486 (76%)]\tlearningLoss: 0.005601\twhole_loss: 0.002279 \n",
            "Train Epoch: 106 [384/486 (79%)]\tlearningLoss: 0.005999\twhole_loss: 0.069794 \n",
            "Train Epoch: 106 [400/486 (82%)]\tlearningLoss: 0.006096\twhole_loss: 0.016889 \n",
            "Train Epoch: 106 [416/486 (86%)]\tlearningLoss: 0.010695\twhole_loss: 0.804885 \n",
            "Train Epoch: 106 [432/486 (89%)]\tlearningLoss: 0.010720\twhole_loss: 0.004328 \n",
            "Train Epoch: 106 [448/486 (92%)]\tlearningLoss: 0.011056\twhole_loss: 0.058831 \n",
            "Train Epoch: 106 [464/486 (95%)]\tlearningLoss: 0.011542\twhole_loss: 0.084990 \n",
            "Train Epoch: 106 [180/486 (37%)]\tlearningLoss: 0.011621\twhole_loss: 0.013893 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 2, 1, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.6725, Accuracy: 69/75(92%)\n",
            "\n",
            "Train Epoch: 107 [0/486 (0%)]\tlearningLoss: 0.000104\twhole_loss: 0.018192 \n",
            "Train Epoch: 107 [16/486 (3%)]\tlearningLoss: 0.001647\twhole_loss: 0.270063 \n",
            "Train Epoch: 107 [32/486 (7%)]\tlearningLoss: 0.001737\twhole_loss: 0.015750 \n",
            "Train Epoch: 107 [48/486 (10%)]\tlearningLoss: 0.001938\twhole_loss: 0.035231 \n",
            "Train Epoch: 107 [64/486 (13%)]\tlearningLoss: 0.002650\twhole_loss: 0.124531 \n",
            "Train Epoch: 107 [80/486 (16%)]\tlearningLoss: 0.002719\twhole_loss: 0.012000 \n",
            "Train Epoch: 107 [96/486 (20%)]\tlearningLoss: 0.005776\twhole_loss: 0.534959 \n",
            "Train Epoch: 107 [112/486 (23%)]\tlearningLoss: 0.007354\twhole_loss: 0.276268 \n",
            "Train Epoch: 107 [128/486 (26%)]\tlearningLoss: 0.008362\twhole_loss: 0.176410 \n",
            "Train Epoch: 107 [144/486 (30%)]\tlearningLoss: 0.010162\twhole_loss: 0.315018 \n",
            "Train Epoch: 107 [160/486 (33%)]\tlearningLoss: 0.010474\twhole_loss: 0.054446 \n",
            "Train Epoch: 107 [176/486 (36%)]\tlearningLoss: 0.010475\twhole_loss: 0.000212 \n",
            "Train Epoch: 107 [192/486 (40%)]\tlearningLoss: 0.010612\twhole_loss: 0.023943 \n",
            "Train Epoch: 107 [208/486 (43%)]\tlearningLoss: 0.010852\twhole_loss: 0.042064 \n",
            "Train Epoch: 107 [224/486 (46%)]\tlearningLoss: 0.011652\twhole_loss: 0.140062 \n",
            "Train Epoch: 107 [240/486 (49%)]\tlearningLoss: 0.011817\twhole_loss: 0.028772 \n",
            "Train Epoch: 107 [256/486 (53%)]\tlearningLoss: 0.021788\twhole_loss: 1.745058 \n",
            "Train Epoch: 107 [272/486 (56%)]\tlearningLoss: 0.022257\twhole_loss: 0.081956 \n",
            "Train Epoch: 107 [288/486 (59%)]\tlearningLoss: 0.022718\twhole_loss: 0.080768 \n",
            "Train Epoch: 107 [304/486 (63%)]\tlearningLoss: 0.023578\twhole_loss: 0.150515 \n",
            "Train Epoch: 107 [320/486 (66%)]\tlearningLoss: 0.024722\twhole_loss: 0.200072 \n",
            "Train Epoch: 107 [336/486 (69%)]\tlearningLoss: 0.024969\twhole_loss: 0.043210 \n",
            "Train Epoch: 107 [352/486 (72%)]\tlearningLoss: 0.026562\twhole_loss: 0.278916 \n",
            "Train Epoch: 107 [368/486 (76%)]\tlearningLoss: 0.027209\twhole_loss: 0.113172 \n",
            "Train Epoch: 107 [384/486 (79%)]\tlearningLoss: 0.027260\twhole_loss: 0.008868 \n",
            "Train Epoch: 107 [400/486 (82%)]\tlearningLoss: 0.027473\twhole_loss: 0.037344 \n",
            "Train Epoch: 107 [416/486 (86%)]\tlearningLoss: 0.027698\twhole_loss: 0.039337 \n",
            "Train Epoch: 107 [432/486 (89%)]\tlearningLoss: 0.028203\twhole_loss: 0.088446 \n",
            "Train Epoch: 107 [448/486 (92%)]\tlearningLoss: 0.028271\twhole_loss: 0.011890 \n",
            "Train Epoch: 107 [464/486 (95%)]\tlearningLoss: 0.028804\twhole_loss: 0.093271 \n",
            "Train Epoch: 107 [180/486 (37%)]\tlearningLoss: 0.028931\twhole_loss: 0.022157 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 2, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.4936, Accuracy: 63/75(84%)\n",
            "\n",
            "Train Epoch: 108 [0/486 (0%)]\tlearningLoss: 0.002914\twhole_loss: 0.509933 \n",
            "Train Epoch: 108 [16/486 (3%)]\tlearningLoss: 0.002950\twhole_loss: 0.006393 \n",
            "Train Epoch: 108 [32/486 (7%)]\tlearningLoss: 0.002989\twhole_loss: 0.006816 \n",
            "Train Epoch: 108 [48/486 (10%)]\tlearningLoss: 0.003468\twhole_loss: 0.083780 \n",
            "Train Epoch: 108 [64/486 (13%)]\tlearningLoss: 0.004553\twhole_loss: 0.189791 \n",
            "Train Epoch: 108 [80/486 (16%)]\tlearningLoss: 0.004937\twhole_loss: 0.067303 \n",
            "Train Epoch: 108 [96/486 (20%)]\tlearningLoss: 0.005053\twhole_loss: 0.020182 \n",
            "Train Epoch: 108 [112/486 (23%)]\tlearningLoss: 0.005361\twhole_loss: 0.053931 \n",
            "Train Epoch: 108 [128/486 (26%)]\tlearningLoss: 0.006985\twhole_loss: 0.284276 \n",
            "Train Epoch: 108 [144/486 (30%)]\tlearningLoss: 0.007085\twhole_loss: 0.017478 \n",
            "Train Epoch: 108 [160/486 (33%)]\tlearningLoss: 0.007573\twhole_loss: 0.085446 \n",
            "Train Epoch: 108 [176/486 (36%)]\tlearningLoss: 0.007701\twhole_loss: 0.022385 \n",
            "Train Epoch: 108 [192/486 (40%)]\tlearningLoss: 0.007972\twhole_loss: 0.047454 \n",
            "Train Epoch: 108 [208/486 (43%)]\tlearningLoss: 0.008056\twhole_loss: 0.014604 \n",
            "Train Epoch: 108 [224/486 (46%)]\tlearningLoss: 0.008103\twhole_loss: 0.008236 \n",
            "Train Epoch: 108 [240/486 (49%)]\tlearningLoss: 0.008441\twhole_loss: 0.059203 \n",
            "Train Epoch: 108 [256/486 (53%)]\tlearningLoss: 0.010097\twhole_loss: 0.289841 \n",
            "Train Epoch: 108 [272/486 (56%)]\tlearningLoss: 0.010574\twhole_loss: 0.083410 \n",
            "Train Epoch: 108 [288/486 (59%)]\tlearningLoss: 0.010747\twhole_loss: 0.030313 \n",
            "Train Epoch: 108 [304/486 (63%)]\tlearningLoss: 0.010932\twhole_loss: 0.032390 \n",
            "Train Epoch: 108 [320/486 (66%)]\tlearningLoss: 0.011798\twhole_loss: 0.151533 \n",
            "Train Epoch: 108 [336/486 (69%)]\tlearningLoss: 0.012856\twhole_loss: 0.185086 \n",
            "Train Epoch: 108 [352/486 (72%)]\tlearningLoss: 0.014034\twhole_loss: 0.206171 \n",
            "Train Epoch: 108 [368/486 (76%)]\tlearningLoss: 0.014869\twhole_loss: 0.146051 \n",
            "Train Epoch: 108 [384/486 (79%)]\tlearningLoss: 0.014936\twhole_loss: 0.011750 \n",
            "Train Epoch: 108 [400/486 (82%)]\tlearningLoss: 0.014990\twhole_loss: 0.009421 \n",
            "Train Epoch: 108 [416/486 (86%)]\tlearningLoss: 0.015021\twhole_loss: 0.005568 \n",
            "Train Epoch: 108 [432/486 (89%)]\tlearningLoss: 0.015229\twhole_loss: 0.036287 \n",
            "Train Epoch: 108 [448/486 (92%)]\tlearningLoss: 0.016913\twhole_loss: 0.294793 \n",
            "Train Epoch: 108 [464/486 (95%)]\tlearningLoss: 0.017192\twhole_loss: 0.048859 \n",
            "Train Epoch: 108 [180/486 (37%)]\tlearningLoss: 0.017232\twhole_loss: 0.006974 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 2, 1, 1, 1, 1, 2, 0, 1, 2, 2, 1, 2, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.1823, Accuracy: 63/75(84%)\n",
            "\n",
            "Train Epoch: 109 [0/486 (0%)]\tlearningLoss: 0.000899\twhole_loss: 0.157341 \n",
            "Train Epoch: 109 [16/486 (3%)]\tlearningLoss: 0.001291\twhole_loss: 0.068511 \n",
            "Train Epoch: 109 [32/486 (7%)]\tlearningLoss: 0.001382\twhole_loss: 0.016004 \n",
            "Train Epoch: 109 [48/486 (10%)]\tlearningLoss: 0.001857\twhole_loss: 0.083197 \n",
            "Train Epoch: 109 [64/486 (13%)]\tlearningLoss: 0.001927\twhole_loss: 0.012160 \n",
            "Train Epoch: 109 [80/486 (16%)]\tlearningLoss: 0.002003\twhole_loss: 0.013277 \n",
            "Train Epoch: 109 [96/486 (20%)]\tlearningLoss: 0.003341\twhole_loss: 0.234270 \n",
            "Train Epoch: 109 [112/486 (23%)]\tlearningLoss: 0.003928\twhole_loss: 0.102576 \n",
            "Train Epoch: 109 [128/486 (26%)]\tlearningLoss: 0.004239\twhole_loss: 0.054503 \n",
            "Train Epoch: 109 [144/486 (30%)]\tlearningLoss: 0.004324\twhole_loss: 0.014916 \n",
            "Train Epoch: 109 [160/486 (33%)]\tlearningLoss: 0.004424\twhole_loss: 0.017473 \n",
            "Train Epoch: 109 [176/486 (36%)]\tlearningLoss: 0.004611\twhole_loss: 0.032779 \n",
            "Train Epoch: 109 [192/486 (40%)]\tlearningLoss: 0.004695\twhole_loss: 0.014620 \n",
            "Train Epoch: 109 [208/486 (43%)]\tlearningLoss: 0.004868\twhole_loss: 0.030259 \n",
            "Train Epoch: 109 [224/486 (46%)]\tlearningLoss: 0.005020\twhole_loss: 0.026552 \n",
            "Train Epoch: 109 [240/486 (49%)]\tlearningLoss: 0.005214\twhole_loss: 0.034040 \n",
            "Train Epoch: 109 [256/486 (53%)]\tlearningLoss: 0.005774\twhole_loss: 0.098042 \n",
            "Train Epoch: 109 [272/486 (56%)]\tlearningLoss: 0.006783\twhole_loss: 0.176549 \n",
            "Train Epoch: 109 [288/486 (59%)]\tlearningLoss: 0.007033\twhole_loss: 0.043744 \n",
            "Train Epoch: 109 [304/486 (63%)]\tlearningLoss: 0.007177\twhole_loss: 0.025195 \n",
            "Train Epoch: 109 [320/486 (66%)]\tlearningLoss: 0.007362\twhole_loss: 0.032349 \n",
            "Train Epoch: 109 [336/486 (69%)]\tlearningLoss: 0.007433\twhole_loss: 0.012396 \n",
            "Train Epoch: 109 [352/486 (72%)]\tlearningLoss: 0.007889\twhole_loss: 0.079847 \n",
            "Train Epoch: 109 [368/486 (76%)]\tlearningLoss: 0.009595\twhole_loss: 0.298523 \n",
            "Train Epoch: 109 [384/486 (79%)]\tlearningLoss: 0.009733\twhole_loss: 0.024145 \n",
            "Train Epoch: 109 [400/486 (82%)]\tlearningLoss: 0.009799\twhole_loss: 0.011591 \n",
            "Train Epoch: 109 [416/486 (86%)]\tlearningLoss: 0.009955\twhole_loss: 0.027312 \n",
            "Train Epoch: 109 [432/486 (89%)]\tlearningLoss: 0.010061\twhole_loss: 0.018562 \n",
            "Train Epoch: 109 [448/486 (92%)]\tlearningLoss: 0.010127\twhole_loss: 0.011449 \n",
            "Train Epoch: 109 [464/486 (95%)]\tlearningLoss: 0.010152\twhole_loss: 0.004349 \n",
            "Train Epoch: 109 [180/486 (37%)]\tlearningLoss: 0.010491\twhole_loss: 0.059420 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.5306, Accuracy: 69/75(92%)\n",
            "\n",
            "Train Epoch: 110 [0/486 (0%)]\tlearningLoss: 0.000030\twhole_loss: 0.005300 \n",
            "Train Epoch: 110 [16/486 (3%)]\tlearningLoss: 0.000079\twhole_loss: 0.008562 \n",
            "Train Epoch: 110 [32/486 (7%)]\tlearningLoss: 0.000217\twhole_loss: 0.024115 \n",
            "Train Epoch: 110 [48/486 (10%)]\tlearningLoss: 0.000551\twhole_loss: 0.058392 \n",
            "Train Epoch: 110 [64/486 (13%)]\tlearningLoss: 0.000604\twhole_loss: 0.009403 \n",
            "Train Epoch: 110 [80/486 (16%)]\tlearningLoss: 0.000650\twhole_loss: 0.007991 \n",
            "Train Epoch: 110 [96/486 (20%)]\tlearningLoss: 0.000717\twhole_loss: 0.011649 \n",
            "Train Epoch: 110 [112/486 (23%)]\tlearningLoss: 0.000772\twhole_loss: 0.009719 \n",
            "Train Epoch: 110 [128/486 (26%)]\tlearningLoss: 0.001081\twhole_loss: 0.053996 \n",
            "Train Epoch: 110 [144/486 (30%)]\tlearningLoss: 0.001528\twhole_loss: 0.078201 \n",
            "Train Epoch: 110 [160/486 (33%)]\tlearningLoss: 0.001680\twhole_loss: 0.026641 \n",
            "Train Epoch: 110 [176/486 (36%)]\tlearningLoss: 0.002186\twhole_loss: 0.088616 \n",
            "Train Epoch: 110 [192/486 (40%)]\tlearningLoss: 0.002310\twhole_loss: 0.021739 \n",
            "Train Epoch: 110 [208/486 (43%)]\tlearningLoss: 0.003997\twhole_loss: 0.295143 \n",
            "Train Epoch: 110 [224/486 (46%)]\tlearningLoss: 0.004254\twhole_loss: 0.044992 \n",
            "Train Epoch: 110 [240/486 (49%)]\tlearningLoss: 0.004287\twhole_loss: 0.005717 \n",
            "Train Epoch: 110 [256/486 (53%)]\tlearningLoss: 0.004668\twhole_loss: 0.066705 \n",
            "Train Epoch: 110 [272/486 (56%)]\tlearningLoss: 0.004859\twhole_loss: 0.033427 \n",
            "Train Epoch: 110 [288/486 (59%)]\tlearningLoss: 0.004943\twhole_loss: 0.014757 \n",
            "Train Epoch: 110 [304/486 (63%)]\tlearningLoss: 0.005029\twhole_loss: 0.015010 \n",
            "Train Epoch: 110 [320/486 (66%)]\tlearningLoss: 0.006636\twhole_loss: 0.281221 \n",
            "Train Epoch: 110 [336/486 (69%)]\tlearningLoss: 0.006715\twhole_loss: 0.013892 \n",
            "Train Epoch: 110 [352/486 (72%)]\tlearningLoss: 0.007257\twhole_loss: 0.094795 \n",
            "Train Epoch: 110 [368/486 (76%)]\tlearningLoss: 0.007688\twhole_loss: 0.075461 \n",
            "Train Epoch: 110 [384/486 (79%)]\tlearningLoss: 0.008582\twhole_loss: 0.156367 \n",
            "Train Epoch: 110 [400/486 (82%)]\tlearningLoss: 0.008685\twhole_loss: 0.018051 \n",
            "Train Epoch: 110 [416/486 (86%)]\tlearningLoss: 0.008740\twhole_loss: 0.009640 \n",
            "Train Epoch: 110 [432/486 (89%)]\tlearningLoss: 0.009049\twhole_loss: 0.054123 \n",
            "Train Epoch: 110 [448/486 (92%)]\tlearningLoss: 0.009116\twhole_loss: 0.011720 \n",
            "Train Epoch: 110 [464/486 (95%)]\tlearningLoss: 0.009297\twhole_loss: 0.031681 \n",
            "Train Epoch: 110 [180/486 (37%)]\tlearningLoss: 0.010372\twhole_loss: 0.188019 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.3999, Accuracy: 69/75(92%)\n",
            "\n",
            "Train Epoch: 111 [0/486 (0%)]\tlearningLoss: 0.000172\twhole_loss: 0.030112 \n",
            "Train Epoch: 111 [16/486 (3%)]\tlearningLoss: 0.000505\twhole_loss: 0.058252 \n",
            "Train Epoch: 111 [32/486 (7%)]\tlearningLoss: 0.001283\twhole_loss: 0.136117 \n",
            "Train Epoch: 111 [48/486 (10%)]\tlearningLoss: 0.001381\twhole_loss: 0.017164 \n",
            "Train Epoch: 111 [64/486 (13%)]\tlearningLoss: 0.001738\twhole_loss: 0.062449 \n",
            "Train Epoch: 111 [80/486 (16%)]\tlearningLoss: 0.002016\twhole_loss: 0.048661 \n",
            "Train Epoch: 111 [96/486 (20%)]\tlearningLoss: 0.002102\twhole_loss: 0.015125 \n",
            "Train Epoch: 111 [112/486 (23%)]\tlearningLoss: 0.002177\twhole_loss: 0.013105 \n",
            "Train Epoch: 111 [128/486 (26%)]\tlearningLoss: 0.002188\twhole_loss: 0.001898 \n",
            "Train Epoch: 111 [144/486 (30%)]\tlearningLoss: 0.002621\twhole_loss: 0.075870 \n",
            "Train Epoch: 111 [160/486 (33%)]\tlearningLoss: 0.002633\twhole_loss: 0.002015 \n",
            "Train Epoch: 111 [176/486 (36%)]\tlearningLoss: 0.002685\twhole_loss: 0.009172 \n",
            "Train Epoch: 111 [192/486 (40%)]\tlearningLoss: 0.002713\twhole_loss: 0.004839 \n",
            "Train Epoch: 111 [208/486 (43%)]\tlearningLoss: 0.002721\twhole_loss: 0.001361 \n",
            "Train Epoch: 111 [224/486 (46%)]\tlearningLoss: 0.002850\twhole_loss: 0.022652 \n",
            "Train Epoch: 111 [240/486 (49%)]\tlearningLoss: 0.002906\twhole_loss: 0.009794 \n",
            "Train Epoch: 111 [256/486 (53%)]\tlearningLoss: 0.003484\twhole_loss: 0.101052 \n",
            "Train Epoch: 111 [272/486 (56%)]\tlearningLoss: 0.005881\twhole_loss: 0.419591 \n",
            "Train Epoch: 111 [288/486 (59%)]\tlearningLoss: 0.006076\twhole_loss: 0.033986 \n",
            "Train Epoch: 111 [304/486 (63%)]\tlearningLoss: 0.006409\twhole_loss: 0.058386 \n",
            "Train Epoch: 111 [320/486 (66%)]\tlearningLoss: 0.006765\twhole_loss: 0.062206 \n",
            "Train Epoch: 111 [336/486 (69%)]\tlearningLoss: 0.007463\twhole_loss: 0.122270 \n",
            "Train Epoch: 111 [352/486 (72%)]\tlearningLoss: 0.007583\twhole_loss: 0.021002 \n",
            "Train Epoch: 111 [368/486 (76%)]\tlearningLoss: 0.007603\twhole_loss: 0.003486 \n",
            "Train Epoch: 111 [384/486 (79%)]\tlearningLoss: 0.008343\twhole_loss: 0.129415 \n",
            "Train Epoch: 111 [400/486 (82%)]\tlearningLoss: 0.008623\twhole_loss: 0.049059 \n",
            "Train Epoch: 111 [416/486 (86%)]\tlearningLoss: 0.008641\twhole_loss: 0.003059 \n",
            "Train Epoch: 111 [432/486 (89%)]\tlearningLoss: 0.009605\twhole_loss: 0.168799 \n",
            "Train Epoch: 111 [448/486 (92%)]\tlearningLoss: 0.011470\twhole_loss: 0.326318 \n",
            "Train Epoch: 111 [464/486 (95%)]\tlearningLoss: 0.011496\twhole_loss: 0.004546 \n",
            "Train Epoch: 111 [180/486 (37%)]\tlearningLoss: 0.013642\twhole_loss: 0.375583 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 2, 1, 1, 2, 1, 1, 2, 0, 2, 2, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:5.1335, Accuracy: 63/75(84%)\n",
            "\n",
            "Train Epoch: 112 [0/486 (0%)]\tlearningLoss: 0.000302\twhole_loss: 0.052784 \n",
            "Train Epoch: 112 [16/486 (3%)]\tlearningLoss: 0.000333\twhole_loss: 0.005555 \n",
            "Train Epoch: 112 [32/486 (7%)]\tlearningLoss: 0.000417\twhole_loss: 0.014691 \n",
            "Train Epoch: 112 [48/486 (10%)]\tlearningLoss: 0.000550\twhole_loss: 0.023244 \n",
            "Train Epoch: 112 [64/486 (13%)]\tlearningLoss: 0.000618\twhole_loss: 0.011933 \n",
            "Train Epoch: 112 [80/486 (16%)]\tlearningLoss: 0.001793\twhole_loss: 0.205594 \n",
            "Train Epoch: 112 [96/486 (20%)]\tlearningLoss: 0.001840\twhole_loss: 0.008152 \n",
            "Train Epoch: 112 [112/486 (23%)]\tlearningLoss: 0.001904\twhole_loss: 0.011196 \n",
            "Train Epoch: 112 [128/486 (26%)]\tlearningLoss: 0.002996\twhole_loss: 0.191188 \n",
            "Train Epoch: 112 [144/486 (30%)]\tlearningLoss: 0.006029\twhole_loss: 0.530692 \n",
            "Train Epoch: 112 [160/486 (33%)]\tlearningLoss: 0.006750\twhole_loss: 0.126200 \n",
            "Train Epoch: 112 [176/486 (36%)]\tlearningLoss: 0.007597\twhole_loss: 0.148301 \n",
            "Train Epoch: 112 [192/486 (40%)]\tlearningLoss: 0.007708\twhole_loss: 0.019375 \n",
            "Train Epoch: 112 [208/486 (43%)]\tlearningLoss: 0.007973\twhole_loss: 0.046349 \n",
            "Train Epoch: 112 [224/486 (46%)]\tlearningLoss: 0.012639\twhole_loss: 0.816506 \n",
            "Train Epoch: 112 [240/486 (49%)]\tlearningLoss: 0.012713\twhole_loss: 0.012987 \n",
            "Train Epoch: 112 [256/486 (53%)]\tlearningLoss: 0.012994\twhole_loss: 0.049250 \n",
            "Train Epoch: 112 [272/486 (56%)]\tlearningLoss: 0.013680\twhole_loss: 0.120054 \n",
            "Train Epoch: 112 [288/486 (59%)]\tlearningLoss: 0.013874\twhole_loss: 0.033905 \n",
            "Train Epoch: 112 [304/486 (63%)]\tlearningLoss: 0.014805\twhole_loss: 0.162960 \n",
            "Train Epoch: 112 [320/486 (66%)]\tlearningLoss: 0.016653\twhole_loss: 0.323442 \n",
            "Train Epoch: 112 [336/486 (69%)]\tlearningLoss: 0.019654\twhole_loss: 0.525129 \n",
            "Train Epoch: 112 [352/486 (72%)]\tlearningLoss: 0.020680\twhole_loss: 0.179462 \n",
            "Train Epoch: 112 [368/486 (76%)]\tlearningLoss: 0.024310\twhole_loss: 0.635337 \n",
            "Train Epoch: 112 [384/486 (79%)]\tlearningLoss: 0.024685\twhole_loss: 0.065601 \n",
            "Train Epoch: 112 [400/486 (82%)]\tlearningLoss: 0.024854\twhole_loss: 0.029589 \n",
            "Train Epoch: 112 [416/486 (86%)]\tlearningLoss: 0.025063\twhole_loss: 0.036592 \n",
            "Train Epoch: 112 [432/486 (89%)]\tlearningLoss: 0.025147\twhole_loss: 0.014716 \n",
            "Train Epoch: 112 [448/486 (92%)]\tlearningLoss: 0.025809\twhole_loss: 0.115756 \n",
            "Train Epoch: 112 [464/486 (95%)]\tlearningLoss: 0.027015\twhole_loss: 0.211058 \n",
            "Train Epoch: 112 [180/486 (37%)]\tlearningLoss: 0.027434\twhole_loss: 0.073305 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.8412, Accuracy: 63/75(84%)\n",
            "\n",
            "Train Epoch: 113 [0/486 (0%)]\tlearningLoss: 0.000351\twhole_loss: 0.061344 \n",
            "Train Epoch: 113 [16/486 (3%)]\tlearningLoss: 0.000827\twhole_loss: 0.083464 \n",
            "Train Epoch: 113 [32/486 (7%)]\tlearningLoss: 0.001203\twhole_loss: 0.065734 \n",
            "Train Epoch: 113 [48/486 (10%)]\tlearningLoss: 0.001257\twhole_loss: 0.009468 \n",
            "Train Epoch: 113 [64/486 (13%)]\tlearningLoss: 0.001548\twhole_loss: 0.050894 \n",
            "Train Epoch: 113 [80/486 (16%)]\tlearningLoss: 0.002110\twhole_loss: 0.098344 \n",
            "Train Epoch: 113 [96/486 (20%)]\tlearningLoss: 0.002559\twhole_loss: 0.078570 \n",
            "Train Epoch: 113 [112/486 (23%)]\tlearningLoss: 0.002682\twhole_loss: 0.021538 \n",
            "Train Epoch: 113 [128/486 (26%)]\tlearningLoss: 0.002731\twhole_loss: 0.008650 \n",
            "Train Epoch: 113 [144/486 (30%)]\tlearningLoss: 0.003240\twhole_loss: 0.088932 \n",
            "Train Epoch: 113 [160/486 (33%)]\tlearningLoss: 0.003802\twhole_loss: 0.098469 \n",
            "Train Epoch: 113 [176/486 (36%)]\tlearningLoss: 0.004305\twhole_loss: 0.087986 \n",
            "Train Epoch: 113 [192/486 (40%)]\tlearningLoss: 0.004398\twhole_loss: 0.016218 \n",
            "Train Epoch: 113 [208/486 (43%)]\tlearningLoss: 0.005578\twhole_loss: 0.206463 \n",
            "Train Epoch: 113 [224/486 (46%)]\tlearningLoss: 0.005682\twhole_loss: 0.018341 \n",
            "Train Epoch: 113 [240/486 (49%)]\tlearningLoss: 0.005850\twhole_loss: 0.029387 \n",
            "Train Epoch: 113 [256/486 (53%)]\tlearningLoss: 0.005957\twhole_loss: 0.018733 \n",
            "Train Epoch: 113 [272/486 (56%)]\tlearningLoss: 0.006114\twhole_loss: 0.027470 \n",
            "Train Epoch: 113 [288/486 (59%)]\tlearningLoss: 0.006493\twhole_loss: 0.066305 \n",
            "Train Epoch: 113 [304/486 (63%)]\tlearningLoss: 0.006592\twhole_loss: 0.017276 \n",
            "Train Epoch: 113 [320/486 (66%)]\tlearningLoss: 0.006895\twhole_loss: 0.052988 \n",
            "Train Epoch: 113 [336/486 (69%)]\tlearningLoss: 0.007110\twhole_loss: 0.037651 \n",
            "Train Epoch: 113 [352/486 (72%)]\tlearningLoss: 0.007849\twhole_loss: 0.129302 \n",
            "Train Epoch: 113 [368/486 (76%)]\tlearningLoss: 0.007917\twhole_loss: 0.011977 \n",
            "Train Epoch: 113 [384/486 (79%)]\tlearningLoss: 0.008046\twhole_loss: 0.022603 \n",
            "Train Epoch: 113 [400/486 (82%)]\tlearningLoss: 0.008061\twhole_loss: 0.002619 \n",
            "Train Epoch: 113 [416/486 (86%)]\tlearningLoss: 0.008539\twhole_loss: 0.083594 \n",
            "Train Epoch: 113 [432/486 (89%)]\tlearningLoss: 0.008598\twhole_loss: 0.010281 \n",
            "Train Epoch: 113 [448/486 (92%)]\tlearningLoss: 0.008892\twhole_loss: 0.051517 \n",
            "Train Epoch: 113 [464/486 (95%)]\tlearningLoss: 0.009088\twhole_loss: 0.034351 \n",
            "Train Epoch: 113 [180/486 (37%)]\tlearningLoss: 0.009094\twhole_loss: 0.000902 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.9495, Accuracy: 69/75(92%)\n",
            "\n",
            "Train Epoch: 114 [0/486 (0%)]\tlearningLoss: 0.000327\twhole_loss: 0.057265 \n",
            "Train Epoch: 114 [16/486 (3%)]\tlearningLoss: 0.000425\twhole_loss: 0.017151 \n",
            "Train Epoch: 114 [32/486 (7%)]\tlearningLoss: 0.000513\twhole_loss: 0.015346 \n",
            "Train Epoch: 114 [48/486 (10%)]\tlearningLoss: 0.000581\twhole_loss: 0.011861 \n",
            "Train Epoch: 114 [64/486 (13%)]\tlearningLoss: 0.000845\twhole_loss: 0.046219 \n",
            "Train Epoch: 114 [80/486 (16%)]\tlearningLoss: 0.001322\twhole_loss: 0.083546 \n",
            "Train Epoch: 114 [96/486 (20%)]\tlearningLoss: 0.001362\twhole_loss: 0.006896 \n",
            "Train Epoch: 114 [112/486 (23%)]\tlearningLoss: 0.001460\twhole_loss: 0.017153 \n",
            "Train Epoch: 114 [128/486 (26%)]\tlearningLoss: 0.001490\twhole_loss: 0.005288 \n",
            "Train Epoch: 114 [144/486 (30%)]\tlearningLoss: 0.001492\twhole_loss: 0.000342 \n",
            "Train Epoch: 114 [160/486 (33%)]\tlearningLoss: 0.001819\twhole_loss: 0.057311 \n",
            "Train Epoch: 114 [176/486 (36%)]\tlearningLoss: 0.002056\twhole_loss: 0.041350 \n",
            "Train Epoch: 114 [192/486 (40%)]\tlearningLoss: 0.002057\twhole_loss: 0.000253 \n",
            "Train Epoch: 114 [208/486 (43%)]\tlearningLoss: 0.002124\twhole_loss: 0.011800 \n",
            "Train Epoch: 114 [224/486 (46%)]\tlearningLoss: 0.002152\twhole_loss: 0.004829 \n",
            "Train Epoch: 114 [240/486 (49%)]\tlearningLoss: 0.002236\twhole_loss: 0.014751 \n",
            "Train Epoch: 114 [256/486 (53%)]\tlearningLoss: 0.002474\twhole_loss: 0.041503 \n",
            "Train Epoch: 114 [272/486 (56%)]\tlearningLoss: 0.002574\twhole_loss: 0.017556 \n",
            "Train Epoch: 114 [288/486 (59%)]\tlearningLoss: 0.002658\twhole_loss: 0.014684 \n",
            "Train Epoch: 114 [304/486 (63%)]\tlearningLoss: 0.003167\twhole_loss: 0.089081 \n",
            "Train Epoch: 114 [320/486 (66%)]\tlearningLoss: 0.003189\twhole_loss: 0.003891 \n",
            "Train Epoch: 114 [336/486 (69%)]\tlearningLoss: 0.003377\twhole_loss: 0.032918 \n",
            "Train Epoch: 114 [352/486 (72%)]\tlearningLoss: 0.003448\twhole_loss: 0.012436 \n",
            "Train Epoch: 114 [368/486 (76%)]\tlearningLoss: 0.003891\twhole_loss: 0.077568 \n",
            "Train Epoch: 114 [384/486 (79%)]\tlearningLoss: 0.004057\twhole_loss: 0.028899 \n",
            "Train Epoch: 114 [400/486 (82%)]\tlearningLoss: 0.004084\twhole_loss: 0.004855 \n",
            "Train Epoch: 114 [416/486 (86%)]\tlearningLoss: 0.004139\twhole_loss: 0.009644 \n",
            "Train Epoch: 114 [432/486 (89%)]\tlearningLoss: 0.004384\twhole_loss: 0.042724 \n",
            "Train Epoch: 114 [448/486 (92%)]\tlearningLoss: 0.004609\twhole_loss: 0.039419 \n",
            "Train Epoch: 114 [464/486 (95%)]\tlearningLoss: 0.005301\twhole_loss: 0.121077 \n",
            "Train Epoch: 114 [180/486 (37%)]\tlearningLoss: 0.005302\twhole_loss: 0.000165 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 2, 1, 1, 1, 1, 2, 0, 2, 2, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:5.3269, Accuracy: 64/75(85%)\n",
            "\n",
            "Train Epoch: 115 [0/486 (0%)]\tlearningLoss: 0.000127\twhole_loss: 0.022192 \n",
            "Train Epoch: 115 [16/486 (3%)]\tlearningLoss: 0.000133\twhole_loss: 0.001024 \n",
            "Train Epoch: 115 [32/486 (7%)]\tlearningLoss: 0.000203\twhole_loss: 0.012235 \n",
            "Train Epoch: 115 [48/486 (10%)]\tlearningLoss: 0.000216\twhole_loss: 0.002413 \n",
            "Train Epoch: 115 [64/486 (13%)]\tlearningLoss: 0.000223\twhole_loss: 0.001209 \n",
            "Train Epoch: 115 [80/486 (16%)]\tlearningLoss: 0.000226\twhole_loss: 0.000462 \n",
            "Train Epoch: 115 [96/486 (20%)]\tlearningLoss: 0.000308\twhole_loss: 0.014290 \n",
            "Train Epoch: 115 [112/486 (23%)]\tlearningLoss: 0.000378\twhole_loss: 0.012391 \n",
            "Train Epoch: 115 [128/486 (26%)]\tlearningLoss: 0.000454\twhole_loss: 0.013313 \n",
            "Train Epoch: 115 [144/486 (30%)]\tlearningLoss: 0.001065\twhole_loss: 0.106820 \n",
            "Train Epoch: 115 [160/486 (33%)]\tlearningLoss: 0.001275\twhole_loss: 0.036692 \n",
            "Train Epoch: 115 [176/486 (36%)]\tlearningLoss: 0.001343\twhole_loss: 0.011922 \n",
            "Train Epoch: 115 [192/486 (40%)]\tlearningLoss: 0.001882\twhole_loss: 0.094342 \n",
            "Train Epoch: 115 [208/486 (43%)]\tlearningLoss: 0.002357\twhole_loss: 0.083243 \n",
            "Train Epoch: 115 [224/486 (46%)]\tlearningLoss: 0.002437\twhole_loss: 0.013993 \n",
            "Train Epoch: 115 [240/486 (49%)]\tlearningLoss: 0.002741\twhole_loss: 0.053201 \n",
            "Train Epoch: 115 [256/486 (53%)]\tlearningLoss: 0.002787\twhole_loss: 0.007980 \n",
            "Train Epoch: 115 [272/486 (56%)]\tlearningLoss: 0.002855\twhole_loss: 0.011980 \n",
            "Train Epoch: 115 [288/486 (59%)]\tlearningLoss: 0.002869\twhole_loss: 0.002404 \n",
            "Train Epoch: 115 [304/486 (63%)]\tlearningLoss: 0.002881\twhole_loss: 0.002019 \n",
            "Train Epoch: 115 [320/486 (66%)]\tlearningLoss: 0.002945\twhole_loss: 0.011212 \n",
            "Train Epoch: 115 [336/486 (69%)]\tlearningLoss: 0.002946\twhole_loss: 0.000281 \n",
            "Train Epoch: 115 [352/486 (72%)]\tlearningLoss: 0.003063\twhole_loss: 0.020322 \n",
            "Train Epoch: 115 [368/486 (76%)]\tlearningLoss: 0.003216\twhole_loss: 0.026778 \n",
            "Train Epoch: 115 [384/486 (79%)]\tlearningLoss: 0.003249\twhole_loss: 0.005878 \n",
            "Train Epoch: 115 [400/486 (82%)]\tlearningLoss: 0.003547\twhole_loss: 0.052088 \n",
            "Train Epoch: 115 [416/486 (86%)]\tlearningLoss: 0.003576\twhole_loss: 0.005156 \n",
            "Train Epoch: 115 [432/486 (89%)]\tlearningLoss: 0.003617\twhole_loss: 0.007150 \n",
            "Train Epoch: 115 [448/486 (92%)]\tlearningLoss: 0.005177\twhole_loss: 0.272995 \n",
            "Train Epoch: 115 [464/486 (95%)]\tlearningLoss: 0.005302\twhole_loss: 0.021900 \n",
            "Train Epoch: 115 [180/486 (37%)]\tlearningLoss: 0.005303\twhole_loss: 0.000056 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.3790, Accuracy: 70/75(93%)\n",
            "\n",
            "Train Epoch: 116 [0/486 (0%)]\tlearningLoss: 0.000807\twhole_loss: 0.141250 \n",
            "Train Epoch: 116 [16/486 (3%)]\tlearningLoss: 0.000890\twhole_loss: 0.014545 \n",
            "Train Epoch: 116 [32/486 (7%)]\tlearningLoss: 0.001132\twhole_loss: 0.042316 \n",
            "Train Epoch: 116 [48/486 (10%)]\tlearningLoss: 0.001143\twhole_loss: 0.001835 \n",
            "Train Epoch: 116 [64/486 (13%)]\tlearningLoss: 0.001168\twhole_loss: 0.004422 \n",
            "Train Epoch: 116 [80/486 (16%)]\tlearningLoss: 0.001248\twhole_loss: 0.014040 \n",
            "Train Epoch: 116 [96/486 (20%)]\tlearningLoss: 0.002049\twhole_loss: 0.140106 \n",
            "Train Epoch: 116 [112/486 (23%)]\tlearningLoss: 0.002052\twhole_loss: 0.000672 \n",
            "Train Epoch: 116 [128/486 (26%)]\tlearningLoss: 0.002243\twhole_loss: 0.033300 \n",
            "Train Epoch: 116 [144/486 (30%)]\tlearningLoss: 0.002669\twhole_loss: 0.074618 \n",
            "Train Epoch: 116 [160/486 (33%)]\tlearningLoss: 0.002703\twhole_loss: 0.005972 \n",
            "Train Epoch: 116 [176/486 (36%)]\tlearningLoss: 0.004179\twhole_loss: 0.258268 \n",
            "Train Epoch: 116 [192/486 (40%)]\tlearningLoss: 0.004626\twhole_loss: 0.078274 \n",
            "Train Epoch: 116 [208/486 (43%)]\tlearningLoss: 0.004700\twhole_loss: 0.012823 \n",
            "Train Epoch: 116 [224/486 (46%)]\tlearningLoss: 0.004779\twhole_loss: 0.013802 \n",
            "Train Epoch: 116 [240/486 (49%)]\tlearningLoss: 0.005511\twhole_loss: 0.128171 \n",
            "Train Epoch: 116 [256/486 (53%)]\tlearningLoss: 0.008294\twhole_loss: 0.487101 \n",
            "Train Epoch: 116 [272/486 (56%)]\tlearningLoss: 0.009437\twhole_loss: 0.199945 \n",
            "Train Epoch: 116 [288/486 (59%)]\tlearningLoss: 0.009458\twhole_loss: 0.003711 \n",
            "Train Epoch: 116 [304/486 (63%)]\tlearningLoss: 0.009557\twhole_loss: 0.017368 \n",
            "Train Epoch: 116 [320/486 (66%)]\tlearningLoss: 0.010198\twhole_loss: 0.112099 \n",
            "Train Epoch: 116 [336/486 (69%)]\tlearningLoss: 0.010285\twhole_loss: 0.015229 \n",
            "Train Epoch: 116 [352/486 (72%)]\tlearningLoss: 0.015576\twhole_loss: 0.925896 \n",
            "Train Epoch: 116 [368/486 (76%)]\tlearningLoss: 0.015823\twhole_loss: 0.043332 \n",
            "Train Epoch: 116 [384/486 (79%)]\tlearningLoss: 0.015981\twhole_loss: 0.027499 \n",
            "Train Epoch: 116 [400/486 (82%)]\tlearningLoss: 0.017630\twhole_loss: 0.288704 \n",
            "Train Epoch: 116 [416/486 (86%)]\tlearningLoss: 0.017901\twhole_loss: 0.047343 \n",
            "Train Epoch: 116 [432/486 (89%)]\tlearningLoss: 0.021037\twhole_loss: 0.548878 \n",
            "Train Epoch: 116 [448/486 (92%)]\tlearningLoss: 0.021811\twhole_loss: 0.135487 \n",
            "Train Epoch: 116 [464/486 (95%)]\tlearningLoss: 0.021954\twhole_loss: 0.024864 \n",
            "Train Epoch: 116 [180/486 (37%)]\tlearningLoss: 0.022038\twhole_loss: 0.014845 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 2, 2, 0, 2, 2], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:5.5866, Accuracy: 61/75(81%)\n",
            "\n",
            "Train Epoch: 117 [0/486 (0%)]\tlearningLoss: 0.001459\twhole_loss: 0.255318 \n",
            "Train Epoch: 117 [16/486 (3%)]\tlearningLoss: 0.003284\twhole_loss: 0.319376 \n",
            "Train Epoch: 117 [32/486 (7%)]\tlearningLoss: 0.003572\twhole_loss: 0.050370 \n",
            "Train Epoch: 117 [48/486 (10%)]\tlearningLoss: 0.005554\twhole_loss: 0.346929 \n",
            "Train Epoch: 117 [64/486 (13%)]\tlearningLoss: 0.007454\twhole_loss: 0.332424 \n",
            "Train Epoch: 117 [80/486 (16%)]\tlearningLoss: 0.008304\twhole_loss: 0.148744 \n",
            "Train Epoch: 117 [96/486 (20%)]\tlearningLoss: 0.009376\twhole_loss: 0.187559 \n",
            "Train Epoch: 117 [112/486 (23%)]\tlearningLoss: 0.009815\twhole_loss: 0.076911 \n",
            "Train Epoch: 117 [128/486 (26%)]\tlearningLoss: 0.010220\twhole_loss: 0.070842 \n",
            "Train Epoch: 117 [144/486 (30%)]\tlearningLoss: 0.010303\twhole_loss: 0.014473 \n",
            "Train Epoch: 117 [160/486 (33%)]\tlearningLoss: 0.010343\twhole_loss: 0.007084 \n",
            "Train Epoch: 117 [176/486 (36%)]\tlearningLoss: 0.011272\twhole_loss: 0.162523 \n",
            "Train Epoch: 117 [192/486 (40%)]\tlearningLoss: 0.011303\twhole_loss: 0.005527 \n",
            "Train Epoch: 117 [208/486 (43%)]\tlearningLoss: 0.011440\twhole_loss: 0.023996 \n",
            "Train Epoch: 117 [224/486 (46%)]\tlearningLoss: 0.012080\twhole_loss: 0.111948 \n",
            "Train Epoch: 117 [240/486 (49%)]\tlearningLoss: 0.012890\twhole_loss: 0.141753 \n",
            "Train Epoch: 117 [256/486 (53%)]\tlearningLoss: 0.013104\twhole_loss: 0.037386 \n",
            "Train Epoch: 117 [272/486 (56%)]\tlearningLoss: 0.013756\twhole_loss: 0.114195 \n",
            "Train Epoch: 117 [288/486 (59%)]\tlearningLoss: 0.013812\twhole_loss: 0.009792 \n",
            "Train Epoch: 117 [304/486 (63%)]\tlearningLoss: 0.014857\twhole_loss: 0.182894 \n",
            "Train Epoch: 117 [320/486 (66%)]\tlearningLoss: 0.015196\twhole_loss: 0.059294 \n",
            "Train Epoch: 117 [336/486 (69%)]\tlearningLoss: 0.015272\twhole_loss: 0.013343 \n",
            "Train Epoch: 117 [352/486 (72%)]\tlearningLoss: 0.015312\twhole_loss: 0.006894 \n",
            "Train Epoch: 117 [368/486 (76%)]\tlearningLoss: 0.016213\twhole_loss: 0.157651 \n",
            "Train Epoch: 117 [384/486 (79%)]\tlearningLoss: 0.019079\twhole_loss: 0.501608 \n",
            "Train Epoch: 117 [400/486 (82%)]\tlearningLoss: 0.019091\twhole_loss: 0.002079 \n",
            "Train Epoch: 117 [416/486 (86%)]\tlearningLoss: 0.019103\twhole_loss: 0.002046 \n",
            "Train Epoch: 117 [432/486 (89%)]\tlearningLoss: 0.020230\twhole_loss: 0.197324 \n",
            "Train Epoch: 117 [448/486 (92%)]\tlearningLoss: 0.021663\twhole_loss: 0.250805 \n",
            "Train Epoch: 117 [464/486 (95%)]\tlearningLoss: 0.021707\twhole_loss: 0.007668 \n",
            "Train Epoch: 117 [180/486 (37%)]\tlearningLoss: 0.021710\twhole_loss: 0.000429 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 2, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.8432, Accuracy: 67/75(89%)\n",
            "\n",
            "Train Epoch: 118 [0/486 (0%)]\tlearningLoss: 0.000055\twhole_loss: 0.009690 \n",
            "Train Epoch: 118 [16/486 (3%)]\tlearningLoss: 0.000283\twhole_loss: 0.039839 \n",
            "Train Epoch: 118 [32/486 (7%)]\tlearningLoss: 0.000542\twhole_loss: 0.045295 \n",
            "Train Epoch: 118 [48/486 (10%)]\tlearningLoss: 0.000646\twhole_loss: 0.018202 \n",
            "Train Epoch: 118 [64/486 (13%)]\tlearningLoss: 0.000918\twhole_loss: 0.047605 \n",
            "Train Epoch: 118 [80/486 (16%)]\tlearningLoss: 0.001311\twhole_loss: 0.068802 \n",
            "Train Epoch: 118 [96/486 (20%)]\tlearningLoss: 0.001425\twhole_loss: 0.020024 \n",
            "Train Epoch: 118 [112/486 (23%)]\tlearningLoss: 0.001482\twhole_loss: 0.009842 \n",
            "Train Epoch: 118 [128/486 (26%)]\tlearningLoss: 0.001606\twhole_loss: 0.021670 \n",
            "Train Epoch: 118 [144/486 (30%)]\tlearningLoss: 0.002009\twhole_loss: 0.070541 \n",
            "Train Epoch: 118 [160/486 (33%)]\tlearningLoss: 0.002077\twhole_loss: 0.012029 \n",
            "Train Epoch: 118 [176/486 (36%)]\tlearningLoss: 0.003057\twhole_loss: 0.171369 \n",
            "Train Epoch: 118 [192/486 (40%)]\tlearningLoss: 0.003164\twhole_loss: 0.018749 \n",
            "Train Epoch: 118 [208/486 (43%)]\tlearningLoss: 0.003408\twhole_loss: 0.042728 \n",
            "Train Epoch: 118 [224/486 (46%)]\tlearningLoss: 0.003509\twhole_loss: 0.017767 \n",
            "Train Epoch: 118 [240/486 (49%)]\tlearningLoss: 0.005330\twhole_loss: 0.318573 \n",
            "Train Epoch: 118 [256/486 (53%)]\tlearningLoss: 0.005409\twhole_loss: 0.013847 \n",
            "Train Epoch: 118 [272/486 (56%)]\tlearningLoss: 0.005438\twhole_loss: 0.005112 \n",
            "Train Epoch: 118 [288/486 (59%)]\tlearningLoss: 0.005632\twhole_loss: 0.033855 \n",
            "Train Epoch: 118 [304/486 (63%)]\tlearningLoss: 0.005699\twhole_loss: 0.011698 \n",
            "Train Epoch: 118 [320/486 (66%)]\tlearningLoss: 0.005742\twhole_loss: 0.007596 \n",
            "Train Epoch: 118 [336/486 (69%)]\tlearningLoss: 0.006795\twhole_loss: 0.184283 \n",
            "Train Epoch: 118 [352/486 (72%)]\tlearningLoss: 0.007152\twhole_loss: 0.062508 \n",
            "Train Epoch: 118 [368/486 (76%)]\tlearningLoss: 0.008085\twhole_loss: 0.163166 \n",
            "Train Epoch: 118 [384/486 (79%)]\tlearningLoss: 0.008906\twhole_loss: 0.143691 \n",
            "Train Epoch: 118 [400/486 (82%)]\tlearningLoss: 0.009006\twhole_loss: 0.017645 \n",
            "Train Epoch: 118 [416/486 (86%)]\tlearningLoss: 0.009042\twhole_loss: 0.006261 \n",
            "Train Epoch: 118 [432/486 (89%)]\tlearningLoss: 0.009371\twhole_loss: 0.057582 \n",
            "Train Epoch: 118 [448/486 (92%)]\tlearningLoss: 0.010042\twhole_loss: 0.117325 \n",
            "Train Epoch: 118 [464/486 (95%)]\tlearningLoss: 0.010635\twhole_loss: 0.103890 \n",
            "Train Epoch: 118 [180/486 (37%)]\tlearningLoss: 0.016065\twhole_loss: 0.950152 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 2, 1, 1, 1, 1, 1, 2, 0, 2, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:5.5085, Accuracy: 64/75(85%)\n",
            "\n",
            "Train Epoch: 119 [0/486 (0%)]\tlearningLoss: 0.000210\twhole_loss: 0.036807 \n",
            "Train Epoch: 119 [16/486 (3%)]\tlearningLoss: 0.000832\twhole_loss: 0.108879 \n",
            "Train Epoch: 119 [32/486 (7%)]\tlearningLoss: 0.000980\twhole_loss: 0.025739 \n",
            "Train Epoch: 119 [48/486 (10%)]\tlearningLoss: 0.001309\twhole_loss: 0.057674 \n",
            "Train Epoch: 119 [64/486 (13%)]\tlearningLoss: 0.001544\twhole_loss: 0.041084 \n",
            "Train Epoch: 119 [80/486 (16%)]\tlearningLoss: 0.002792\twhole_loss: 0.218458 \n",
            "Train Epoch: 119 [96/486 (20%)]\tlearningLoss: 0.003315\twhole_loss: 0.091435 \n",
            "Train Epoch: 119 [112/486 (23%)]\tlearningLoss: 0.003398\twhole_loss: 0.014497 \n",
            "Train Epoch: 119 [128/486 (26%)]\tlearningLoss: 0.004994\twhole_loss: 0.279367 \n",
            "Train Epoch: 119 [144/486 (30%)]\tlearningLoss: 0.005192\twhole_loss: 0.034698 \n",
            "Train Epoch: 119 [160/486 (33%)]\tlearningLoss: 0.005451\twhole_loss: 0.045323 \n",
            "Train Epoch: 119 [176/486 (36%)]\tlearningLoss: 0.006251\twhole_loss: 0.139890 \n",
            "Train Epoch: 119 [192/486 (40%)]\tlearningLoss: 0.007636\twhole_loss: 0.242439 \n",
            "Train Epoch: 119 [208/486 (43%)]\tlearningLoss: 0.007806\twhole_loss: 0.029809 \n",
            "Train Epoch: 119 [224/486 (46%)]\tlearningLoss: 0.007820\twhole_loss: 0.002362 \n",
            "Train Epoch: 119 [240/486 (49%)]\tlearningLoss: 0.009372\twhole_loss: 0.271710 \n",
            "Train Epoch: 119 [256/486 (53%)]\tlearningLoss: 0.009618\twhole_loss: 0.043026 \n",
            "Train Epoch: 119 [272/486 (56%)]\tlearningLoss: 0.010183\twhole_loss: 0.098895 \n",
            "Train Epoch: 119 [288/486 (59%)]\tlearningLoss: 0.010616\twhole_loss: 0.075695 \n",
            "Train Epoch: 119 [304/486 (63%)]\tlearningLoss: 0.010791\twhole_loss: 0.030590 \n",
            "Train Epoch: 119 [320/486 (66%)]\tlearningLoss: 0.010819\twhole_loss: 0.004875 \n",
            "Train Epoch: 119 [336/486 (69%)]\tlearningLoss: 0.010958\twhole_loss: 0.024471 \n",
            "Train Epoch: 119 [352/486 (72%)]\tlearningLoss: 0.011519\twhole_loss: 0.098060 \n",
            "Train Epoch: 119 [368/486 (76%)]\tlearningLoss: 0.011520\twhole_loss: 0.000147 \n",
            "Train Epoch: 119 [384/486 (79%)]\tlearningLoss: 0.011729\twhole_loss: 0.036555 \n",
            "Train Epoch: 119 [400/486 (82%)]\tlearningLoss: 0.014802\twhole_loss: 0.537801 \n",
            "Train Epoch: 119 [416/486 (86%)]\tlearningLoss: 0.015558\twhole_loss: 0.132390 \n",
            "Train Epoch: 119 [432/486 (89%)]\tlearningLoss: 0.016361\twhole_loss: 0.140511 \n",
            "Train Epoch: 119 [448/486 (92%)]\tlearningLoss: 0.016434\twhole_loss: 0.012674 \n",
            "Train Epoch: 119 [464/486 (95%)]\tlearningLoss: 0.016547\twhole_loss: 0.019779 \n",
            "Train Epoch: 119 [180/486 (37%)]\tlearningLoss: 0.016846\twhole_loss: 0.052353 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.7694, Accuracy: 67/75(89%)\n",
            "\n",
            "Train Epoch: 120 [0/486 (0%)]\tlearningLoss: 0.000936\twhole_loss: 0.163741 \n",
            "Train Epoch: 120 [16/486 (3%)]\tlearningLoss: 0.000980\twhole_loss: 0.007747 \n",
            "Train Epoch: 120 [32/486 (7%)]\tlearningLoss: 0.001500\twhole_loss: 0.090969 \n",
            "Train Epoch: 120 [48/486 (10%)]\tlearningLoss: 0.001642\twhole_loss: 0.024858 \n",
            "Train Epoch: 120 [64/486 (13%)]\tlearningLoss: 0.002321\twhole_loss: 0.118849 \n",
            "Train Epoch: 120 [80/486 (16%)]\tlearningLoss: 0.002389\twhole_loss: 0.011997 \n",
            "Train Epoch: 120 [96/486 (20%)]\tlearningLoss: 0.004753\twhole_loss: 0.413592 \n",
            "Train Epoch: 120 [112/486 (23%)]\tlearningLoss: 0.004914\twhole_loss: 0.028259 \n",
            "Train Epoch: 120 [128/486 (26%)]\tlearningLoss: 0.005071\twhole_loss: 0.027369 \n",
            "Train Epoch: 120 [144/486 (30%)]\tlearningLoss: 0.006687\twhole_loss: 0.282770 \n",
            "Train Epoch: 120 [160/486 (33%)]\tlearningLoss: 0.007728\twhole_loss: 0.182315 \n",
            "Train Epoch: 120 [176/486 (36%)]\tlearningLoss: 0.008023\twhole_loss: 0.051536 \n",
            "Train Epoch: 120 [192/486 (40%)]\tlearningLoss: 0.008043\twhole_loss: 0.003570 \n",
            "Train Epoch: 120 [208/486 (43%)]\tlearningLoss: 0.008115\twhole_loss: 0.012491 \n",
            "Train Epoch: 120 [224/486 (46%)]\tlearningLoss: 0.008483\twhole_loss: 0.064536 \n",
            "Train Epoch: 120 [240/486 (49%)]\tlearningLoss: 0.008912\twhole_loss: 0.075062 \n",
            "Train Epoch: 120 [256/486 (53%)]\tlearningLoss: 0.010303\twhole_loss: 0.243440 \n",
            "Train Epoch: 120 [272/486 (56%)]\tlearningLoss: 0.010500\twhole_loss: 0.034354 \n",
            "Train Epoch: 120 [288/486 (59%)]\tlearningLoss: 0.010711\twhole_loss: 0.037056 \n",
            "Train Epoch: 120 [304/486 (63%)]\tlearningLoss: 0.010877\twhole_loss: 0.028994 \n",
            "Train Epoch: 120 [320/486 (66%)]\tlearningLoss: 0.011311\twhole_loss: 0.075949 \n",
            "Train Epoch: 120 [336/486 (69%)]\tlearningLoss: 0.011354\twhole_loss: 0.007574 \n",
            "Train Epoch: 120 [352/486 (72%)]\tlearningLoss: 0.011743\twhole_loss: 0.068038 \n",
            "Train Epoch: 120 [368/486 (76%)]\tlearningLoss: 0.012926\twhole_loss: 0.207045 \n",
            "Train Epoch: 120 [384/486 (79%)]\tlearningLoss: 0.013309\twhole_loss: 0.066899 \n",
            "Train Epoch: 120 [400/486 (82%)]\tlearningLoss: 0.015608\twhole_loss: 0.402436 \n",
            "Train Epoch: 120 [416/486 (86%)]\tlearningLoss: 0.016708\twhole_loss: 0.192411 \n",
            "Train Epoch: 120 [432/486 (89%)]\tlearningLoss: 0.016958\twhole_loss: 0.043746 \n",
            "Train Epoch: 120 [448/486 (92%)]\tlearningLoss: 0.017585\twhole_loss: 0.109835 \n",
            "Train Epoch: 120 [464/486 (95%)]\tlearningLoss: 0.017779\twhole_loss: 0.033888 \n",
            "Train Epoch: 120 [180/486 (37%)]\tlearningLoss: 0.017846\twhole_loss: 0.011756 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 2, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.4899, Accuracy: 65/75(87%)\n",
            "\n",
            "Train Epoch: 121 [0/486 (0%)]\tlearningLoss: 0.001492\twhole_loss: 0.261105 \n",
            "Train Epoch: 121 [16/486 (3%)]\tlearningLoss: 0.001578\twhole_loss: 0.015064 \n",
            "Train Epoch: 121 [32/486 (7%)]\tlearningLoss: 0.001912\twhole_loss: 0.058509 \n",
            "Train Epoch: 121 [48/486 (10%)]\tlearningLoss: 0.002250\twhole_loss: 0.059102 \n",
            "Train Epoch: 121 [64/486 (13%)]\tlearningLoss: 0.002325\twhole_loss: 0.013087 \n",
            "Train Epoch: 121 [80/486 (16%)]\tlearningLoss: 0.002665\twhole_loss: 0.059440 \n",
            "Train Epoch: 121 [96/486 (20%)]\tlearningLoss: 0.004074\twhole_loss: 0.246729 \n",
            "Train Epoch: 121 [112/486 (23%)]\tlearningLoss: 0.004263\twhole_loss: 0.033049 \n",
            "Train Epoch: 121 [128/486 (26%)]\tlearningLoss: 0.005901\twhole_loss: 0.286674 \n",
            "Train Epoch: 121 [144/486 (30%)]\tlearningLoss: 0.005944\twhole_loss: 0.007515 \n",
            "Train Epoch: 121 [160/486 (33%)]\tlearningLoss: 0.006088\twhole_loss: 0.025174 \n",
            "Train Epoch: 121 [176/486 (36%)]\tlearningLoss: 0.007430\twhole_loss: 0.234811 \n",
            "Train Epoch: 121 [192/486 (40%)]\tlearningLoss: 0.007656\twhole_loss: 0.039557 \n",
            "Train Epoch: 121 [208/486 (43%)]\tlearningLoss: 0.009405\twhole_loss: 0.306110 \n",
            "Train Epoch: 121 [224/486 (46%)]\tlearningLoss: 0.009513\twhole_loss: 0.018866 \n",
            "Train Epoch: 121 [240/486 (49%)]\tlearningLoss: 0.010178\twhole_loss: 0.116394 \n",
            "Train Epoch: 121 [256/486 (53%)]\tlearningLoss: 0.010337\twhole_loss: 0.027796 \n",
            "Train Epoch: 121 [272/486 (56%)]\tlearningLoss: 0.010401\twhole_loss: 0.011167 \n",
            "Train Epoch: 121 [288/486 (59%)]\tlearningLoss: 0.010593\twhole_loss: 0.033660 \n",
            "Train Epoch: 121 [304/486 (63%)]\tlearningLoss: 0.010660\twhole_loss: 0.011764 \n",
            "Train Epoch: 121 [320/486 (66%)]\tlearningLoss: 0.011140\twhole_loss: 0.083878 \n",
            "Train Epoch: 121 [336/486 (69%)]\tlearningLoss: 0.011174\twhole_loss: 0.006028 \n",
            "Train Epoch: 121 [352/486 (72%)]\tlearningLoss: 0.012570\twhole_loss: 0.244357 \n",
            "Train Epoch: 121 [368/486 (76%)]\tlearningLoss: 0.012769\twhole_loss: 0.034740 \n",
            "Train Epoch: 121 [384/486 (79%)]\tlearningLoss: 0.013097\twhole_loss: 0.057351 \n",
            "Train Epoch: 121 [400/486 (82%)]\tlearningLoss: 0.013516\twhole_loss: 0.073440 \n",
            "Train Epoch: 121 [416/486 (86%)]\tlearningLoss: 0.013534\twhole_loss: 0.003170 \n",
            "Train Epoch: 121 [432/486 (89%)]\tlearningLoss: 0.013800\twhole_loss: 0.046442 \n",
            "Train Epoch: 121 [448/486 (92%)]\tlearningLoss: 0.013921\twhole_loss: 0.021190 \n",
            "Train Epoch: 121 [464/486 (95%)]\tlearningLoss: 0.014098\twhole_loss: 0.031004 \n",
            "Train Epoch: 121 [180/486 (37%)]\tlearningLoss: 0.014249\twhole_loss: 0.026384 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 1, 1, 1, 1, 1, 1, 2, 0, 2, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.0366, Accuracy: 67/75(89%)\n",
            "\n",
            "Train Epoch: 122 [0/486 (0%)]\tlearningLoss: 0.000891\twhole_loss: 0.155970 \n",
            "Train Epoch: 122 [16/486 (3%)]\tlearningLoss: 0.001150\twhole_loss: 0.045201 \n",
            "Train Epoch: 122 [32/486 (7%)]\tlearningLoss: 0.001799\twhole_loss: 0.113721 \n",
            "Train Epoch: 122 [48/486 (10%)]\tlearningLoss: 0.003141\twhole_loss: 0.234771 \n",
            "Train Epoch: 122 [64/486 (13%)]\tlearningLoss: 0.003267\twhole_loss: 0.022022 \n",
            "Train Epoch: 122 [80/486 (16%)]\tlearningLoss: 0.003328\twhole_loss: 0.010676 \n",
            "Train Epoch: 122 [96/486 (20%)]\tlearningLoss: 0.003518\twhole_loss: 0.033208 \n",
            "Train Epoch: 122 [112/486 (23%)]\tlearningLoss: 0.003532\twhole_loss: 0.002469 \n",
            "Train Epoch: 122 [128/486 (26%)]\tlearningLoss: 0.003660\twhole_loss: 0.022397 \n",
            "Train Epoch: 122 [144/486 (30%)]\tlearningLoss: 0.003803\twhole_loss: 0.025174 \n",
            "Train Epoch: 122 [160/486 (33%)]\tlearningLoss: 0.004984\twhole_loss: 0.206522 \n",
            "Train Epoch: 122 [176/486 (36%)]\tlearningLoss: 0.005242\twhole_loss: 0.045155 \n",
            "Train Epoch: 122 [192/486 (40%)]\tlearningLoss: 0.005454\twhole_loss: 0.037149 \n",
            "Train Epoch: 122 [208/486 (43%)]\tlearningLoss: 0.005717\twhole_loss: 0.046091 \n",
            "Train Epoch: 122 [224/486 (46%)]\tlearningLoss: 0.006097\twhole_loss: 0.066514 \n",
            "Train Epoch: 122 [240/486 (49%)]\tlearningLoss: 0.006159\twhole_loss: 0.010814 \n",
            "Train Epoch: 122 [256/486 (53%)]\tlearningLoss: 0.006187\twhole_loss: 0.004796 \n",
            "Train Epoch: 122 [272/486 (56%)]\tlearningLoss: 0.006752\twhole_loss: 0.099034 \n",
            "Train Epoch: 122 [288/486 (59%)]\tlearningLoss: 0.006858\twhole_loss: 0.018489 \n",
            "Train Epoch: 122 [304/486 (63%)]\tlearningLoss: 0.006886\twhole_loss: 0.004814 \n",
            "Train Epoch: 122 [320/486 (66%)]\tlearningLoss: 0.007273\twhole_loss: 0.067723 \n",
            "Train Epoch: 122 [336/486 (69%)]\tlearningLoss: 0.007364\twhole_loss: 0.016011 \n",
            "Train Epoch: 122 [352/486 (72%)]\tlearningLoss: 0.007735\twhole_loss: 0.064875 \n",
            "Train Epoch: 122 [368/486 (76%)]\tlearningLoss: 0.007803\twhole_loss: 0.011934 \n",
            "Train Epoch: 122 [384/486 (79%)]\tlearningLoss: 0.007879\twhole_loss: 0.013292 \n",
            "Train Epoch: 122 [400/486 (82%)]\tlearningLoss: 0.007930\twhole_loss: 0.008922 \n",
            "Train Epoch: 122 [416/486 (86%)]\tlearningLoss: 0.008291\twhole_loss: 0.063246 \n",
            "Train Epoch: 122 [432/486 (89%)]\tlearningLoss: 0.008444\twhole_loss: 0.026715 \n",
            "Train Epoch: 122 [448/486 (92%)]\tlearningLoss: 0.008506\twhole_loss: 0.010799 \n",
            "Train Epoch: 122 [464/486 (95%)]\tlearningLoss: 0.008701\twhole_loss: 0.034234 \n",
            "Train Epoch: 122 [180/486 (37%)]\tlearningLoss: 0.008715\twhole_loss: 0.002448 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.2249, Accuracy: 69/75(92%)\n",
            "\n",
            "Train Epoch: 123 [0/486 (0%)]\tlearningLoss: 0.000098\twhole_loss: 0.017109 \n",
            "Train Epoch: 123 [16/486 (3%)]\tlearningLoss: 0.000174\twhole_loss: 0.013321 \n",
            "Train Epoch: 123 [32/486 (7%)]\tlearningLoss: 0.000179\twhole_loss: 0.000922 \n",
            "Train Epoch: 123 [48/486 (10%)]\tlearningLoss: 0.000249\twhole_loss: 0.012151 \n",
            "Train Epoch: 123 [64/486 (13%)]\tlearningLoss: 0.000325\twhole_loss: 0.013307 \n",
            "Train Epoch: 123 [80/486 (16%)]\tlearningLoss: 0.000367\twhole_loss: 0.007467 \n",
            "Train Epoch: 123 [96/486 (20%)]\tlearningLoss: 0.000396\twhole_loss: 0.005017 \n",
            "Train Epoch: 123 [112/486 (23%)]\tlearningLoss: 0.000487\twhole_loss: 0.015993 \n",
            "Train Epoch: 123 [128/486 (26%)]\tlearningLoss: 0.000800\twhole_loss: 0.054658 \n",
            "Train Epoch: 123 [144/486 (30%)]\tlearningLoss: 0.000807\twhole_loss: 0.001323 \n",
            "Train Epoch: 123 [160/486 (33%)]\tlearningLoss: 0.000848\twhole_loss: 0.007185 \n",
            "Train Epoch: 123 [176/486 (36%)]\tlearningLoss: 0.000919\twhole_loss: 0.012422 \n",
            "Train Epoch: 123 [192/486 (40%)]\tlearningLoss: 0.000944\twhole_loss: 0.004357 \n",
            "Train Epoch: 123 [208/486 (43%)]\tlearningLoss: 0.001104\twhole_loss: 0.028038 \n",
            "Train Epoch: 123 [224/486 (46%)]\tlearningLoss: 0.001326\twhole_loss: 0.038770 \n",
            "Train Epoch: 123 [240/486 (49%)]\tlearningLoss: 0.001582\twhole_loss: 0.044804 \n",
            "Train Epoch: 123 [256/486 (53%)]\tlearningLoss: 0.001603\twhole_loss: 0.003689 \n",
            "Train Epoch: 123 [272/486 (56%)]\tlearningLoss: 0.001613\twhole_loss: 0.001823 \n",
            "Train Epoch: 123 [288/486 (59%)]\tlearningLoss: 0.001624\twhole_loss: 0.001888 \n",
            "Train Epoch: 123 [304/486 (63%)]\tlearningLoss: 0.001888\twhole_loss: 0.046216 \n",
            "Train Epoch: 123 [320/486 (66%)]\tlearningLoss: 0.001985\twhole_loss: 0.016902 \n",
            "Train Epoch: 123 [336/486 (69%)]\tlearningLoss: 0.002027\twhole_loss: 0.007342 \n",
            "Train Epoch: 123 [352/486 (72%)]\tlearningLoss: 0.002269\twhole_loss: 0.042452 \n",
            "Train Epoch: 123 [368/486 (76%)]\tlearningLoss: 0.002276\twhole_loss: 0.001113 \n",
            "Train Epoch: 123 [384/486 (79%)]\tlearningLoss: 0.002277\twhole_loss: 0.000265 \n",
            "Train Epoch: 123 [400/486 (82%)]\tlearningLoss: 0.002288\twhole_loss: 0.001915 \n",
            "Train Epoch: 123 [416/486 (86%)]\tlearningLoss: 0.002295\twhole_loss: 0.001255 \n",
            "Train Epoch: 123 [432/486 (89%)]\tlearningLoss: 0.002296\twhole_loss: 0.000018 \n",
            "Train Epoch: 123 [448/486 (92%)]\tlearningLoss: 0.002358\twhole_loss: 0.010954 \n",
            "Train Epoch: 123 [464/486 (95%)]\tlearningLoss: 0.003209\twhole_loss: 0.148969 \n",
            "Train Epoch: 123 [180/486 (37%)]\tlearningLoss: 0.003213\twhole_loss: 0.000616 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.6419, Accuracy: 67/75(89%)\n",
            "\n",
            "Train Epoch: 124 [0/486 (0%)]\tlearningLoss: 0.000239\twhole_loss: 0.041854 \n",
            "Train Epoch: 124 [16/486 (3%)]\tlearningLoss: 0.000368\twhole_loss: 0.022506 \n",
            "Train Epoch: 124 [32/486 (7%)]\tlearningLoss: 0.000384\twhole_loss: 0.002854 \n",
            "Train Epoch: 124 [48/486 (10%)]\tlearningLoss: 0.000524\twhole_loss: 0.024551 \n",
            "Train Epoch: 124 [64/486 (13%)]\tlearningLoss: 0.000546\twhole_loss: 0.003748 \n",
            "Train Epoch: 124 [80/486 (16%)]\tlearningLoss: 0.000654\twhole_loss: 0.018911 \n",
            "Train Epoch: 124 [96/486 (20%)]\tlearningLoss: 0.000655\twhole_loss: 0.000274 \n",
            "Train Epoch: 124 [112/486 (23%)]\tlearningLoss: 0.000701\twhole_loss: 0.008011 \n",
            "Train Epoch: 124 [128/486 (26%)]\tlearningLoss: 0.000714\twhole_loss: 0.002315 \n",
            "Train Epoch: 124 [144/486 (30%)]\tlearningLoss: 0.000722\twhole_loss: 0.001396 \n",
            "Train Epoch: 124 [160/486 (33%)]\tlearningLoss: 0.000728\twhole_loss: 0.001038 \n",
            "Train Epoch: 124 [176/486 (36%)]\tlearningLoss: 0.001053\twhole_loss: 0.056745 \n",
            "Train Epoch: 124 [192/486 (40%)]\tlearningLoss: 0.001098\twhole_loss: 0.007977 \n",
            "Train Epoch: 124 [208/486 (43%)]\tlearningLoss: 0.001132\twhole_loss: 0.005990 \n",
            "Train Epoch: 124 [224/486 (46%)]\tlearningLoss: 0.001240\twhole_loss: 0.018761 \n",
            "Train Epoch: 124 [240/486 (49%)]\tlearningLoss: 0.001330\twhole_loss: 0.015812 \n",
            "Train Epoch: 124 [256/486 (53%)]\tlearningLoss: 0.001835\twhole_loss: 0.088301 \n",
            "Train Epoch: 124 [272/486 (56%)]\tlearningLoss: 0.001835\twhole_loss: 0.000114 \n",
            "Train Epoch: 124 [288/486 (59%)]\tlearningLoss: 0.001849\twhole_loss: 0.002404 \n",
            "Train Epoch: 124 [304/486 (63%)]\tlearningLoss: 0.001863\twhole_loss: 0.002438 \n",
            "Train Epoch: 124 [320/486 (66%)]\tlearningLoss: 0.001931\twhole_loss: 0.011856 \n",
            "Train Epoch: 124 [336/486 (69%)]\tlearningLoss: 0.002007\twhole_loss: 0.013285 \n",
            "Train Epoch: 124 [352/486 (72%)]\tlearningLoss: 0.002264\twhole_loss: 0.045031 \n",
            "Train Epoch: 124 [368/486 (76%)]\tlearningLoss: 0.002440\twhole_loss: 0.030848 \n",
            "Train Epoch: 124 [384/486 (79%)]\tlearningLoss: 0.002911\twhole_loss: 0.082455 \n",
            "Train Epoch: 124 [400/486 (82%)]\tlearningLoss: 0.002947\twhole_loss: 0.006283 \n",
            "Train Epoch: 124 [416/486 (86%)]\tlearningLoss: 0.002956\twhole_loss: 0.001498 \n",
            "Train Epoch: 124 [432/486 (89%)]\tlearningLoss: 0.002980\twhole_loss: 0.004197 \n",
            "Train Epoch: 124 [448/486 (92%)]\tlearningLoss: 0.002998\twhole_loss: 0.003258 \n",
            "Train Epoch: 124 [464/486 (95%)]\tlearningLoss: 0.003002\twhole_loss: 0.000654 \n",
            "Train Epoch: 124 [180/486 (37%)]\tlearningLoss: 0.003044\twhole_loss: 0.007391 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 2, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.4017, Accuracy: 66/75(88%)\n",
            "\n",
            "Train Epoch: 125 [0/486 (0%)]\tlearningLoss: 0.000002\twhole_loss: 0.000374 \n",
            "Train Epoch: 125 [16/486 (3%)]\tlearningLoss: 0.000012\twhole_loss: 0.001743 \n",
            "Train Epoch: 125 [32/486 (7%)]\tlearningLoss: 0.000056\twhole_loss: 0.007620 \n",
            "Train Epoch: 125 [48/486 (10%)]\tlearningLoss: 0.000072\twhole_loss: 0.002838 \n",
            "Train Epoch: 125 [64/486 (13%)]\tlearningLoss: 0.001089\twhole_loss: 0.178069 \n",
            "Train Epoch: 125 [80/486 (16%)]\tlearningLoss: 0.001116\twhole_loss: 0.004736 \n",
            "Train Epoch: 125 [96/486 (20%)]\tlearningLoss: 0.001134\twhole_loss: 0.002993 \n",
            "Train Epoch: 125 [112/486 (23%)]\tlearningLoss: 0.001286\twhole_loss: 0.026765 \n",
            "Train Epoch: 125 [128/486 (26%)]\tlearningLoss: 0.001308\twhole_loss: 0.003807 \n",
            "Train Epoch: 125 [144/486 (30%)]\tlearningLoss: 0.001329\twhole_loss: 0.003709 \n",
            "Train Epoch: 125 [160/486 (33%)]\tlearningLoss: 0.001396\twhole_loss: 0.011664 \n",
            "Train Epoch: 125 [176/486 (36%)]\tlearningLoss: 0.001416\twhole_loss: 0.003413 \n",
            "Train Epoch: 125 [192/486 (40%)]\tlearningLoss: 0.001424\twhole_loss: 0.001443 \n",
            "Train Epoch: 125 [208/486 (43%)]\tlearningLoss: 0.001466\twhole_loss: 0.007448 \n",
            "Train Epoch: 125 [224/486 (46%)]\tlearningLoss: 0.001487\twhole_loss: 0.003643 \n",
            "Train Epoch: 125 [240/486 (49%)]\tlearningLoss: 0.001498\twhole_loss: 0.001820 \n",
            "Train Epoch: 125 [256/486 (53%)]\tlearningLoss: 0.001507\twhole_loss: 0.001585 \n",
            "Train Epoch: 125 [272/486 (56%)]\tlearningLoss: 0.001510\twhole_loss: 0.000573 \n",
            "Train Epoch: 125 [288/486 (59%)]\tlearningLoss: 0.001537\twhole_loss: 0.004792 \n",
            "Train Epoch: 125 [304/486 (63%)]\tlearningLoss: 0.001698\twhole_loss: 0.028198 \n",
            "Train Epoch: 125 [320/486 (66%)]\tlearningLoss: 0.001715\twhole_loss: 0.002955 \n",
            "Train Epoch: 125 [336/486 (69%)]\tlearningLoss: 0.001735\twhole_loss: 0.003425 \n",
            "Train Epoch: 125 [352/486 (72%)]\tlearningLoss: 0.001880\twhole_loss: 0.025324 \n",
            "Train Epoch: 125 [368/486 (76%)]\tlearningLoss: 0.001935\twhole_loss: 0.009721 \n",
            "Train Epoch: 125 [384/486 (79%)]\tlearningLoss: 0.001961\twhole_loss: 0.004574 \n",
            "Train Epoch: 125 [400/486 (82%)]\tlearningLoss: 0.001964\twhole_loss: 0.000544 \n",
            "Train Epoch: 125 [416/486 (86%)]\tlearningLoss: 0.001965\twhole_loss: 0.000057 \n",
            "Train Epoch: 125 [432/486 (89%)]\tlearningLoss: 0.002026\twhole_loss: 0.010756 \n",
            "Train Epoch: 125 [448/486 (92%)]\tlearningLoss: 0.002242\twhole_loss: 0.037719 \n",
            "Train Epoch: 125 [464/486 (95%)]\tlearningLoss: 0.002284\twhole_loss: 0.007411 \n",
            "Train Epoch: 125 [180/486 (37%)]\tlearningLoss: 0.002289\twhole_loss: 0.000915 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 2, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.1645, Accuracy: 66/75(88%)\n",
            "\n",
            "Train Epoch: 126 [0/486 (0%)]\tlearningLoss: 0.000021\twhole_loss: 0.003725 \n",
            "Train Epoch: 126 [16/486 (3%)]\tlearningLoss: 0.000043\twhole_loss: 0.003810 \n",
            "Train Epoch: 126 [32/486 (7%)]\tlearningLoss: 0.000048\twhole_loss: 0.000847 \n",
            "Train Epoch: 126 [48/486 (10%)]\tlearningLoss: 0.000106\twhole_loss: 0.010216 \n",
            "Train Epoch: 126 [64/486 (13%)]\tlearningLoss: 0.000108\twhole_loss: 0.000273 \n",
            "Train Epoch: 126 [80/486 (16%)]\tlearningLoss: 0.000125\twhole_loss: 0.003036 \n",
            "Train Epoch: 126 [96/486 (20%)]\tlearningLoss: 0.000131\twhole_loss: 0.001081 \n",
            "Train Epoch: 126 [112/486 (23%)]\tlearningLoss: 0.000135\twhole_loss: 0.000692 \n",
            "Train Epoch: 126 [128/486 (26%)]\tlearningLoss: 0.000143\twhole_loss: 0.001405 \n",
            "Train Epoch: 126 [144/486 (30%)]\tlearningLoss: 0.000162\twhole_loss: 0.003341 \n",
            "Train Epoch: 126 [160/486 (33%)]\tlearningLoss: 0.000401\twhole_loss: 0.041736 \n",
            "Train Epoch: 126 [176/486 (36%)]\tlearningLoss: 0.000473\twhole_loss: 0.012624 \n",
            "Train Epoch: 126 [192/486 (40%)]\tlearningLoss: 0.000552\twhole_loss: 0.013840 \n",
            "Train Epoch: 126 [208/486 (43%)]\tlearningLoss: 0.000553\twhole_loss: 0.000205 \n",
            "Train Epoch: 126 [224/486 (46%)]\tlearningLoss: 0.000556\twhole_loss: 0.000521 \n",
            "Train Epoch: 126 [240/486 (49%)]\tlearningLoss: 0.000566\twhole_loss: 0.001668 \n",
            "Train Epoch: 126 [256/486 (53%)]\tlearningLoss: 0.000626\twhole_loss: 0.010449 \n",
            "Train Epoch: 126 [272/486 (56%)]\tlearningLoss: 0.000631\twhole_loss: 0.000955 \n",
            "Train Epoch: 126 [288/486 (59%)]\tlearningLoss: 0.000656\twhole_loss: 0.004417 \n",
            "Train Epoch: 126 [304/486 (63%)]\tlearningLoss: 0.000703\twhole_loss: 0.008130 \n",
            "Train Epoch: 126 [320/486 (66%)]\tlearningLoss: 0.000730\twhole_loss: 0.004794 \n",
            "Train Epoch: 126 [336/486 (69%)]\tlearningLoss: 0.000733\twhole_loss: 0.000433 \n",
            "Train Epoch: 126 [352/486 (72%)]\tlearningLoss: 0.000744\twhole_loss: 0.002043 \n",
            "Train Epoch: 126 [368/486 (76%)]\tlearningLoss: 0.000762\twhole_loss: 0.003025 \n",
            "Train Epoch: 126 [384/486 (79%)]\tlearningLoss: 0.000767\twhole_loss: 0.000899 \n",
            "Train Epoch: 126 [400/486 (82%)]\tlearningLoss: 0.000771\twhole_loss: 0.000688 \n",
            "Train Epoch: 126 [416/486 (86%)]\tlearningLoss: 0.000798\twhole_loss: 0.004717 \n",
            "Train Epoch: 126 [432/486 (89%)]\tlearningLoss: 0.000801\twhole_loss: 0.000605 \n",
            "Train Epoch: 126 [448/486 (92%)]\tlearningLoss: 0.000925\twhole_loss: 0.021615 \n",
            "Train Epoch: 126 [464/486 (95%)]\tlearningLoss: 0.000943\twhole_loss: 0.003231 \n",
            "Train Epoch: 126 [180/486 (37%)]\tlearningLoss: 0.000968\twhole_loss: 0.004445 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 2, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.6947, Accuracy: 67/75(89%)\n",
            "\n",
            "Train Epoch: 127 [0/486 (0%)]\tlearningLoss: 0.000028\twhole_loss: 0.004910 \n",
            "Train Epoch: 127 [16/486 (3%)]\tlearningLoss: 0.000039\twhole_loss: 0.001895 \n",
            "Train Epoch: 127 [32/486 (7%)]\tlearningLoss: 0.000052\twhole_loss: 0.002378 \n",
            "Train Epoch: 127 [48/486 (10%)]\tlearningLoss: 0.000056\twhole_loss: 0.000584 \n",
            "Train Epoch: 127 [64/486 (13%)]\tlearningLoss: 0.000074\twhole_loss: 0.003252 \n",
            "Train Epoch: 127 [80/486 (16%)]\tlearningLoss: 0.000109\twhole_loss: 0.006007 \n",
            "Train Epoch: 127 [96/486 (20%)]\tlearningLoss: 0.000124\twhole_loss: 0.002611 \n",
            "Train Epoch: 127 [112/486 (23%)]\tlearningLoss: 0.000150\twhole_loss: 0.004692 \n",
            "Train Epoch: 127 [128/486 (26%)]\tlearningLoss: 0.000196\twhole_loss: 0.007961 \n",
            "Train Epoch: 127 [144/486 (30%)]\tlearningLoss: 0.000246\twhole_loss: 0.008704 \n",
            "Train Epoch: 127 [160/486 (33%)]\tlearningLoss: 0.000316\twhole_loss: 0.012257 \n",
            "Train Epoch: 127 [176/486 (36%)]\tlearningLoss: 0.000336\twhole_loss: 0.003603 \n",
            "Train Epoch: 127 [192/486 (40%)]\tlearningLoss: 0.000374\twhole_loss: 0.006514 \n",
            "Train Epoch: 127 [208/486 (43%)]\tlearningLoss: 0.000390\twhole_loss: 0.002852 \n",
            "Train Epoch: 127 [224/486 (46%)]\tlearningLoss: 0.000394\twhole_loss: 0.000732 \n",
            "Train Epoch: 127 [240/486 (49%)]\tlearningLoss: 0.000398\twhole_loss: 0.000686 \n",
            "Train Epoch: 127 [256/486 (53%)]\tlearningLoss: 0.000513\twhole_loss: 0.020148 \n",
            "Train Epoch: 127 [272/486 (56%)]\tlearningLoss: 0.000515\twhole_loss: 0.000271 \n",
            "Train Epoch: 127 [288/486 (59%)]\tlearningLoss: 0.000521\twhole_loss: 0.001144 \n",
            "Train Epoch: 127 [304/486 (63%)]\tlearningLoss: 0.000527\twhole_loss: 0.001084 \n",
            "Train Epoch: 127 [320/486 (66%)]\tlearningLoss: 0.000531\twhole_loss: 0.000683 \n",
            "Train Epoch: 127 [336/486 (69%)]\tlearningLoss: 0.000588\twhole_loss: 0.009857 \n",
            "Train Epoch: 127 [352/486 (72%)]\tlearningLoss: 0.000593\twhole_loss: 0.000987 \n",
            "Train Epoch: 127 [368/486 (76%)]\tlearningLoss: 0.000599\twhole_loss: 0.001093 \n",
            "Train Epoch: 127 [384/486 (79%)]\tlearningLoss: 0.000680\twhole_loss: 0.014041 \n",
            "Train Epoch: 127 [400/486 (82%)]\tlearningLoss: 0.000693\twhole_loss: 0.002366 \n",
            "Train Epoch: 127 [416/486 (86%)]\tlearningLoss: 0.000748\twhole_loss: 0.009541 \n",
            "Train Epoch: 127 [432/486 (89%)]\tlearningLoss: 0.000764\twhole_loss: 0.002936 \n",
            "Train Epoch: 127 [448/486 (92%)]\tlearningLoss: 0.000772\twhole_loss: 0.001368 \n",
            "Train Epoch: 127 [464/486 (95%)]\tlearningLoss: 0.000785\twhole_loss: 0.002179 \n",
            "Train Epoch: 127 [180/486 (37%)]\tlearningLoss: 0.000793\twhole_loss: 0.001372 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.3043, Accuracy: 68/75(91%)\n",
            "\n",
            "Train Epoch: 128 [0/486 (0%)]\tlearningLoss: 0.000003\twhole_loss: 0.000589 \n",
            "Train Epoch: 128 [16/486 (3%)]\tlearningLoss: 0.000007\twhole_loss: 0.000594 \n",
            "Train Epoch: 128 [32/486 (7%)]\tlearningLoss: 0.000017\twhole_loss: 0.001806 \n",
            "Train Epoch: 128 [48/486 (10%)]\tlearningLoss: 0.000070\twhole_loss: 0.009335 \n",
            "Train Epoch: 128 [64/486 (13%)]\tlearningLoss: 0.000075\twhole_loss: 0.000783 \n",
            "Train Epoch: 128 [80/486 (16%)]\tlearningLoss: 0.000078\twhole_loss: 0.000585 \n",
            "Train Epoch: 128 [96/486 (20%)]\tlearningLoss: 0.000102\twhole_loss: 0.004216 \n",
            "Train Epoch: 128 [112/486 (23%)]\tlearningLoss: 0.000106\twhole_loss: 0.000575 \n",
            "Train Epoch: 128 [128/486 (26%)]\tlearningLoss: 0.000116\twhole_loss: 0.001820 \n",
            "Train Epoch: 128 [144/486 (30%)]\tlearningLoss: 0.000119\twhole_loss: 0.000540 \n",
            "Train Epoch: 128 [160/486 (33%)]\tlearningLoss: 0.000242\twhole_loss: 0.021553 \n",
            "Train Epoch: 128 [176/486 (36%)]\tlearningLoss: 0.000248\twhole_loss: 0.001034 \n",
            "Train Epoch: 128 [192/486 (40%)]\tlearningLoss: 0.000253\twhole_loss: 0.000763 \n",
            "Train Epoch: 128 [208/486 (43%)]\tlearningLoss: 0.000334\twhole_loss: 0.014269 \n",
            "Train Epoch: 128 [224/486 (46%)]\tlearningLoss: 0.000377\twhole_loss: 0.007528 \n",
            "Train Epoch: 128 [240/486 (49%)]\tlearningLoss: 0.000391\twhole_loss: 0.002509 \n",
            "Train Epoch: 128 [256/486 (53%)]\tlearningLoss: 0.001477\twhole_loss: 0.190005 \n",
            "Train Epoch: 128 [272/486 (56%)]\tlearningLoss: 0.003646\twhole_loss: 0.379466 \n",
            "Train Epoch: 128 [288/486 (59%)]\tlearningLoss: 0.003656\twhole_loss: 0.001786 \n",
            "Train Epoch: 128 [304/486 (63%)]\tlearningLoss: 0.003770\twhole_loss: 0.020024 \n",
            "Train Epoch: 128 [320/486 (66%)]\tlearningLoss: 0.003786\twhole_loss: 0.002692 \n",
            "Train Epoch: 128 [336/486 (69%)]\tlearningLoss: 0.003823\twhole_loss: 0.006639 \n",
            "Train Epoch: 128 [352/486 (72%)]\tlearningLoss: 0.003838\twhole_loss: 0.002497 \n",
            "Train Epoch: 128 [368/486 (76%)]\tlearningLoss: 0.003848\twhole_loss: 0.001831 \n",
            "Train Epoch: 128 [384/486 (79%)]\tlearningLoss: 0.003850\twhole_loss: 0.000266 \n",
            "Train Epoch: 128 [400/486 (82%)]\tlearningLoss: 0.003850\twhole_loss: 0.000013 \n",
            "Train Epoch: 128 [416/486 (86%)]\tlearningLoss: 0.003854\twhole_loss: 0.000703 \n",
            "Train Epoch: 128 [432/486 (89%)]\tlearningLoss: 0.003871\twhole_loss: 0.003056 \n",
            "Train Epoch: 128 [448/486 (92%)]\tlearningLoss: 0.004064\twhole_loss: 0.033772 \n",
            "Train Epoch: 128 [464/486 (95%)]\tlearningLoss: 0.004135\twhole_loss: 0.012343 \n",
            "Train Epoch: 128 [180/486 (37%)]\tlearningLoss: 0.005067\twhole_loss: 0.163186 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 2, 1, 2, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:6.7703, Accuracy: 65/75(87%)\n",
            "\n",
            "Train Epoch: 129 [0/486 (0%)]\tlearningLoss: 0.000814\twhole_loss: 0.142452 \n",
            "Train Epoch: 129 [16/486 (3%)]\tlearningLoss: 0.000946\twhole_loss: 0.023132 \n",
            "Train Epoch: 129 [32/486 (7%)]\tlearningLoss: 0.002766\twhole_loss: 0.318488 \n",
            "Train Epoch: 129 [48/486 (10%)]\tlearningLoss: 0.002773\twhole_loss: 0.001244 \n",
            "Train Epoch: 129 [64/486 (13%)]\tlearningLoss: 0.002823\twhole_loss: 0.008682 \n",
            "Train Epoch: 129 [80/486 (16%)]\tlearningLoss: 0.003241\twhole_loss: 0.073224 \n",
            "Train Epoch: 129 [96/486 (20%)]\tlearningLoss: 0.003603\twhole_loss: 0.063287 \n",
            "Train Epoch: 129 [112/486 (23%)]\tlearningLoss: 0.004008\twhole_loss: 0.070972 \n",
            "Train Epoch: 129 [128/486 (26%)]\tlearningLoss: 0.004030\twhole_loss: 0.003743 \n",
            "Train Epoch: 129 [144/486 (30%)]\tlearningLoss: 0.004121\twhole_loss: 0.015892 \n",
            "Train Epoch: 129 [160/486 (33%)]\tlearningLoss: 0.004206\twhole_loss: 0.014949 \n",
            "Train Epoch: 129 [176/486 (36%)]\tlearningLoss: 0.004962\twhole_loss: 0.132199 \n",
            "Train Epoch: 129 [192/486 (40%)]\tlearningLoss: 0.005318\twhole_loss: 0.062352 \n",
            "Train Epoch: 129 [208/486 (43%)]\tlearningLoss: 0.005354\twhole_loss: 0.006285 \n",
            "Train Epoch: 129 [224/486 (46%)]\tlearningLoss: 0.005432\twhole_loss: 0.013690 \n",
            "Train Epoch: 129 [240/486 (49%)]\tlearningLoss: 0.005940\twhole_loss: 0.088968 \n",
            "Train Epoch: 129 [256/486 (53%)]\tlearningLoss: 0.005982\twhole_loss: 0.007336 \n",
            "Train Epoch: 129 [272/486 (56%)]\tlearningLoss: 0.005992\twhole_loss: 0.001649 \n",
            "Train Epoch: 129 [288/486 (59%)]\tlearningLoss: 0.006122\twhole_loss: 0.022860 \n",
            "Train Epoch: 129 [304/486 (63%)]\tlearningLoss: 0.006219\twhole_loss: 0.017004 \n",
            "Train Epoch: 129 [320/486 (66%)]\tlearningLoss: 0.006674\twhole_loss: 0.079569 \n",
            "Train Epoch: 129 [336/486 (69%)]\tlearningLoss: 0.007425\twhole_loss: 0.131325 \n",
            "Train Epoch: 129 [352/486 (72%)]\tlearningLoss: 0.008728\twhole_loss: 0.228102 \n",
            "Train Epoch: 129 [368/486 (76%)]\tlearningLoss: 0.009409\twhole_loss: 0.119115 \n",
            "Train Epoch: 129 [384/486 (79%)]\tlearningLoss: 0.009443\twhole_loss: 0.006055 \n",
            "Train Epoch: 129 [400/486 (82%)]\tlearningLoss: 0.009487\twhole_loss: 0.007646 \n",
            "Train Epoch: 129 [416/486 (86%)]\tlearningLoss: 0.010464\twhole_loss: 0.170962 \n",
            "Train Epoch: 129 [432/486 (89%)]\tlearningLoss: 0.012392\twhole_loss: 0.337334 \n",
            "Train Epoch: 129 [448/486 (92%)]\tlearningLoss: 0.013010\twhole_loss: 0.108305 \n",
            "Train Epoch: 129 [464/486 (95%)]\tlearningLoss: 0.013237\twhole_loss: 0.039663 \n",
            "Train Epoch: 129 [180/486 (37%)]\tlearningLoss: 0.019987\twhole_loss: 1.181312 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.0304, Accuracy: 68/75(91%)\n",
            "\n",
            "Train Epoch: 130 [0/486 (0%)]\tlearningLoss: 0.000032\twhole_loss: 0.005614 \n",
            "Train Epoch: 130 [16/486 (3%)]\tlearningLoss: 0.001079\twhole_loss: 0.183154 \n",
            "Train Epoch: 130 [32/486 (7%)]\tlearningLoss: 0.007990\twhole_loss: 1.209409 \n",
            "Train Epoch: 130 [48/486 (10%)]\tlearningLoss: 0.008181\twhole_loss: 0.033516 \n",
            "Train Epoch: 130 [64/486 (13%)]\tlearningLoss: 0.008463\twhole_loss: 0.049280 \n",
            "Train Epoch: 130 [80/486 (16%)]\tlearningLoss: 0.009568\twhole_loss: 0.193353 \n",
            "Train Epoch: 130 [96/486 (20%)]\tlearningLoss: 0.010739\twhole_loss: 0.204978 \n",
            "Train Epoch: 130 [112/486 (23%)]\tlearningLoss: 0.010742\twhole_loss: 0.000591 \n",
            "Train Epoch: 130 [128/486 (26%)]\tlearningLoss: 0.013079\twhole_loss: 0.408931 \n",
            "Train Epoch: 130 [144/486 (30%)]\tlearningLoss: 0.013621\twhole_loss: 0.094915 \n",
            "Train Epoch: 130 [160/486 (33%)]\tlearningLoss: 0.015358\twhole_loss: 0.303915 \n",
            "Train Epoch: 130 [176/486 (36%)]\tlearningLoss: 0.015516\twhole_loss: 0.027628 \n",
            "Train Epoch: 130 [192/486 (40%)]\tlearningLoss: 0.016108\twhole_loss: 0.103606 \n",
            "Train Epoch: 130 [208/486 (43%)]\tlearningLoss: 0.017070\twhole_loss: 0.168399 \n",
            "Train Epoch: 130 [224/486 (46%)]\tlearningLoss: 0.017112\twhole_loss: 0.007385 \n",
            "Train Epoch: 130 [240/486 (49%)]\tlearningLoss: 0.017578\twhole_loss: 0.081554 \n",
            "Train Epoch: 130 [256/486 (53%)]\tlearningLoss: 0.018877\twhole_loss: 0.227213 \n",
            "Train Epoch: 130 [272/486 (56%)]\tlearningLoss: 0.019216\twhole_loss: 0.059297 \n",
            "Train Epoch: 130 [288/486 (59%)]\tlearningLoss: 0.022128\twhole_loss: 0.509740 \n",
            "Train Epoch: 130 [304/486 (63%)]\tlearningLoss: 0.023423\twhole_loss: 0.226576 \n",
            "Train Epoch: 130 [320/486 (66%)]\tlearningLoss: 0.023830\twhole_loss: 0.071182 \n",
            "Train Epoch: 130 [336/486 (69%)]\tlearningLoss: 0.025127\twhole_loss: 0.227008 \n",
            "Train Epoch: 130 [352/486 (72%)]\tlearningLoss: 0.025336\twhole_loss: 0.036593 \n",
            "Train Epoch: 130 [368/486 (76%)]\tlearningLoss: 0.026246\twhole_loss: 0.159174 \n",
            "Train Epoch: 130 [384/486 (79%)]\tlearningLoss: 0.028502\twhole_loss: 0.394799 \n",
            "Train Epoch: 130 [400/486 (82%)]\tlearningLoss: 0.029288\twhole_loss: 0.137551 \n",
            "Train Epoch: 130 [416/486 (86%)]\tlearningLoss: 0.030215\twhole_loss: 0.162345 \n",
            "Train Epoch: 130 [432/486 (89%)]\tlearningLoss: 0.031344\twhole_loss: 0.197448 \n",
            "Train Epoch: 130 [448/486 (92%)]\tlearningLoss: 0.034726\twhole_loss: 0.591831 \n",
            "Train Epoch: 130 [464/486 (95%)]\tlearningLoss: 0.035296\twhole_loss: 0.099843 \n",
            "Train Epoch: 130 [180/486 (37%)]\tlearningLoss: 0.035841\twhole_loss: 0.095281 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([2, 0, 1, 1, 1, 1, 1, 2, 0, 2, 2, 2, 1, 2, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.5958, Accuracy: 61/75(81%)\n",
            "\n",
            "Train Epoch: 131 [0/486 (0%)]\tlearningLoss: 0.000809\twhole_loss: 0.141488 \n",
            "Train Epoch: 131 [16/486 (3%)]\tlearningLoss: 0.001849\twhole_loss: 0.182005 \n",
            "Train Epoch: 131 [32/486 (7%)]\tlearningLoss: 0.001972\twhole_loss: 0.021602 \n",
            "Train Epoch: 131 [48/486 (10%)]\tlearningLoss: 0.002022\twhole_loss: 0.008682 \n",
            "Train Epoch: 131 [64/486 (13%)]\tlearningLoss: 0.003653\twhole_loss: 0.285506 \n",
            "Train Epoch: 131 [80/486 (16%)]\tlearningLoss: 0.004831\twhole_loss: 0.206217 \n",
            "Train Epoch: 131 [96/486 (20%)]\tlearningLoss: 0.004868\twhole_loss: 0.006455 \n",
            "Train Epoch: 131 [112/486 (23%)]\tlearningLoss: 0.006370\twhole_loss: 0.262764 \n",
            "Train Epoch: 131 [128/486 (26%)]\tlearningLoss: 0.006666\twhole_loss: 0.051815 \n",
            "Train Epoch: 131 [144/486 (30%)]\tlearningLoss: 0.007010\twhole_loss: 0.060302 \n",
            "Train Epoch: 131 [160/486 (33%)]\tlearningLoss: 0.007611\twhole_loss: 0.105002 \n",
            "Train Epoch: 131 [176/486 (36%)]\tlearningLoss: 0.007637\twhole_loss: 0.004715 \n",
            "Train Epoch: 131 [192/486 (40%)]\tlearningLoss: 0.007810\twhole_loss: 0.030166 \n",
            "Train Epoch: 131 [208/486 (43%)]\tlearningLoss: 0.008737\twhole_loss: 0.162196 \n",
            "Train Epoch: 131 [224/486 (46%)]\tlearningLoss: 0.009139\twhole_loss: 0.070458 \n",
            "Train Epoch: 131 [240/486 (49%)]\tlearningLoss: 0.009308\twhole_loss: 0.029591 \n",
            "Train Epoch: 131 [256/486 (53%)]\tlearningLoss: 0.009591\twhole_loss: 0.049481 \n",
            "Train Epoch: 131 [272/486 (56%)]\tlearningLoss: 0.009971\twhole_loss: 0.066534 \n",
            "Train Epoch: 131 [288/486 (59%)]\tlearningLoss: 0.010168\twhole_loss: 0.034448 \n",
            "Train Epoch: 131 [304/486 (63%)]\tlearningLoss: 0.010237\twhole_loss: 0.012052 \n",
            "Train Epoch: 131 [320/486 (66%)]\tlearningLoss: 0.015400\twhole_loss: 0.903508 \n",
            "Train Epoch: 131 [336/486 (69%)]\tlearningLoss: 0.015987\twhole_loss: 0.102652 \n",
            "Train Epoch: 131 [352/486 (72%)]\tlearningLoss: 0.016219\twhole_loss: 0.040605 \n",
            "Train Epoch: 131 [368/486 (76%)]\tlearningLoss: 0.017096\twhole_loss: 0.153569 \n",
            "Train Epoch: 131 [384/486 (79%)]\tlearningLoss: 0.017637\twhole_loss: 0.094660 \n",
            "Train Epoch: 131 [400/486 (82%)]\tlearningLoss: 0.021243\twhole_loss: 0.631122 \n",
            "Train Epoch: 131 [416/486 (86%)]\tlearningLoss: 0.022450\twhole_loss: 0.211169 \n",
            "Train Epoch: 131 [432/486 (89%)]\tlearningLoss: 0.023350\twhole_loss: 0.157458 \n",
            "Train Epoch: 131 [448/486 (92%)]\tlearningLoss: 0.023391\twhole_loss: 0.007254 \n",
            "Train Epoch: 131 [464/486 (95%)]\tlearningLoss: 0.024530\twhole_loss: 0.199254 \n",
            "Train Epoch: 131 [180/486 (37%)]\tlearningLoss: 0.026981\twhole_loss: 0.428886 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 2, 2, 2, 0, 2], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:5.2804, Accuracy: 55/75(73%)\n",
            "\n",
            "Train Epoch: 132 [0/486 (0%)]\tlearningLoss: 0.000810\twhole_loss: 0.141754 \n",
            "Train Epoch: 132 [16/486 (3%)]\tlearningLoss: 0.001387\twhole_loss: 0.101033 \n",
            "Train Epoch: 132 [32/486 (7%)]\tlearningLoss: 0.001662\twhole_loss: 0.048056 \n",
            "Train Epoch: 132 [48/486 (10%)]\tlearningLoss: 0.002771\twhole_loss: 0.194026 \n",
            "Train Epoch: 132 [64/486 (13%)]\tlearningLoss: 0.003189\twhole_loss: 0.073212 \n",
            "Train Epoch: 132 [80/486 (16%)]\tlearningLoss: 0.003261\twhole_loss: 0.012626 \n",
            "Train Epoch: 132 [96/486 (20%)]\tlearningLoss: 0.008399\twhole_loss: 0.899180 \n",
            "Train Epoch: 132 [112/486 (23%)]\tlearningLoss: 0.009367\twhole_loss: 0.169310 \n",
            "Train Epoch: 132 [128/486 (26%)]\tlearningLoss: 0.011008\twhole_loss: 0.287235 \n",
            "Train Epoch: 132 [144/486 (30%)]\tlearningLoss: 0.011351\twhole_loss: 0.060031 \n",
            "Train Epoch: 132 [160/486 (33%)]\tlearningLoss: 0.011875\twhole_loss: 0.091745 \n",
            "Train Epoch: 132 [176/486 (36%)]\tlearningLoss: 0.011927\twhole_loss: 0.008999 \n",
            "Train Epoch: 132 [192/486 (40%)]\tlearningLoss: 0.013022\twhole_loss: 0.191726 \n",
            "Train Epoch: 132 [208/486 (43%)]\tlearningLoss: 0.013482\twhole_loss: 0.080358 \n",
            "Train Epoch: 132 [224/486 (46%)]\tlearningLoss: 0.013813\twhole_loss: 0.057928 \n",
            "Train Epoch: 132 [240/486 (49%)]\tlearningLoss: 0.013895\twhole_loss: 0.014346 \n",
            "Train Epoch: 132 [256/486 (53%)]\tlearningLoss: 0.014338\twhole_loss: 0.077589 \n",
            "Train Epoch: 132 [272/486 (56%)]\tlearningLoss: 0.015065\twhole_loss: 0.127150 \n",
            "Train Epoch: 132 [288/486 (59%)]\tlearningLoss: 0.015717\twhole_loss: 0.114247 \n",
            "Train Epoch: 132 [304/486 (63%)]\tlearningLoss: 0.016287\twhole_loss: 0.099596 \n",
            "Train Epoch: 132 [320/486 (66%)]\tlearningLoss: 0.016953\twhole_loss: 0.116675 \n",
            "Train Epoch: 132 [336/486 (69%)]\tlearningLoss: 0.017125\twhole_loss: 0.030092 \n",
            "Train Epoch: 132 [352/486 (72%)]\tlearningLoss: 0.017495\twhole_loss: 0.064771 \n",
            "Train Epoch: 132 [368/486 (76%)]\tlearningLoss: 0.018910\twhole_loss: 0.247607 \n",
            "Train Epoch: 132 [384/486 (79%)]\tlearningLoss: 0.019354\twhole_loss: 0.077680 \n",
            "Train Epoch: 132 [400/486 (82%)]\tlearningLoss: 0.019642\twhole_loss: 0.050367 \n",
            "Train Epoch: 132 [416/486 (86%)]\tlearningLoss: 0.021172\twhole_loss: 0.267816 \n",
            "Train Epoch: 132 [432/486 (89%)]\tlearningLoss: 0.021432\twhole_loss: 0.045515 \n",
            "Train Epoch: 132 [448/486 (92%)]\tlearningLoss: 0.021498\twhole_loss: 0.011502 \n",
            "Train Epoch: 132 [464/486 (95%)]\tlearningLoss: 0.021520\twhole_loss: 0.003810 \n",
            "Train Epoch: 132 [180/486 (37%)]\tlearningLoss: 0.021560\twhole_loss: 0.006969 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.8372, Accuracy: 66/75(88%)\n",
            "\n",
            "Train Epoch: 133 [0/486 (0%)]\tlearningLoss: 0.000660\twhole_loss: 0.115496 \n",
            "Train Epoch: 133 [16/486 (3%)]\tlearningLoss: 0.000981\twhole_loss: 0.056140 \n",
            "Train Epoch: 133 [32/486 (7%)]\tlearningLoss: 0.001127\twhole_loss: 0.025629 \n",
            "Train Epoch: 133 [48/486 (10%)]\tlearningLoss: 0.001243\twhole_loss: 0.020297 \n",
            "Train Epoch: 133 [64/486 (13%)]\tlearningLoss: 0.001658\twhole_loss: 0.072600 \n",
            "Train Epoch: 133 [80/486 (16%)]\tlearningLoss: 0.001700\twhole_loss: 0.007360 \n",
            "Train Epoch: 133 [96/486 (20%)]\tlearningLoss: 0.003580\twhole_loss: 0.329032 \n",
            "Train Epoch: 133 [112/486 (23%)]\tlearningLoss: 0.003892\twhole_loss: 0.054569 \n",
            "Train Epoch: 133 [128/486 (26%)]\tlearningLoss: 0.004049\twhole_loss: 0.027407 \n",
            "Train Epoch: 133 [144/486 (30%)]\tlearningLoss: 0.004625\twhole_loss: 0.100904 \n",
            "Train Epoch: 133 [160/486 (33%)]\tlearningLoss: 0.004715\twhole_loss: 0.015653 \n",
            "Train Epoch: 133 [176/486 (36%)]\tlearningLoss: 0.004887\twhole_loss: 0.030209 \n",
            "Train Epoch: 133 [192/486 (40%)]\tlearningLoss: 0.006282\twhole_loss: 0.244094 \n",
            "Train Epoch: 133 [208/486 (43%)]\tlearningLoss: 0.007063\twhole_loss: 0.136659 \n",
            "Train Epoch: 133 [224/486 (46%)]\tlearningLoss: 0.007137\twhole_loss: 0.012949 \n",
            "Train Epoch: 133 [240/486 (49%)]\tlearningLoss: 0.007483\twhole_loss: 0.060557 \n",
            "Train Epoch: 133 [256/486 (53%)]\tlearningLoss: 0.007577\twhole_loss: 0.016505 \n",
            "Train Epoch: 133 [272/486 (56%)]\tlearningLoss: 0.007667\twhole_loss: 0.015620 \n",
            "Train Epoch: 133 [288/486 (59%)]\tlearningLoss: 0.007746\twhole_loss: 0.013901 \n",
            "Train Epoch: 133 [304/486 (63%)]\tlearningLoss: 0.007940\twhole_loss: 0.033862 \n",
            "Train Epoch: 133 [320/486 (66%)]\tlearningLoss: 0.007956\twhole_loss: 0.002856 \n",
            "Train Epoch: 133 [336/486 (69%)]\tlearningLoss: 0.008097\twhole_loss: 0.024746 \n",
            "Train Epoch: 133 [352/486 (72%)]\tlearningLoss: 0.008849\twhole_loss: 0.131516 \n",
            "Train Epoch: 133 [368/486 (76%)]\tlearningLoss: 0.008922\twhole_loss: 0.012767 \n",
            "Train Epoch: 133 [384/486 (79%)]\tlearningLoss: 0.008973\twhole_loss: 0.008889 \n",
            "Train Epoch: 133 [400/486 (82%)]\tlearningLoss: 0.009039\twhole_loss: 0.011549 \n",
            "Train Epoch: 133 [416/486 (86%)]\tlearningLoss: 0.010405\twhole_loss: 0.239194 \n",
            "Train Epoch: 133 [432/486 (89%)]\tlearningLoss: 0.010485\twhole_loss: 0.013905 \n",
            "Train Epoch: 133 [448/486 (92%)]\tlearningLoss: 0.011272\twhole_loss: 0.137798 \n",
            "Train Epoch: 133 [464/486 (95%)]\tlearningLoss: 0.011367\twhole_loss: 0.016643 \n",
            "Train Epoch: 133 [180/486 (37%)]\tlearningLoss: 0.014226\twhole_loss: 0.500158 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 2, 1, 1, 1, 1, 1, 2, 0, 2, 2, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.0139, Accuracy: 64/75(85%)\n",
            "\n",
            "Train Epoch: 134 [0/486 (0%)]\tlearningLoss: 0.000013\twhole_loss: 0.002313 \n",
            "Train Epoch: 134 [16/486 (3%)]\tlearningLoss: 0.000080\twhole_loss: 0.011705 \n",
            "Train Epoch: 134 [32/486 (7%)]\tlearningLoss: 0.000482\twhole_loss: 0.070308 \n",
            "Train Epoch: 134 [48/486 (10%)]\tlearningLoss: 0.000834\twhole_loss: 0.061662 \n",
            "Train Epoch: 134 [64/486 (13%)]\tlearningLoss: 0.001082\twhole_loss: 0.043321 \n",
            "Train Epoch: 134 [80/486 (16%)]\tlearningLoss: 0.001770\twhole_loss: 0.120394 \n",
            "Train Epoch: 134 [96/486 (20%)]\tlearningLoss: 0.003226\twhole_loss: 0.254809 \n",
            "Train Epoch: 134 [112/486 (23%)]\tlearningLoss: 0.005042\twhole_loss: 0.317831 \n",
            "Train Epoch: 134 [128/486 (26%)]\tlearningLoss: 0.005169\twhole_loss: 0.022219 \n",
            "Train Epoch: 134 [144/486 (30%)]\tlearningLoss: 0.009389\twhole_loss: 0.738542 \n",
            "Train Epoch: 134 [160/486 (33%)]\tlearningLoss: 0.010480\twhole_loss: 0.190881 \n",
            "Train Epoch: 134 [176/486 (36%)]\tlearningLoss: 0.011166\twhole_loss: 0.120151 \n",
            "Train Epoch: 134 [192/486 (40%)]\tlearningLoss: 0.011261\twhole_loss: 0.016587 \n",
            "Train Epoch: 134 [208/486 (43%)]\tlearningLoss: 0.012427\twhole_loss: 0.204046 \n",
            "Train Epoch: 134 [224/486 (46%)]\tlearningLoss: 0.013246\twhole_loss: 0.143229 \n",
            "Train Epoch: 134 [240/486 (49%)]\tlearningLoss: 0.013715\twhole_loss: 0.082158 \n",
            "Train Epoch: 134 [256/486 (53%)]\tlearningLoss: 0.014842\twhole_loss: 0.197197 \n",
            "Train Epoch: 134 [272/486 (56%)]\tlearningLoss: 0.016178\twhole_loss: 0.233766 \n",
            "Train Epoch: 134 [288/486 (59%)]\tlearningLoss: 0.017086\twhole_loss: 0.158860 \n",
            "Train Epoch: 134 [304/486 (63%)]\tlearningLoss: 0.017325\twhole_loss: 0.041903 \n",
            "Train Epoch: 134 [320/486 (66%)]\tlearningLoss: 0.017972\twhole_loss: 0.113222 \n",
            "Train Epoch: 134 [336/486 (69%)]\tlearningLoss: 0.018481\twhole_loss: 0.089152 \n",
            "Train Epoch: 134 [352/486 (72%)]\tlearningLoss: 0.021407\twhole_loss: 0.512040 \n",
            "Train Epoch: 134 [368/486 (76%)]\tlearningLoss: 0.021727\twhole_loss: 0.055978 \n",
            "Train Epoch: 134 [384/486 (79%)]\tlearningLoss: 0.022540\twhole_loss: 0.142260 \n",
            "Train Epoch: 134 [400/486 (82%)]\tlearningLoss: 0.022960\twhole_loss: 0.073504 \n",
            "Train Epoch: 134 [416/486 (86%)]\tlearningLoss: 0.023222\twhole_loss: 0.045874 \n",
            "Train Epoch: 134 [432/486 (89%)]\tlearningLoss: 0.025310\twhole_loss: 0.365413 \n",
            "Train Epoch: 134 [448/486 (92%)]\tlearningLoss: 0.025352\twhole_loss: 0.007257 \n",
            "Train Epoch: 134 [464/486 (95%)]\tlearningLoss: 0.026333\twhole_loss: 0.171649 \n",
            "Train Epoch: 134 [180/486 (37%)]\tlearningLoss: 0.026370\twhole_loss: 0.006544 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:1.9884, Accuracy: 68/75(91%)\n",
            "\n",
            "Train Epoch: 135 [0/486 (0%)]\tlearningLoss: 0.000580\twhole_loss: 0.101451 \n",
            "Train Epoch: 135 [16/486 (3%)]\tlearningLoss: 0.001865\twhole_loss: 0.224854 \n",
            "Train Epoch: 135 [32/486 (7%)]\tlearningLoss: 0.002019\twhole_loss: 0.026986 \n",
            "Train Epoch: 135 [48/486 (10%)]\tlearningLoss: 0.002735\twhole_loss: 0.125306 \n",
            "Train Epoch: 135 [64/486 (13%)]\tlearningLoss: 0.003084\twhole_loss: 0.061111 \n",
            "Train Epoch: 135 [80/486 (16%)]\tlearningLoss: 0.003192\twhole_loss: 0.018950 \n",
            "Train Epoch: 135 [96/486 (20%)]\tlearningLoss: 0.003302\twhole_loss: 0.019265 \n",
            "Train Epoch: 135 [112/486 (23%)]\tlearningLoss: 0.003383\twhole_loss: 0.014086 \n",
            "Train Epoch: 135 [128/486 (26%)]\tlearningLoss: 0.003883\twhole_loss: 0.087491 \n",
            "Train Epoch: 135 [144/486 (30%)]\tlearningLoss: 0.004640\twhole_loss: 0.132552 \n",
            "Train Epoch: 135 [160/486 (33%)]\tlearningLoss: 0.004684\twhole_loss: 0.007603 \n",
            "Train Epoch: 135 [176/486 (36%)]\tlearningLoss: 0.004970\twhole_loss: 0.050094 \n",
            "Train Epoch: 135 [192/486 (40%)]\tlearningLoss: 0.005041\twhole_loss: 0.012383 \n",
            "Train Epoch: 135 [208/486 (43%)]\tlearningLoss: 0.005157\twhole_loss: 0.020349 \n",
            "Train Epoch: 135 [224/486 (46%)]\tlearningLoss: 0.005229\twhole_loss: 0.012580 \n",
            "Train Epoch: 135 [240/486 (49%)]\tlearningLoss: 0.005907\twhole_loss: 0.118749 \n",
            "Train Epoch: 135 [256/486 (53%)]\tlearningLoss: 0.006236\twhole_loss: 0.057412 \n",
            "Train Epoch: 135 [272/486 (56%)]\tlearningLoss: 0.006319\twhole_loss: 0.014636 \n",
            "Train Epoch: 135 [288/486 (59%)]\tlearningLoss: 0.007042\twhole_loss: 0.126540 \n",
            "Train Epoch: 135 [304/486 (63%)]\tlearningLoss: 0.007096\twhole_loss: 0.009406 \n",
            "Train Epoch: 135 [320/486 (66%)]\tlearningLoss: 0.007198\twhole_loss: 0.017800 \n",
            "Train Epoch: 135 [336/486 (69%)]\tlearningLoss: 0.007364\twhole_loss: 0.029017 \n",
            "Train Epoch: 135 [352/486 (72%)]\tlearningLoss: 0.007885\twhole_loss: 0.091343 \n",
            "Train Epoch: 135 [368/486 (76%)]\tlearningLoss: 0.008348\twhole_loss: 0.080986 \n",
            "Train Epoch: 135 [384/486 (79%)]\tlearningLoss: 0.008418\twhole_loss: 0.012188 \n",
            "Train Epoch: 135 [400/486 (82%)]\tlearningLoss: 0.008542\twhole_loss: 0.021719 \n",
            "Train Epoch: 135 [416/486 (86%)]\tlearningLoss: 0.008722\twhole_loss: 0.031516 \n",
            "Train Epoch: 135 [432/486 (89%)]\tlearningLoss: 0.008804\twhole_loss: 0.014344 \n",
            "Train Epoch: 135 [448/486 (92%)]\tlearningLoss: 0.008828\twhole_loss: 0.004156 \n",
            "Train Epoch: 135 [464/486 (95%)]\tlearningLoss: 0.009590\twhole_loss: 0.133435 \n",
            "Train Epoch: 135 [180/486 (37%)]\tlearningLoss: 0.009595\twhole_loss: 0.000887 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 2, 2, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.5199, Accuracy: 65/75(87%)\n",
            "\n",
            "Train Epoch: 136 [0/486 (0%)]\tlearningLoss: 0.001797\twhole_loss: 0.314531 \n",
            "Train Epoch: 136 [16/486 (3%)]\tlearningLoss: 0.001820\twhole_loss: 0.003973 \n",
            "Train Epoch: 136 [32/486 (7%)]\tlearningLoss: 0.001944\twhole_loss: 0.021776 \n",
            "Train Epoch: 136 [48/486 (10%)]\tlearningLoss: 0.002025\twhole_loss: 0.014130 \n",
            "Train Epoch: 136 [64/486 (13%)]\tlearningLoss: 0.002041\twhole_loss: 0.002804 \n",
            "Train Epoch: 136 [80/486 (16%)]\tlearningLoss: 0.002119\twhole_loss: 0.013633 \n",
            "Train Epoch: 136 [96/486 (20%)]\tlearningLoss: 0.002121\twhole_loss: 0.000298 \n",
            "Train Epoch: 136 [112/486 (23%)]\tlearningLoss: 0.002244\twhole_loss: 0.021551 \n",
            "Train Epoch: 136 [128/486 (26%)]\tlearningLoss: 0.002486\twhole_loss: 0.042363 \n",
            "Train Epoch: 136 [144/486 (30%)]\tlearningLoss: 0.002575\twhole_loss: 0.015639 \n",
            "Train Epoch: 136 [160/486 (33%)]\tlearningLoss: 0.002973\twhole_loss: 0.069497 \n",
            "Train Epoch: 136 [176/486 (36%)]\tlearningLoss: 0.003149\twhole_loss: 0.030807 \n",
            "Train Epoch: 136 [192/486 (40%)]\tlearningLoss: 0.003195\twhole_loss: 0.008210 \n",
            "Train Epoch: 136 [208/486 (43%)]\tlearningLoss: 0.003712\twhole_loss: 0.090405 \n",
            "Train Epoch: 136 [224/486 (46%)]\tlearningLoss: 0.003807\twhole_loss: 0.016576 \n",
            "Train Epoch: 136 [240/486 (49%)]\tlearningLoss: 0.003976\twhole_loss: 0.029576 \n",
            "Train Epoch: 136 [256/486 (53%)]\tlearningLoss: 0.004010\twhole_loss: 0.005954 \n",
            "Train Epoch: 136 [272/486 (56%)]\tlearningLoss: 0.004329\twhole_loss: 0.055919 \n",
            "Train Epoch: 136 [288/486 (59%)]\tlearningLoss: 0.004433\twhole_loss: 0.018147 \n",
            "Train Epoch: 136 [304/486 (63%)]\tlearningLoss: 0.004533\twhole_loss: 0.017524 \n",
            "Train Epoch: 136 [320/486 (66%)]\tlearningLoss: 0.004682\twhole_loss: 0.026059 \n",
            "Train Epoch: 136 [336/486 (69%)]\tlearningLoss: 0.004692\twhole_loss: 0.001735 \n",
            "Train Epoch: 136 [352/486 (72%)]\tlearningLoss: 0.004778\twhole_loss: 0.014999 \n",
            "Train Epoch: 136 [368/486 (76%)]\tlearningLoss: 0.004860\twhole_loss: 0.014336 \n",
            "Train Epoch: 136 [384/486 (79%)]\tlearningLoss: 0.004866\twhole_loss: 0.001153 \n",
            "Train Epoch: 136 [400/486 (82%)]\tlearningLoss: 0.004970\twhole_loss: 0.018168 \n",
            "Train Epoch: 136 [416/486 (86%)]\tlearningLoss: 0.005021\twhole_loss: 0.008971 \n",
            "Train Epoch: 136 [432/486 (89%)]\tlearningLoss: 0.005101\twhole_loss: 0.013886 \n",
            "Train Epoch: 136 [448/486 (92%)]\tlearningLoss: 0.005350\twhole_loss: 0.043690 \n",
            "Train Epoch: 136 [464/486 (95%)]\tlearningLoss: 0.005533\twhole_loss: 0.031907 \n",
            "Train Epoch: 136 [180/486 (37%)]\tlearningLoss: 0.005537\twhole_loss: 0.000738 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 2, 2, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.2414, Accuracy: 65/75(87%)\n",
            "\n",
            "Train Epoch: 137 [0/486 (0%)]\tlearningLoss: 0.000050\twhole_loss: 0.008671 \n",
            "Train Epoch: 137 [16/486 (3%)]\tlearningLoss: 0.000064\twhole_loss: 0.002473 \n",
            "Train Epoch: 137 [32/486 (7%)]\tlearningLoss: 0.000075\twhole_loss: 0.001926 \n",
            "Train Epoch: 137 [48/486 (10%)]\tlearningLoss: 0.000203\twhole_loss: 0.022368 \n",
            "Train Epoch: 137 [64/486 (13%)]\tlearningLoss: 0.000234\twhole_loss: 0.005527 \n",
            "Train Epoch: 137 [80/486 (16%)]\tlearningLoss: 0.002100\twhole_loss: 0.326494 \n",
            "Train Epoch: 137 [96/486 (20%)]\tlearningLoss: 0.002398\twhole_loss: 0.052211 \n",
            "Train Epoch: 137 [112/486 (23%)]\tlearningLoss: 0.002425\twhole_loss: 0.004731 \n",
            "Train Epoch: 137 [128/486 (26%)]\tlearningLoss: 0.002541\twhole_loss: 0.020291 \n",
            "Train Epoch: 137 [144/486 (30%)]\tlearningLoss: 0.002804\twhole_loss: 0.046000 \n",
            "Train Epoch: 137 [160/486 (33%)]\tlearningLoss: 0.002819\twhole_loss: 0.002695 \n",
            "Train Epoch: 137 [176/486 (36%)]\tlearningLoss: 0.003015\twhole_loss: 0.034242 \n",
            "Train Epoch: 137 [192/486 (40%)]\tlearningLoss: 0.003261\twhole_loss: 0.043045 \n",
            "Train Epoch: 137 [208/486 (43%)]\tlearningLoss: 0.003284\twhole_loss: 0.004063 \n",
            "Train Epoch: 137 [224/486 (46%)]\tlearningLoss: 0.003316\twhole_loss: 0.005612 \n",
            "Train Epoch: 137 [240/486 (49%)]\tlearningLoss: 0.003378\twhole_loss: 0.010738 \n",
            "Train Epoch: 137 [256/486 (53%)]\tlearningLoss: 0.003406\twhole_loss: 0.004948 \n",
            "Train Epoch: 137 [272/486 (56%)]\tlearningLoss: 0.003414\twhole_loss: 0.001479 \n",
            "Train Epoch: 137 [288/486 (59%)]\tlearningLoss: 0.003431\twhole_loss: 0.002994 \n",
            "Train Epoch: 137 [304/486 (63%)]\tlearningLoss: 0.003482\twhole_loss: 0.008758 \n",
            "Train Epoch: 137 [320/486 (66%)]\tlearningLoss: 0.003517\twhole_loss: 0.006165 \n",
            "Train Epoch: 137 [336/486 (69%)]\tlearningLoss: 0.003722\twhole_loss: 0.035891 \n",
            "Train Epoch: 137 [352/486 (72%)]\tlearningLoss: 0.003765\twhole_loss: 0.007540 \n",
            "Train Epoch: 137 [368/486 (76%)]\tlearningLoss: 0.003781\twhole_loss: 0.002727 \n",
            "Train Epoch: 137 [384/486 (79%)]\tlearningLoss: 0.003810\twhole_loss: 0.005247 \n",
            "Train Epoch: 137 [400/486 (82%)]\tlearningLoss: 0.004266\twhole_loss: 0.079667 \n",
            "Train Epoch: 137 [416/486 (86%)]\tlearningLoss: 0.004272\twhole_loss: 0.001124 \n",
            "Train Epoch: 137 [432/486 (89%)]\tlearningLoss: 0.004361\twhole_loss: 0.015529 \n",
            "Train Epoch: 137 [448/486 (92%)]\tlearningLoss: 0.004375\twhole_loss: 0.002505 \n",
            "Train Epoch: 137 [464/486 (95%)]\tlearningLoss: 0.004907\twhole_loss: 0.093038 \n",
            "Train Epoch: 137 [180/486 (37%)]\tlearningLoss: 0.004913\twhole_loss: 0.001022 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.9824, Accuracy: 69/75(92%)\n",
            "\n",
            "Train Epoch: 138 [0/486 (0%)]\tlearningLoss: 0.000005\twhole_loss: 0.000884 \n",
            "Train Epoch: 138 [16/486 (3%)]\tlearningLoss: 0.000131\twhole_loss: 0.022125 \n",
            "Train Epoch: 138 [32/486 (7%)]\tlearningLoss: 0.000319\twhole_loss: 0.032876 \n",
            "Train Epoch: 138 [48/486 (10%)]\tlearningLoss: 0.000416\twhole_loss: 0.016990 \n",
            "Train Epoch: 138 [64/486 (13%)]\tlearningLoss: 0.000435\twhole_loss: 0.003284 \n",
            "Train Epoch: 138 [80/486 (16%)]\tlearningLoss: 0.001005\twhole_loss: 0.099665 \n",
            "Train Epoch: 138 [96/486 (20%)]\tlearningLoss: 0.001048\twhole_loss: 0.007562 \n",
            "Train Epoch: 138 [112/486 (23%)]\tlearningLoss: 0.001080\twhole_loss: 0.005669 \n",
            "Train Epoch: 138 [128/486 (26%)]\tlearningLoss: 0.001081\twhole_loss: 0.000173 \n",
            "Train Epoch: 138 [144/486 (30%)]\tlearningLoss: 0.001132\twhole_loss: 0.008930 \n",
            "Train Epoch: 138 [160/486 (33%)]\tlearningLoss: 0.001195\twhole_loss: 0.011027 \n",
            "Train Epoch: 138 [176/486 (36%)]\tlearningLoss: 0.001335\twhole_loss: 0.024448 \n",
            "Train Epoch: 138 [192/486 (40%)]\tlearningLoss: 0.001337\twhole_loss: 0.000429 \n",
            "Train Epoch: 138 [208/486 (43%)]\tlearningLoss: 0.001354\twhole_loss: 0.002954 \n",
            "Train Epoch: 138 [224/486 (46%)]\tlearningLoss: 0.001387\twhole_loss: 0.005758 \n",
            "Train Epoch: 138 [240/486 (49%)]\tlearningLoss: 0.001388\twhole_loss: 0.000135 \n",
            "Train Epoch: 138 [256/486 (53%)]\tlearningLoss: 0.001446\twhole_loss: 0.010056 \n",
            "Train Epoch: 138 [272/486 (56%)]\tlearningLoss: 0.001462\twhole_loss: 0.002956 \n",
            "Train Epoch: 138 [288/486 (59%)]\tlearningLoss: 0.001938\twhole_loss: 0.083216 \n",
            "Train Epoch: 138 [304/486 (63%)]\tlearningLoss: 0.001949\twhole_loss: 0.001871 \n",
            "Train Epoch: 138 [320/486 (66%)]\tlearningLoss: 0.001952\twhole_loss: 0.000639 \n",
            "Train Epoch: 138 [336/486 (69%)]\tlearningLoss: 0.001964\twhole_loss: 0.002098 \n",
            "Train Epoch: 138 [352/486 (72%)]\tlearningLoss: 0.001997\twhole_loss: 0.005646 \n",
            "Train Epoch: 138 [368/486 (76%)]\tlearningLoss: 0.002028\twhole_loss: 0.005452 \n",
            "Train Epoch: 138 [384/486 (79%)]\tlearningLoss: 0.002038\twhole_loss: 0.001875 \n",
            "Train Epoch: 138 [400/486 (82%)]\tlearningLoss: 0.002683\twhole_loss: 0.112892 \n",
            "Train Epoch: 138 [416/486 (86%)]\tlearningLoss: 0.002724\twhole_loss: 0.007138 \n",
            "Train Epoch: 138 [432/486 (89%)]\tlearningLoss: 0.002799\twhole_loss: 0.013064 \n",
            "Train Epoch: 138 [448/486 (92%)]\tlearningLoss: 0.002871\twhole_loss: 0.012674 \n",
            "Train Epoch: 138 [464/486 (95%)]\tlearningLoss: 0.002881\twhole_loss: 0.001603 \n",
            "Train Epoch: 138 [180/486 (37%)]\tlearningLoss: 0.002881\twhole_loss: 0.000002 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.3527, Accuracy: 68/75(91%)\n",
            "\n",
            "Train Epoch: 139 [0/486 (0%)]\tlearningLoss: 0.000006\twhole_loss: 0.001011 \n",
            "Train Epoch: 139 [16/486 (3%)]\tlearningLoss: 0.000035\twhole_loss: 0.005085 \n",
            "Train Epoch: 139 [32/486 (7%)]\tlearningLoss: 0.000046\twhole_loss: 0.001934 \n",
            "Train Epoch: 139 [48/486 (10%)]\tlearningLoss: 0.000143\twhole_loss: 0.016964 \n",
            "Train Epoch: 139 [64/486 (13%)]\tlearningLoss: 0.000381\twhole_loss: 0.041727 \n",
            "Train Epoch: 139 [80/486 (16%)]\tlearningLoss: 0.000391\twhole_loss: 0.001705 \n",
            "Train Epoch: 139 [96/486 (20%)]\tlearningLoss: 0.000572\twhole_loss: 0.031644 \n",
            "Train Epoch: 139 [112/486 (23%)]\tlearningLoss: 0.000781\twhole_loss: 0.036540 \n",
            "Train Epoch: 139 [128/486 (26%)]\tlearningLoss: 0.000782\twhole_loss: 0.000255 \n",
            "Train Epoch: 139 [144/486 (30%)]\tlearningLoss: 0.000894\twhole_loss: 0.019501 \n",
            "Train Epoch: 139 [160/486 (33%)]\tlearningLoss: 0.000901\twhole_loss: 0.001270 \n",
            "Train Epoch: 139 [176/486 (36%)]\tlearningLoss: 0.001335\twhole_loss: 0.075916 \n",
            "Train Epoch: 139 [192/486 (40%)]\tlearningLoss: 0.001346\twhole_loss: 0.002056 \n",
            "Train Epoch: 139 [208/486 (43%)]\tlearningLoss: 0.001449\twhole_loss: 0.018010 \n",
            "Train Epoch: 139 [224/486 (46%)]\tlearningLoss: 0.001723\twhole_loss: 0.047924 \n",
            "Train Epoch: 139 [240/486 (49%)]\tlearningLoss: 0.001736\twhole_loss: 0.002189 \n",
            "Train Epoch: 139 [256/486 (53%)]\tlearningLoss: 0.001752\twhole_loss: 0.002853 \n",
            "Train Epoch: 139 [272/486 (56%)]\tlearningLoss: 0.001782\twhole_loss: 0.005238 \n",
            "Train Epoch: 139 [288/486 (59%)]\tlearningLoss: 0.002089\twhole_loss: 0.053701 \n",
            "Train Epoch: 139 [304/486 (63%)]\tlearningLoss: 0.002128\twhole_loss: 0.006894 \n",
            "Train Epoch: 139 [320/486 (66%)]\tlearningLoss: 0.002133\twhole_loss: 0.000798 \n",
            "Train Epoch: 139 [336/486 (69%)]\tlearningLoss: 0.002161\twhole_loss: 0.004917 \n",
            "Train Epoch: 139 [352/486 (72%)]\tlearningLoss: 0.002776\twhole_loss: 0.107670 \n",
            "Train Epoch: 139 [368/486 (76%)]\tlearningLoss: 0.002785\twhole_loss: 0.001619 \n",
            "Train Epoch: 139 [384/486 (79%)]\tlearningLoss: 0.002794\twhole_loss: 0.001535 \n",
            "Train Epoch: 139 [400/486 (82%)]\tlearningLoss: 0.002796\twhole_loss: 0.000420 \n",
            "Train Epoch: 139 [416/486 (86%)]\tlearningLoss: 0.002848\twhole_loss: 0.009066 \n",
            "Train Epoch: 139 [432/486 (89%)]\tlearningLoss: 0.002933\twhole_loss: 0.014912 \n",
            "Train Epoch: 139 [448/486 (92%)]\tlearningLoss: 0.002954\twhole_loss: 0.003657 \n",
            "Train Epoch: 139 [464/486 (95%)]\tlearningLoss: 0.004920\twhole_loss: 0.343956 \n",
            "Train Epoch: 139 [180/486 (37%)]\tlearningLoss: 0.004926\twhole_loss: 0.001005 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 2, 1, 1, 1, 1, 2, 0, 2, 2, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:6.4201, Accuracy: 61/75(81%)\n",
            "\n",
            "Train Epoch: 140 [0/486 (0%)]\tlearningLoss: 0.000719\twhole_loss: 0.125881 \n",
            "Train Epoch: 140 [16/486 (3%)]\tlearningLoss: 0.000963\twhole_loss: 0.042617 \n",
            "Train Epoch: 140 [32/486 (7%)]\tlearningLoss: 0.000972\twhole_loss: 0.001685 \n",
            "Train Epoch: 140 [48/486 (10%)]\tlearningLoss: 0.000979\twhole_loss: 0.001139 \n",
            "Train Epoch: 140 [64/486 (13%)]\tlearningLoss: 0.001586\twhole_loss: 0.106276 \n",
            "Train Epoch: 140 [80/486 (16%)]\tlearningLoss: 0.001588\twhole_loss: 0.000230 \n",
            "Train Epoch: 140 [96/486 (20%)]\tlearningLoss: 0.002119\twhole_loss: 0.093073 \n",
            "Train Epoch: 140 [112/486 (23%)]\tlearningLoss: 0.002127\twhole_loss: 0.001402 \n",
            "Train Epoch: 140 [128/486 (26%)]\tlearningLoss: 0.002186\twhole_loss: 0.010273 \n",
            "Train Epoch: 140 [144/486 (30%)]\tlearningLoss: 0.002211\twhole_loss: 0.004373 \n",
            "Train Epoch: 140 [160/486 (33%)]\tlearningLoss: 0.002580\twhole_loss: 0.064631 \n",
            "Train Epoch: 140 [176/486 (36%)]\tlearningLoss: 0.002617\twhole_loss: 0.006478 \n",
            "Train Epoch: 140 [192/486 (40%)]\tlearningLoss: 0.002689\twhole_loss: 0.012601 \n",
            "Train Epoch: 140 [208/486 (43%)]\tlearningLoss: 0.002733\twhole_loss: 0.007626 \n",
            "Train Epoch: 140 [224/486 (46%)]\tlearningLoss: 0.003142\twhole_loss: 0.071614 \n",
            "Train Epoch: 140 [240/486 (49%)]\tlearningLoss: 0.003182\twhole_loss: 0.006910 \n",
            "Train Epoch: 140 [256/486 (53%)]\tlearningLoss: 0.003210\twhole_loss: 0.005023 \n",
            "Train Epoch: 140 [272/486 (56%)]\tlearningLoss: 0.004034\twhole_loss: 0.144202 \n",
            "Train Epoch: 140 [288/486 (59%)]\tlearningLoss: 0.004037\twhole_loss: 0.000412 \n",
            "Train Epoch: 140 [304/486 (63%)]\tlearningLoss: 0.004288\twhole_loss: 0.043996 \n",
            "Train Epoch: 140 [320/486 (66%)]\tlearningLoss: 0.004778\twhole_loss: 0.085641 \n",
            "Train Epoch: 140 [336/486 (69%)]\tlearningLoss: 0.004951\twhole_loss: 0.030295 \n",
            "Train Epoch: 140 [352/486 (72%)]\tlearningLoss: 0.005077\twhole_loss: 0.022075 \n",
            "Train Epoch: 140 [368/486 (76%)]\tlearningLoss: 0.005179\twhole_loss: 0.017814 \n",
            "Train Epoch: 140 [384/486 (79%)]\tlearningLoss: 0.005186\twhole_loss: 0.001281 \n",
            "Train Epoch: 140 [400/486 (82%)]\tlearningLoss: 0.005198\twhole_loss: 0.002088 \n",
            "Train Epoch: 140 [416/486 (86%)]\tlearningLoss: 0.005351\twhole_loss: 0.026809 \n",
            "Train Epoch: 140 [432/486 (89%)]\tlearningLoss: 0.005441\twhole_loss: 0.015729 \n",
            "Train Epoch: 140 [448/486 (92%)]\tlearningLoss: 0.005448\twhole_loss: 0.001292 \n",
            "Train Epoch: 140 [464/486 (95%)]\tlearningLoss: 0.005624\twhole_loss: 0.030810 \n",
            "Train Epoch: 140 [180/486 (37%)]\tlearningLoss: 0.005641\twhole_loss: 0.002977 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.8870, Accuracy: 70/75(93%)\n",
            "\n",
            "Train Epoch: 141 [0/486 (0%)]\tlearningLoss: 0.000035\twhole_loss: 0.006117 \n",
            "Train Epoch: 141 [16/486 (3%)]\tlearningLoss: 0.000191\twhole_loss: 0.027305 \n",
            "Train Epoch: 141 [32/486 (7%)]\tlearningLoss: 0.000196\twhole_loss: 0.000793 \n",
            "Train Epoch: 141 [48/486 (10%)]\tlearningLoss: 0.000231\twhole_loss: 0.006136 \n",
            "Train Epoch: 141 [64/486 (13%)]\tlearningLoss: 0.000269\twhole_loss: 0.006638 \n",
            "Train Epoch: 141 [80/486 (16%)]\tlearningLoss: 0.001752\twhole_loss: 0.259698 \n",
            "Train Epoch: 141 [96/486 (20%)]\tlearningLoss: 0.001901\twhole_loss: 0.025968 \n",
            "Train Epoch: 141 [112/486 (23%)]\tlearningLoss: 0.001926\twhole_loss: 0.004434 \n",
            "Train Epoch: 141 [128/486 (26%)]\tlearningLoss: 0.004028\twhole_loss: 0.367852 \n",
            "Train Epoch: 141 [144/486 (30%)]\tlearningLoss: 0.004070\twhole_loss: 0.007231 \n",
            "Train Epoch: 141 [160/486 (33%)]\tlearningLoss: 0.004140\twhole_loss: 0.012306 \n",
            "Train Epoch: 141 [176/486 (36%)]\tlearningLoss: 0.004499\twhole_loss: 0.062905 \n",
            "Train Epoch: 141 [192/486 (40%)]\tlearningLoss: 0.004510\twhole_loss: 0.001888 \n",
            "Train Epoch: 141 [208/486 (43%)]\tlearningLoss: 0.005735\twhole_loss: 0.214350 \n",
            "Train Epoch: 141 [224/486 (46%)]\tlearningLoss: 0.005802\twhole_loss: 0.011716 \n",
            "Train Epoch: 141 [240/486 (49%)]\tlearningLoss: 0.005813\twhole_loss: 0.001852 \n",
            "Train Epoch: 141 [256/486 (53%)]\tlearningLoss: 0.006325\twhole_loss: 0.089695 \n",
            "Train Epoch: 141 [272/486 (56%)]\tlearningLoss: 0.006471\twhole_loss: 0.025628 \n",
            "Train Epoch: 141 [288/486 (59%)]\tlearningLoss: 0.007572\twhole_loss: 0.192601 \n",
            "Train Epoch: 141 [304/486 (63%)]\tlearningLoss: 0.007617\twhole_loss: 0.007902 \n",
            "Train Epoch: 141 [320/486 (66%)]\tlearningLoss: 0.007966\twhole_loss: 0.060952 \n",
            "Train Epoch: 141 [336/486 (69%)]\tlearningLoss: 0.008584\twhole_loss: 0.108264 \n",
            "Train Epoch: 141 [352/486 (72%)]\tlearningLoss: 0.008595\twhole_loss: 0.001832 \n",
            "Train Epoch: 141 [368/486 (76%)]\tlearningLoss: 0.008940\twhole_loss: 0.060503 \n",
            "Train Epoch: 141 [384/486 (79%)]\tlearningLoss: 0.013040\twhole_loss: 0.717483 \n",
            "Train Epoch: 141 [400/486 (82%)]\tlearningLoss: 0.013216\twhole_loss: 0.030768 \n",
            "Train Epoch: 141 [416/486 (86%)]\tlearningLoss: 0.013259\twhole_loss: 0.007559 \n",
            "Train Epoch: 141 [432/486 (89%)]\tlearningLoss: 0.014324\twhole_loss: 0.186287 \n",
            "Train Epoch: 141 [448/486 (92%)]\tlearningLoss: 0.015444\twhole_loss: 0.195966 \n",
            "Train Epoch: 141 [464/486 (95%)]\tlearningLoss: 0.015660\twhole_loss: 0.037876 \n",
            "Train Epoch: 141 [180/486 (37%)]\tlearningLoss: 0.015694\twhole_loss: 0.005910 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.3985, Accuracy: 68/75(91%)\n",
            "\n",
            "Train Epoch: 142 [0/486 (0%)]\tlearningLoss: 0.001007\twhole_loss: 0.176251 \n",
            "Train Epoch: 142 [16/486 (3%)]\tlearningLoss: 0.002154\twhole_loss: 0.200616 \n",
            "Train Epoch: 142 [32/486 (7%)]\tlearningLoss: 0.003452\twhole_loss: 0.227227 \n",
            "Train Epoch: 142 [48/486 (10%)]\tlearningLoss: 0.003976\twhole_loss: 0.091774 \n",
            "Train Epoch: 142 [64/486 (13%)]\tlearningLoss: 0.004038\twhole_loss: 0.010827 \n",
            "Train Epoch: 142 [80/486 (16%)]\tlearningLoss: 0.006375\twhole_loss: 0.408897 \n",
            "Train Epoch: 142 [96/486 (20%)]\tlearningLoss: 0.006382\twhole_loss: 0.001256 \n",
            "Train Epoch: 142 [112/486 (23%)]\tlearningLoss: 0.007571\twhole_loss: 0.208164 \n",
            "Train Epoch: 142 [128/486 (26%)]\tlearningLoss: 0.007649\twhole_loss: 0.013494 \n",
            "Train Epoch: 142 [144/486 (30%)]\tlearningLoss: 0.007731\twhole_loss: 0.014498 \n",
            "Train Epoch: 142 [160/486 (33%)]\tlearningLoss: 0.011251\twhole_loss: 0.615889 \n",
            "Train Epoch: 142 [176/486 (36%)]\tlearningLoss: 0.011321\twhole_loss: 0.012221 \n",
            "Train Epoch: 142 [192/486 (40%)]\tlearningLoss: 0.012020\twhole_loss: 0.122355 \n",
            "Train Epoch: 142 [208/486 (43%)]\tlearningLoss: 0.012791\twhole_loss: 0.134887 \n",
            "Train Epoch: 142 [224/486 (46%)]\tlearningLoss: 0.012975\twhole_loss: 0.032341 \n",
            "Train Epoch: 142 [240/486 (49%)]\tlearningLoss: 0.014475\twhole_loss: 0.262419 \n",
            "Train Epoch: 142 [256/486 (53%)]\tlearningLoss: 0.014536\twhole_loss: 0.010657 \n",
            "Train Epoch: 142 [272/486 (56%)]\tlearningLoss: 0.015223\twhole_loss: 0.120292 \n",
            "Train Epoch: 142 [288/486 (59%)]\tlearningLoss: 0.016538\twhole_loss: 0.230105 \n",
            "Train Epoch: 142 [304/486 (63%)]\tlearningLoss: 0.017091\twhole_loss: 0.096840 \n",
            "Train Epoch: 142 [320/486 (66%)]\tlearningLoss: 0.018265\twhole_loss: 0.205334 \n",
            "Train Epoch: 142 [336/486 (69%)]\tlearningLoss: 0.018485\twhole_loss: 0.038616 \n",
            "Train Epoch: 142 [352/486 (72%)]\tlearningLoss: 0.020348\twhole_loss: 0.326010 \n",
            "Train Epoch: 142 [368/486 (76%)]\tlearningLoss: 0.020833\twhole_loss: 0.084860 \n",
            "Train Epoch: 142 [384/486 (79%)]\tlearningLoss: 0.022303\twhole_loss: 0.257129 \n",
            "Train Epoch: 142 [400/486 (82%)]\tlearningLoss: 0.022818\twhole_loss: 0.090169 \n",
            "Train Epoch: 142 [416/486 (86%)]\tlearningLoss: 0.023596\twhole_loss: 0.136132 \n",
            "Train Epoch: 142 [432/486 (89%)]\tlearningLoss: 0.023647\twhole_loss: 0.008883 \n",
            "Train Epoch: 142 [448/486 (92%)]\tlearningLoss: 0.025229\twhole_loss: 0.276911 \n",
            "Train Epoch: 142 [464/486 (95%)]\tlearningLoss: 0.026546\twhole_loss: 0.230457 \n",
            "Train Epoch: 142 [180/486 (37%)]\tlearningLoss: 0.026904\twhole_loss: 0.062745 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 2, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 1, 0, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.5185, Accuracy: 65/75(87%)\n",
            "\n",
            "Train Epoch: 143 [0/486 (0%)]\tlearningLoss: 0.000249\twhole_loss: 0.043612 \n",
            "Train Epoch: 143 [16/486 (3%)]\tlearningLoss: 0.000552\twhole_loss: 0.053052 \n",
            "Train Epoch: 143 [32/486 (7%)]\tlearningLoss: 0.000828\twhole_loss: 0.048307 \n",
            "Train Epoch: 143 [48/486 (10%)]\tlearningLoss: 0.001289\twhole_loss: 0.080670 \n",
            "Train Epoch: 143 [64/486 (13%)]\tlearningLoss: 0.001351\twhole_loss: 0.010739 \n",
            "Train Epoch: 143 [80/486 (16%)]\tlearningLoss: 0.001425\twhole_loss: 0.012993 \n",
            "Train Epoch: 143 [96/486 (20%)]\tlearningLoss: 0.001810\twhole_loss: 0.067410 \n",
            "Train Epoch: 143 [112/486 (23%)]\tlearningLoss: 0.001884\twhole_loss: 0.012917 \n",
            "Train Epoch: 143 [128/486 (26%)]\tlearningLoss: 0.001970\twhole_loss: 0.015056 \n",
            "Train Epoch: 143 [144/486 (30%)]\tlearningLoss: 0.003039\twhole_loss: 0.187061 \n",
            "Train Epoch: 143 [160/486 (33%)]\tlearningLoss: 0.003414\twhole_loss: 0.065612 \n",
            "Train Epoch: 143 [176/486 (36%)]\tlearningLoss: 0.003468\twhole_loss: 0.009432 \n",
            "Train Epoch: 143 [192/486 (40%)]\tlearningLoss: 0.003644\twhole_loss: 0.030842 \n",
            "Train Epoch: 143 [208/486 (43%)]\tlearningLoss: 0.003670\twhole_loss: 0.004632 \n",
            "Train Epoch: 143 [224/486 (46%)]\tlearningLoss: 0.004039\twhole_loss: 0.064433 \n",
            "Train Epoch: 143 [240/486 (49%)]\tlearningLoss: 0.004426\twhole_loss: 0.067774 \n",
            "Train Epoch: 143 [256/486 (53%)]\tlearningLoss: 0.004458\twhole_loss: 0.005689 \n",
            "Train Epoch: 143 [272/486 (56%)]\tlearningLoss: 0.005021\twhole_loss: 0.098391 \n",
            "Train Epoch: 143 [288/486 (59%)]\tlearningLoss: 0.005391\twhole_loss: 0.064873 \n",
            "Train Epoch: 143 [304/486 (63%)]\tlearningLoss: 0.005401\twhole_loss: 0.001733 \n",
            "Train Epoch: 143 [320/486 (66%)]\tlearningLoss: 0.005659\twhole_loss: 0.045146 \n",
            "Train Epoch: 143 [336/486 (69%)]\tlearningLoss: 0.005835\twhole_loss: 0.030755 \n",
            "Train Epoch: 143 [352/486 (72%)]\tlearningLoss: 0.005848\twhole_loss: 0.002210 \n",
            "Train Epoch: 143 [368/486 (76%)]\tlearningLoss: 0.006815\twhole_loss: 0.169274 \n",
            "Train Epoch: 143 [384/486 (79%)]\tlearningLoss: 0.006901\twhole_loss: 0.014996 \n",
            "Train Epoch: 143 [400/486 (82%)]\tlearningLoss: 0.006982\twhole_loss: 0.014205 \n",
            "Train Epoch: 143 [416/486 (86%)]\tlearningLoss: 0.007027\twhole_loss: 0.007854 \n",
            "Train Epoch: 143 [432/486 (89%)]\tlearningLoss: 0.007073\twhole_loss: 0.008122 \n",
            "Train Epoch: 143 [448/486 (92%)]\tlearningLoss: 0.007400\twhole_loss: 0.057200 \n",
            "Train Epoch: 143 [464/486 (95%)]\tlearningLoss: 0.007561\twhole_loss: 0.028203 \n",
            "Train Epoch: 143 [180/486 (37%)]\tlearningLoss: 0.007564\twhole_loss: 0.000572 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 2, 2, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.2626, Accuracy: 64/75(85%)\n",
            "\n",
            "Train Epoch: 144 [0/486 (0%)]\tlearningLoss: 0.000281\twhole_loss: 0.049260 \n",
            "Train Epoch: 144 [16/486 (3%)]\tlearningLoss: 0.000743\twhole_loss: 0.080818 \n",
            "Train Epoch: 144 [32/486 (7%)]\tlearningLoss: 0.000787\twhole_loss: 0.007586 \n",
            "Train Epoch: 144 [48/486 (10%)]\tlearningLoss: 0.000928\twhole_loss: 0.024821 \n",
            "Train Epoch: 144 [64/486 (13%)]\tlearningLoss: 0.001464\twhole_loss: 0.093768 \n",
            "Train Epoch: 144 [80/486 (16%)]\tlearningLoss: 0.001651\twhole_loss: 0.032756 \n",
            "Train Epoch: 144 [96/486 (20%)]\tlearningLoss: 0.001699\twhole_loss: 0.008384 \n",
            "Train Epoch: 144 [112/486 (23%)]\tlearningLoss: 0.002016\twhole_loss: 0.055443 \n",
            "Train Epoch: 144 [128/486 (26%)]\tlearningLoss: 0.002050\twhole_loss: 0.005988 \n",
            "Train Epoch: 144 [144/486 (30%)]\tlearningLoss: 0.002189\twhole_loss: 0.024321 \n",
            "Train Epoch: 144 [160/486 (33%)]\tlearningLoss: 0.002311\twhole_loss: 0.021338 \n",
            "Train Epoch: 144 [176/486 (36%)]\tlearningLoss: 0.003329\twhole_loss: 0.178116 \n",
            "Train Epoch: 144 [192/486 (40%)]\tlearningLoss: 0.003376\twhole_loss: 0.008181 \n",
            "Train Epoch: 144 [208/486 (43%)]\tlearningLoss: 0.003689\twhole_loss: 0.054844 \n",
            "Train Epoch: 144 [224/486 (46%)]\tlearningLoss: 0.003704\twhole_loss: 0.002642 \n",
            "Train Epoch: 144 [240/486 (49%)]\tlearningLoss: 0.003713\twhole_loss: 0.001550 \n",
            "Train Epoch: 144 [256/486 (53%)]\tlearningLoss: 0.003725\twhole_loss: 0.002122 \n",
            "Train Epoch: 144 [272/486 (56%)]\tlearningLoss: 0.003737\twhole_loss: 0.002052 \n",
            "Train Epoch: 144 [288/486 (59%)]\tlearningLoss: 0.003808\twhole_loss: 0.012387 \n",
            "Train Epoch: 144 [304/486 (63%)]\tlearningLoss: 0.003844\twhole_loss: 0.006364 \n",
            "Train Epoch: 144 [320/486 (66%)]\tlearningLoss: 0.004101\twhole_loss: 0.044914 \n",
            "Train Epoch: 144 [336/486 (69%)]\tlearningLoss: 0.004208\twhole_loss: 0.018691 \n",
            "Train Epoch: 144 [352/486 (72%)]\tlearningLoss: 0.004212\twhole_loss: 0.000759 \n",
            "Train Epoch: 144 [368/486 (76%)]\tlearningLoss: 0.004237\twhole_loss: 0.004340 \n",
            "Train Epoch: 144 [384/486 (79%)]\tlearningLoss: 0.004253\twhole_loss: 0.002754 \n",
            "Train Epoch: 144 [400/486 (82%)]\tlearningLoss: 0.004495\twhole_loss: 0.042464 \n",
            "Train Epoch: 144 [416/486 (86%)]\tlearningLoss: 0.004496\twhole_loss: 0.000208 \n",
            "Train Epoch: 144 [432/486 (89%)]\tlearningLoss: 0.004689\twhole_loss: 0.033636 \n",
            "Train Epoch: 144 [448/486 (92%)]\tlearningLoss: 0.004908\twhole_loss: 0.038462 \n",
            "Train Epoch: 144 [464/486 (95%)]\tlearningLoss: 0.005003\twhole_loss: 0.016608 \n",
            "Train Epoch: 144 [180/486 (37%)]\tlearningLoss: 0.005018\twhole_loss: 0.002634 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 2, 1, 1, 1, 1, 1, 2, 0, 2, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.4950, Accuracy: 64/75(85%)\n",
            "\n",
            "Train Epoch: 145 [0/486 (0%)]\tlearningLoss: 0.000005\twhole_loss: 0.000920 \n",
            "Train Epoch: 145 [16/486 (3%)]\tlearningLoss: 0.000010\twhole_loss: 0.000902 \n",
            "Train Epoch: 145 [32/486 (7%)]\tlearningLoss: 0.000027\twhole_loss: 0.002819 \n",
            "Train Epoch: 145 [48/486 (10%)]\tlearningLoss: 0.000058\twhole_loss: 0.005579 \n",
            "Train Epoch: 145 [64/486 (13%)]\tlearningLoss: 0.000076\twhole_loss: 0.003059 \n",
            "Train Epoch: 145 [80/486 (16%)]\tlearningLoss: 0.000087\twhole_loss: 0.002019 \n",
            "Train Epoch: 145 [96/486 (20%)]\tlearningLoss: 0.000094\twhole_loss: 0.001140 \n",
            "Train Epoch: 145 [112/486 (23%)]\tlearningLoss: 0.000107\twhole_loss: 0.002367 \n",
            "Train Epoch: 145 [128/486 (26%)]\tlearningLoss: 0.000115\twhole_loss: 0.001314 \n",
            "Train Epoch: 145 [144/486 (30%)]\tlearningLoss: 0.000168\twhole_loss: 0.009358 \n",
            "Train Epoch: 145 [160/486 (33%)]\tlearningLoss: 0.000170\twhole_loss: 0.000206 \n",
            "Train Epoch: 145 [176/486 (36%)]\tlearningLoss: 0.000225\twhole_loss: 0.009746 \n",
            "Train Epoch: 145 [192/486 (40%)]\tlearningLoss: 0.000241\twhole_loss: 0.002797 \n",
            "Train Epoch: 145 [208/486 (43%)]\tlearningLoss: 0.003854\twhole_loss: 0.632135 \n",
            "Train Epoch: 145 [224/486 (46%)]\tlearningLoss: 0.006866\twhole_loss: 0.527220 \n",
            "Train Epoch: 145 [240/486 (49%)]\tlearningLoss: 0.006966\twhole_loss: 0.017506 \n",
            "Train Epoch: 145 [256/486 (53%)]\tlearningLoss: 0.007052\twhole_loss: 0.015048 \n",
            "Train Epoch: 145 [272/486 (56%)]\tlearningLoss: 0.009209\twhole_loss: 0.377408 \n",
            "Train Epoch: 145 [288/486 (59%)]\tlearningLoss: 0.010343\twhole_loss: 0.198547 \n",
            "Train Epoch: 145 [304/486 (63%)]\tlearningLoss: 0.010865\twhole_loss: 0.091266 \n",
            "Train Epoch: 145 [320/486 (66%)]\tlearningLoss: 0.011799\twhole_loss: 0.163417 \n",
            "Train Epoch: 145 [336/486 (69%)]\tlearningLoss: 0.011802\twhole_loss: 0.000618 \n",
            "Train Epoch: 145 [352/486 (72%)]\tlearningLoss: 0.011808\twhole_loss: 0.000966 \n",
            "Train Epoch: 145 [368/486 (76%)]\tlearningLoss: 0.011840\twhole_loss: 0.005570 \n",
            "Train Epoch: 145 [384/486 (79%)]\tlearningLoss: 0.011896\twhole_loss: 0.009866 \n",
            "Train Epoch: 145 [400/486 (82%)]\tlearningLoss: 0.011992\twhole_loss: 0.016847 \n",
            "Train Epoch: 145 [416/486 (86%)]\tlearningLoss: 0.012172\twhole_loss: 0.031524 \n",
            "Train Epoch: 145 [432/486 (89%)]\tlearningLoss: 0.012323\twhole_loss: 0.026397 \n",
            "Train Epoch: 145 [448/486 (92%)]\tlearningLoss: 0.013117\twhole_loss: 0.138981 \n",
            "Train Epoch: 145 [464/486 (95%)]\tlearningLoss: 0.013157\twhole_loss: 0.006916 \n",
            "Train Epoch: 145 [180/486 (37%)]\tlearningLoss: 0.013160\twhole_loss: 0.000457 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 2, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 1, 0, 0, 0, 2, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.9534, Accuracy: 65/75(87%)\n",
            "\n",
            "Train Epoch: 146 [0/486 (0%)]\tlearningLoss: 0.000003\twhole_loss: 0.000599 \n",
            "Train Epoch: 146 [16/486 (3%)]\tlearningLoss: 0.000732\twhole_loss: 0.127476 \n",
            "Train Epoch: 146 [32/486 (7%)]\tlearningLoss: 0.001019\twhole_loss: 0.050205 \n",
            "Train Epoch: 146 [48/486 (10%)]\tlearningLoss: 0.001452\twhole_loss: 0.075813 \n",
            "Train Epoch: 146 [64/486 (13%)]\tlearningLoss: 0.001791\twhole_loss: 0.059352 \n",
            "Train Epoch: 146 [80/486 (16%)]\tlearningLoss: 0.005559\twhole_loss: 0.659405 \n",
            "Train Epoch: 146 [96/486 (20%)]\tlearningLoss: 0.005599\twhole_loss: 0.006947 \n",
            "Train Epoch: 146 [112/486 (23%)]\tlearningLoss: 0.005622\twhole_loss: 0.004007 \n",
            "Train Epoch: 146 [128/486 (26%)]\tlearningLoss: 0.005927\twhole_loss: 0.053435 \n",
            "Train Epoch: 146 [144/486 (30%)]\tlearningLoss: 0.005955\twhole_loss: 0.004935 \n",
            "Train Epoch: 146 [160/486 (33%)]\tlearningLoss: 0.006056\twhole_loss: 0.017619 \n",
            "Train Epoch: 146 [176/486 (36%)]\tlearningLoss: 0.006091\twhole_loss: 0.006218 \n",
            "Train Epoch: 146 [192/486 (40%)]\tlearningLoss: 0.006142\twhole_loss: 0.008906 \n",
            "Train Epoch: 146 [208/486 (43%)]\tlearningLoss: 0.006548\twhole_loss: 0.071039 \n",
            "Train Epoch: 146 [224/486 (46%)]\tlearningLoss: 0.006609\twhole_loss: 0.010699 \n",
            "Train Epoch: 146 [240/486 (49%)]\tlearningLoss: 0.006978\twhole_loss: 0.064453 \n",
            "Train Epoch: 146 [256/486 (53%)]\tlearningLoss: 0.007048\twhole_loss: 0.012205 \n",
            "Train Epoch: 146 [272/486 (56%)]\tlearningLoss: 0.008190\twhole_loss: 0.200018 \n",
            "Train Epoch: 146 [288/486 (59%)]\tlearningLoss: 0.010453\twhole_loss: 0.395902 \n",
            "Train Epoch: 146 [304/486 (63%)]\tlearningLoss: 0.010466\twhole_loss: 0.002326 \n",
            "Train Epoch: 146 [320/486 (66%)]\tlearningLoss: 0.010672\twhole_loss: 0.036084 \n",
            "Train Epoch: 146 [336/486 (69%)]\tlearningLoss: 0.012030\twhole_loss: 0.237530 \n",
            "Train Epoch: 146 [352/486 (72%)]\tlearningLoss: 0.012125\twhole_loss: 0.016631 \n",
            "Train Epoch: 146 [368/486 (76%)]\tlearningLoss: 0.012264\twhole_loss: 0.024419 \n",
            "Train Epoch: 146 [384/486 (79%)]\tlearningLoss: 0.012269\twhole_loss: 0.000790 \n",
            "Train Epoch: 146 [400/486 (82%)]\tlearningLoss: 0.012444\twhole_loss: 0.030694 \n",
            "Train Epoch: 146 [416/486 (86%)]\tlearningLoss: 0.012601\twhole_loss: 0.027392 \n",
            "Train Epoch: 146 [432/486 (89%)]\tlearningLoss: 0.013509\twhole_loss: 0.158999 \n",
            "Train Epoch: 146 [448/486 (92%)]\tlearningLoss: 0.013611\twhole_loss: 0.017835 \n",
            "Train Epoch: 146 [464/486 (95%)]\tlearningLoss: 0.013782\twhole_loss: 0.029962 \n",
            "Train Epoch: 146 [180/486 (37%)]\tlearningLoss: 0.013783\twhole_loss: 0.000204 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 2, 1, 1, 2, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 1, 0, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.1398, Accuracy: 64/75(85%)\n",
            "\n",
            "Train Epoch: 147 [0/486 (0%)]\tlearningLoss: 0.000645\twhole_loss: 0.112876 \n",
            "Train Epoch: 147 [16/486 (3%)]\tlearningLoss: 0.001398\twhole_loss: 0.131757 \n",
            "Train Epoch: 147 [32/486 (7%)]\tlearningLoss: 0.001505\twhole_loss: 0.018829 \n",
            "Train Epoch: 147 [48/486 (10%)]\tlearningLoss: 0.001551\twhole_loss: 0.007980 \n",
            "Train Epoch: 147 [64/486 (13%)]\tlearningLoss: 0.001636\twhole_loss: 0.014863 \n",
            "Train Epoch: 147 [80/486 (16%)]\tlearningLoss: 0.001707\twhole_loss: 0.012421 \n",
            "Train Epoch: 147 [96/486 (20%)]\tlearningLoss: 0.001960\twhole_loss: 0.044234 \n",
            "Train Epoch: 147 [112/486 (23%)]\tlearningLoss: 0.002047\twhole_loss: 0.015225 \n",
            "Train Epoch: 147 [128/486 (26%)]\tlearningLoss: 0.002161\twhole_loss: 0.019951 \n",
            "Train Epoch: 147 [144/486 (30%)]\tlearningLoss: 0.002185\twhole_loss: 0.004263 \n",
            "Train Epoch: 147 [160/486 (33%)]\tlearningLoss: 0.002594\twhole_loss: 0.071479 \n",
            "Train Epoch: 147 [176/486 (36%)]\tlearningLoss: 0.003110\twhole_loss: 0.090459 \n",
            "Train Epoch: 147 [192/486 (40%)]\tlearningLoss: 0.003249\twhole_loss: 0.024257 \n",
            "Train Epoch: 147 [208/486 (43%)]\tlearningLoss: 0.004231\twhole_loss: 0.171880 \n",
            "Train Epoch: 147 [224/486 (46%)]\tlearningLoss: 0.004609\twhole_loss: 0.066125 \n",
            "Train Epoch: 147 [240/486 (49%)]\tlearningLoss: 0.004657\twhole_loss: 0.008348 \n",
            "Train Epoch: 147 [256/486 (53%)]\tlearningLoss: 0.004659\twhole_loss: 0.000359 \n",
            "Train Epoch: 147 [272/486 (56%)]\tlearningLoss: 0.004772\twhole_loss: 0.019859 \n",
            "Train Epoch: 147 [288/486 (59%)]\tlearningLoss: 0.005269\twhole_loss: 0.086857 \n",
            "Train Epoch: 147 [304/486 (63%)]\tlearningLoss: 0.005476\twhole_loss: 0.036254 \n",
            "Train Epoch: 147 [320/486 (66%)]\tlearningLoss: 0.005581\twhole_loss: 0.018461 \n",
            "Train Epoch: 147 [336/486 (69%)]\tlearningLoss: 0.006051\twhole_loss: 0.082117 \n",
            "Train Epoch: 147 [352/486 (72%)]\tlearningLoss: 0.006056\twhole_loss: 0.000924 \n",
            "Train Epoch: 147 [368/486 (76%)]\tlearningLoss: 0.006159\twhole_loss: 0.018108 \n",
            "Train Epoch: 147 [384/486 (79%)]\tlearningLoss: 0.006309\twhole_loss: 0.026148 \n",
            "Train Epoch: 147 [400/486 (82%)]\tlearningLoss: 0.006789\twhole_loss: 0.083988 \n",
            "Train Epoch: 147 [416/486 (86%)]\tlearningLoss: 0.006820\twhole_loss: 0.005496 \n",
            "Train Epoch: 147 [432/486 (89%)]\tlearningLoss: 0.006835\twhole_loss: 0.002527 \n",
            "Train Epoch: 147 [448/486 (92%)]\tlearningLoss: 0.006911\twhole_loss: 0.013453 \n",
            "Train Epoch: 147 [464/486 (95%)]\tlearningLoss: 0.006936\twhole_loss: 0.004306 \n",
            "Train Epoch: 147 [180/486 (37%)]\tlearningLoss: 0.006937\twhole_loss: 0.000098 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 1, 0, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.4433, Accuracy: 68/75(91%)\n",
            "\n",
            "Train Epoch: 148 [0/486 (0%)]\tlearningLoss: 0.000762\twhole_loss: 0.133353 \n",
            "Train Epoch: 148 [16/486 (3%)]\tlearningLoss: 0.000768\twhole_loss: 0.000989 \n",
            "Train Epoch: 148 [32/486 (7%)]\tlearningLoss: 0.000903\twhole_loss: 0.023596 \n",
            "Train Epoch: 148 [48/486 (10%)]\tlearningLoss: 0.001022\twhole_loss: 0.020835 \n",
            "Train Epoch: 148 [64/486 (13%)]\tlearningLoss: 0.001030\twhole_loss: 0.001528 \n",
            "Train Epoch: 148 [80/486 (16%)]\tlearningLoss: 0.001158\twhole_loss: 0.022345 \n",
            "Train Epoch: 148 [96/486 (20%)]\tlearningLoss: 0.001387\twhole_loss: 0.040057 \n",
            "Train Epoch: 148 [112/486 (23%)]\tlearningLoss: 0.002429\twhole_loss: 0.182421 \n",
            "Train Epoch: 148 [128/486 (26%)]\tlearningLoss: 0.002442\twhole_loss: 0.002141 \n",
            "Train Epoch: 148 [144/486 (30%)]\tlearningLoss: 0.002590\twhole_loss: 0.025937 \n",
            "Train Epoch: 148 [160/486 (33%)]\tlearningLoss: 0.002597\twhole_loss: 0.001240 \n",
            "Train Epoch: 148 [176/486 (36%)]\tlearningLoss: 0.002603\twhole_loss: 0.001003 \n",
            "Train Epoch: 148 [192/486 (40%)]\tlearningLoss: 0.002611\twhole_loss: 0.001565 \n",
            "Train Epoch: 148 [208/486 (43%)]\tlearningLoss: 0.002697\twhole_loss: 0.014907 \n",
            "Train Epoch: 148 [224/486 (46%)]\tlearningLoss: 0.002719\twhole_loss: 0.003982 \n",
            "Train Epoch: 148 [240/486 (49%)]\tlearningLoss: 0.002827\twhole_loss: 0.018846 \n",
            "Train Epoch: 148 [256/486 (53%)]\tlearningLoss: 0.002828\twhole_loss: 0.000220 \n",
            "Train Epoch: 148 [272/486 (56%)]\tlearningLoss: 0.002836\twhole_loss: 0.001285 \n",
            "Train Epoch: 148 [288/486 (59%)]\tlearningLoss: 0.003011\twhole_loss: 0.030677 \n",
            "Train Epoch: 148 [304/486 (63%)]\tlearningLoss: 0.003041\twhole_loss: 0.005160 \n",
            "Train Epoch: 148 [320/486 (66%)]\tlearningLoss: 0.003049\twhole_loss: 0.001530 \n",
            "Train Epoch: 148 [336/486 (69%)]\tlearningLoss: 0.003113\twhole_loss: 0.011081 \n",
            "Train Epoch: 148 [352/486 (72%)]\tlearningLoss: 0.004019\twhole_loss: 0.158563 \n",
            "Train Epoch: 148 [368/486 (76%)]\tlearningLoss: 0.004024\twhole_loss: 0.000861 \n",
            "Train Epoch: 148 [384/486 (79%)]\tlearningLoss: 0.004108\twhole_loss: 0.014793 \n",
            "Train Epoch: 148 [400/486 (82%)]\tlearningLoss: 0.004515\twhole_loss: 0.071200 \n",
            "Train Epoch: 148 [416/486 (86%)]\tlearningLoss: 0.005682\twhole_loss: 0.204261 \n",
            "Train Epoch: 148 [432/486 (89%)]\tlearningLoss: 0.007090\twhole_loss: 0.246347 \n",
            "Train Epoch: 148 [448/486 (92%)]\tlearningLoss: 0.007909\twhole_loss: 0.143272 \n",
            "Train Epoch: 148 [464/486 (95%)]\tlearningLoss: 0.008074\twhole_loss: 0.028886 \n",
            "Train Epoch: 148 [180/486 (37%)]\tlearningLoss: 0.008152\twhole_loss: 0.013704 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 2, 2, 2, 1, 0, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:5.5715, Accuracy: 61/75(81%)\n",
            "\n",
            "Train Epoch: 149 [0/486 (0%)]\tlearningLoss: 0.000042\twhole_loss: 0.007279 \n",
            "Train Epoch: 149 [16/486 (3%)]\tlearningLoss: 0.000384\twhole_loss: 0.059944 \n",
            "Train Epoch: 149 [32/486 (7%)]\tlearningLoss: 0.000728\twhole_loss: 0.060108 \n",
            "Train Epoch: 149 [48/486 (10%)]\tlearningLoss: 0.000743\twhole_loss: 0.002682 \n",
            "Train Epoch: 149 [64/486 (13%)]\tlearningLoss: 0.001671\twhole_loss: 0.162350 \n",
            "Train Epoch: 149 [80/486 (16%)]\tlearningLoss: 0.001864\twhole_loss: 0.033789 \n",
            "Train Epoch: 149 [96/486 (20%)]\tlearningLoss: 0.001909\twhole_loss: 0.007905 \n",
            "Train Epoch: 149 [112/486 (23%)]\tlearningLoss: 0.001948\twhole_loss: 0.006922 \n",
            "Train Epoch: 149 [128/486 (26%)]\tlearningLoss: 0.002273\twhole_loss: 0.056757 \n",
            "Train Epoch: 149 [144/486 (30%)]\tlearningLoss: 0.002312\twhole_loss: 0.006800 \n",
            "Train Epoch: 149 [160/486 (33%)]\tlearningLoss: 0.004712\twhole_loss: 0.420121 \n",
            "Train Epoch: 149 [176/486 (36%)]\tlearningLoss: 0.005172\twhole_loss: 0.080388 \n",
            "Train Epoch: 149 [192/486 (40%)]\tlearningLoss: 0.005205\twhole_loss: 0.005822 \n",
            "Train Epoch: 149 [208/486 (43%)]\tlearningLoss: 0.005358\twhole_loss: 0.026726 \n",
            "Train Epoch: 149 [224/486 (46%)]\tlearningLoss: 0.005949\twhole_loss: 0.103476 \n",
            "Train Epoch: 149 [240/486 (49%)]\tlearningLoss: 0.006179\twhole_loss: 0.040227 \n",
            "Train Epoch: 149 [256/486 (53%)]\tlearningLoss: 0.006182\twhole_loss: 0.000613 \n",
            "Train Epoch: 149 [272/486 (56%)]\tlearningLoss: 0.006246\twhole_loss: 0.011075 \n",
            "Train Epoch: 149 [288/486 (59%)]\tlearningLoss: 0.006299\twhole_loss: 0.009367 \n",
            "Train Epoch: 149 [304/486 (63%)]\tlearningLoss: 0.006301\twhole_loss: 0.000343 \n",
            "Train Epoch: 149 [320/486 (66%)]\tlearningLoss: 0.006562\twhole_loss: 0.045571 \n",
            "Train Epoch: 149 [336/486 (69%)]\tlearningLoss: 0.006581\twhole_loss: 0.003362 \n",
            "Train Epoch: 149 [352/486 (72%)]\tlearningLoss: 0.007266\twhole_loss: 0.119884 \n",
            "Train Epoch: 149 [368/486 (76%)]\tlearningLoss: 0.007308\twhole_loss: 0.007408 \n",
            "Train Epoch: 149 [384/486 (79%)]\tlearningLoss: 0.007394\twhole_loss: 0.014967 \n",
            "Train Epoch: 149 [400/486 (82%)]\tlearningLoss: 0.007597\twhole_loss: 0.035628 \n",
            "Train Epoch: 149 [416/486 (86%)]\tlearningLoss: 0.007738\twhole_loss: 0.024710 \n",
            "Train Epoch: 149 [432/486 (89%)]\tlearningLoss: 0.007761\twhole_loss: 0.003975 \n",
            "Train Epoch: 149 [448/486 (92%)]\tlearningLoss: 0.007767\twhole_loss: 0.001093 \n",
            "Train Epoch: 149 [464/486 (95%)]\tlearningLoss: 0.007776\twhole_loss: 0.001481 \n",
            "Train Epoch: 149 [180/486 (37%)]\tlearningLoss: 0.007920\twhole_loss: 0.025312 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 1, 1, 1, 1, 1, 1, 2, 0, 2, 2, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.2779, Accuracy: 66/75(88%)\n",
            "\n",
            "Train Epoch: 150 [0/486 (0%)]\tlearningLoss: 0.000198\twhole_loss: 0.034623 \n",
            "Train Epoch: 150 [16/486 (3%)]\tlearningLoss: 0.000288\twhole_loss: 0.015822 \n",
            "Train Epoch: 150 [32/486 (7%)]\tlearningLoss: 0.000524\twhole_loss: 0.041335 \n",
            "Train Epoch: 150 [48/486 (10%)]\tlearningLoss: 0.000607\twhole_loss: 0.014517 \n",
            "Train Epoch: 150 [64/486 (13%)]\tlearningLoss: 0.000623\twhole_loss: 0.002642 \n",
            "Train Epoch: 150 [80/486 (16%)]\tlearningLoss: 0.001238\twhole_loss: 0.107641 \n",
            "Train Epoch: 150 [96/486 (20%)]\tlearningLoss: 0.001267\twhole_loss: 0.005094 \n",
            "Train Epoch: 150 [112/486 (23%)]\tlearningLoss: 0.001570\twhole_loss: 0.053121 \n",
            "Train Epoch: 150 [128/486 (26%)]\tlearningLoss: 0.002254\twhole_loss: 0.119693 \n",
            "Train Epoch: 150 [144/486 (30%)]\tlearningLoss: 0.002278\twhole_loss: 0.004100 \n",
            "Train Epoch: 150 [160/486 (33%)]\tlearningLoss: 0.002286\twhole_loss: 0.001445 \n",
            "Train Epoch: 150 [176/486 (36%)]\tlearningLoss: 0.002294\twhole_loss: 0.001395 \n",
            "Train Epoch: 150 [192/486 (40%)]\tlearningLoss: 0.002333\twhole_loss: 0.006789 \n",
            "Train Epoch: 150 [208/486 (43%)]\tlearningLoss: 0.002349\twhole_loss: 0.002809 \n",
            "Train Epoch: 150 [224/486 (46%)]\tlearningLoss: 0.002569\twhole_loss: 0.038486 \n",
            "Train Epoch: 150 [240/486 (49%)]\tlearningLoss: 0.002635\twhole_loss: 0.011535 \n",
            "Train Epoch: 150 [256/486 (53%)]\tlearningLoss: 0.002804\twhole_loss: 0.029681 \n",
            "Train Epoch: 150 [272/486 (56%)]\tlearningLoss: 0.002872\twhole_loss: 0.011937 \n",
            "Train Epoch: 150 [288/486 (59%)]\tlearningLoss: 0.002978\twhole_loss: 0.018472 \n",
            "Train Epoch: 150 [304/486 (63%)]\tlearningLoss: 0.003576\twhole_loss: 0.104694 \n",
            "Train Epoch: 150 [320/486 (66%)]\tlearningLoss: 0.003587\twhole_loss: 0.001840 \n",
            "Train Epoch: 150 [336/486 (69%)]\tlearningLoss: 0.003604\twhole_loss: 0.003075 \n",
            "Train Epoch: 150 [352/486 (72%)]\tlearningLoss: 0.003617\twhole_loss: 0.002153 \n",
            "Train Epoch: 150 [368/486 (76%)]\tlearningLoss: 0.004327\twhole_loss: 0.124240 \n",
            "Train Epoch: 150 [384/486 (79%)]\tlearningLoss: 0.004432\twhole_loss: 0.018513 \n",
            "Train Epoch: 150 [400/486 (82%)]\tlearningLoss: 0.004569\twhole_loss: 0.023998 \n",
            "Train Epoch: 150 [416/486 (86%)]\tlearningLoss: 0.004689\twhole_loss: 0.020869 \n",
            "Train Epoch: 150 [432/486 (89%)]\tlearningLoss: 0.004716\twhole_loss: 0.004755 \n",
            "Train Epoch: 150 [448/486 (92%)]\tlearningLoss: 0.004729\twhole_loss: 0.002308 \n",
            "Train Epoch: 150 [464/486 (95%)]\tlearningLoss: 0.004739\twhole_loss: 0.001680 \n",
            "Train Epoch: 150 [180/486 (37%)]\tlearningLoss: 0.004743\twhole_loss: 0.000835 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 2, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:5.3344, Accuracy: 65/75(87%)\n",
            "\n",
            "Train Epoch: 151 [0/486 (0%)]\tlearningLoss: 0.000038\twhole_loss: 0.006688 \n",
            "Train Epoch: 151 [16/486 (3%)]\tlearningLoss: 0.000054\twhole_loss: 0.002812 \n",
            "Train Epoch: 151 [32/486 (7%)]\tlearningLoss: 0.000186\twhole_loss: 0.023112 \n",
            "Train Epoch: 151 [48/486 (10%)]\tlearningLoss: 0.000197\twhole_loss: 0.001826 \n",
            "Train Epoch: 151 [64/486 (13%)]\tlearningLoss: 0.000210\twhole_loss: 0.002293 \n",
            "Train Epoch: 151 [80/486 (16%)]\tlearningLoss: 0.000229\twhole_loss: 0.003383 \n",
            "Train Epoch: 151 [96/486 (20%)]\tlearningLoss: 0.000565\twhole_loss: 0.058730 \n",
            "Train Epoch: 151 [112/486 (23%)]\tlearningLoss: 0.000568\twhole_loss: 0.000491 \n",
            "Train Epoch: 151 [128/486 (26%)]\tlearningLoss: 0.000573\twhole_loss: 0.000883 \n",
            "Train Epoch: 151 [144/486 (30%)]\tlearningLoss: 0.000574\twhole_loss: 0.000192 \n",
            "Train Epoch: 151 [160/486 (33%)]\tlearningLoss: 0.001657\twhole_loss: 0.189539 \n",
            "Train Epoch: 151 [176/486 (36%)]\tlearningLoss: 0.001681\twhole_loss: 0.004251 \n",
            "Train Epoch: 151 [192/486 (40%)]\tlearningLoss: 0.001692\twhole_loss: 0.001952 \n",
            "Train Epoch: 151 [208/486 (43%)]\tlearningLoss: 0.001695\twhole_loss: 0.000437 \n",
            "Train Epoch: 151 [224/486 (46%)]\tlearningLoss: 0.001723\twhole_loss: 0.004863 \n",
            "Train Epoch: 151 [240/486 (49%)]\tlearningLoss: 0.001762\twhole_loss: 0.006813 \n",
            "Train Epoch: 151 [256/486 (53%)]\tlearningLoss: 0.002123\twhole_loss: 0.063316 \n",
            "Train Epoch: 151 [272/486 (56%)]\tlearningLoss: 0.002317\twhole_loss: 0.033889 \n",
            "Train Epoch: 151 [288/486 (59%)]\tlearningLoss: 0.002848\twhole_loss: 0.092914 \n",
            "Train Epoch: 151 [304/486 (63%)]\tlearningLoss: 0.004116\twhole_loss: 0.221844 \n",
            "Train Epoch: 151 [320/486 (66%)]\tlearningLoss: 0.004161\twhole_loss: 0.008030 \n",
            "Train Epoch: 151 [336/486 (69%)]\tlearningLoss: 0.004168\twhole_loss: 0.001217 \n",
            "Train Epoch: 151 [352/486 (72%)]\tlearningLoss: 0.004175\twhole_loss: 0.001068 \n",
            "Train Epoch: 151 [368/486 (76%)]\tlearningLoss: 0.004177\twhole_loss: 0.000516 \n",
            "Train Epoch: 151 [384/486 (79%)]\tlearningLoss: 0.005234\twhole_loss: 0.184884 \n",
            "Train Epoch: 151 [400/486 (82%)]\tlearningLoss: 0.006278\twhole_loss: 0.182732 \n",
            "Train Epoch: 151 [416/486 (86%)]\tlearningLoss: 0.006291\twhole_loss: 0.002278 \n",
            "Train Epoch: 151 [432/486 (89%)]\tlearningLoss: 0.008743\twhole_loss: 0.429113 \n",
            "Train Epoch: 151 [448/486 (92%)]\tlearningLoss: 0.008902\twhole_loss: 0.027801 \n",
            "Train Epoch: 151 [464/486 (95%)]\tlearningLoss: 0.008952\twhole_loss: 0.008722 \n",
            "Train Epoch: 151 [180/486 (37%)]\tlearningLoss: 0.014466\twhole_loss: 0.964918 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.5884, Accuracy: 71/75(95%)\n",
            "\n",
            "Train Epoch: 152 [0/486 (0%)]\tlearningLoss: 0.000142\twhole_loss: 0.024828 \n",
            "Train Epoch: 152 [16/486 (3%)]\tlearningLoss: 0.000738\twhole_loss: 0.104356 \n",
            "Train Epoch: 152 [32/486 (7%)]\tlearningLoss: 0.000916\twhole_loss: 0.031043 \n",
            "Train Epoch: 152 [48/486 (10%)]\tlearningLoss: 0.001141\twhole_loss: 0.039465 \n",
            "Train Epoch: 152 [64/486 (13%)]\tlearningLoss: 0.002462\twhole_loss: 0.231227 \n",
            "Train Epoch: 152 [80/486 (16%)]\tlearningLoss: 0.005764\twhole_loss: 0.577700 \n",
            "Train Epoch: 152 [96/486 (20%)]\tlearningLoss: 0.005764\twhole_loss: 0.000141 \n",
            "Train Epoch: 152 [112/486 (23%)]\tlearningLoss: 0.005806\twhole_loss: 0.007316 \n",
            "Train Epoch: 152 [128/486 (26%)]\tlearningLoss: 0.005853\twhole_loss: 0.008246 \n",
            "Train Epoch: 152 [144/486 (30%)]\tlearningLoss: 0.006138\twhole_loss: 0.049907 \n",
            "Train Epoch: 152 [160/486 (33%)]\tlearningLoss: 0.006283\twhole_loss: 0.025340 \n",
            "Train Epoch: 152 [176/486 (36%)]\tlearningLoss: 0.006542\twhole_loss: 0.045284 \n",
            "Train Epoch: 152 [192/486 (40%)]\tlearningLoss: 0.006810\twhole_loss: 0.046919 \n",
            "Train Epoch: 152 [208/486 (43%)]\tlearningLoss: 0.006874\twhole_loss: 0.011205 \n",
            "Train Epoch: 152 [224/486 (46%)]\tlearningLoss: 0.006913\twhole_loss: 0.006731 \n",
            "Train Epoch: 152 [240/486 (49%)]\tlearningLoss: 0.008146\twhole_loss: 0.215778 \n",
            "Train Epoch: 152 [256/486 (53%)]\tlearningLoss: 0.008542\twhole_loss: 0.069289 \n",
            "Train Epoch: 152 [272/486 (56%)]\tlearningLoss: 0.008585\twhole_loss: 0.007632 \n",
            "Train Epoch: 152 [288/486 (59%)]\tlearningLoss: 0.008611\twhole_loss: 0.004455 \n",
            "Train Epoch: 152 [304/486 (63%)]\tlearningLoss: 0.009021\twhole_loss: 0.071805 \n",
            "Train Epoch: 152 [320/486 (66%)]\tlearningLoss: 0.009077\twhole_loss: 0.009842 \n",
            "Train Epoch: 152 [336/486 (69%)]\tlearningLoss: 0.009150\twhole_loss: 0.012758 \n",
            "Train Epoch: 152 [352/486 (72%)]\tlearningLoss: 0.009508\twhole_loss: 0.062656 \n",
            "Train Epoch: 152 [368/486 (76%)]\tlearningLoss: 0.009528\twhole_loss: 0.003508 \n",
            "Train Epoch: 152 [384/486 (79%)]\tlearningLoss: 0.010067\twhole_loss: 0.094297 \n",
            "Train Epoch: 152 [400/486 (82%)]\tlearningLoss: 0.010403\twhole_loss: 0.058791 \n",
            "Train Epoch: 152 [416/486 (86%)]\tlearningLoss: 0.010471\twhole_loss: 0.011844 \n",
            "Train Epoch: 152 [432/486 (89%)]\tlearningLoss: 0.010647\twhole_loss: 0.030784 \n",
            "Train Epoch: 152 [448/486 (92%)]\tlearningLoss: 0.010732\twhole_loss: 0.014890 \n",
            "Train Epoch: 152 [464/486 (95%)]\tlearningLoss: 0.010980\twhole_loss: 0.043453 \n",
            "Train Epoch: 152 [180/486 (37%)]\tlearningLoss: 0.011019\twhole_loss: 0.006819 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 1, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.0085, Accuracy: 69/75(92%)\n",
            "\n",
            "Train Epoch: 153 [0/486 (0%)]\tlearningLoss: 0.000596\twhole_loss: 0.104309 \n",
            "Train Epoch: 153 [16/486 (3%)]\tlearningLoss: 0.000721\twhole_loss: 0.021931 \n",
            "Train Epoch: 153 [32/486 (7%)]\tlearningLoss: 0.000733\twhole_loss: 0.002023 \n",
            "Train Epoch: 153 [48/486 (10%)]\tlearningLoss: 0.000755\twhole_loss: 0.003837 \n",
            "Train Epoch: 153 [64/486 (13%)]\tlearningLoss: 0.000755\twhole_loss: 0.000049 \n",
            "Train Epoch: 153 [80/486 (16%)]\tlearningLoss: 0.000767\twhole_loss: 0.002016 \n",
            "Train Epoch: 153 [96/486 (20%)]\tlearningLoss: 0.000918\twhole_loss: 0.026452 \n",
            "Train Epoch: 153 [112/486 (23%)]\tlearningLoss: 0.000935\twhole_loss: 0.003013 \n",
            "Train Epoch: 153 [128/486 (26%)]\tlearningLoss: 0.000968\twhole_loss: 0.005701 \n",
            "Train Epoch: 153 [144/486 (30%)]\tlearningLoss: 0.000993\twhole_loss: 0.004513 \n",
            "Train Epoch: 153 [160/486 (33%)]\tlearningLoss: 0.001042\twhole_loss: 0.008485 \n",
            "Train Epoch: 153 [176/486 (36%)]\tlearningLoss: 0.001066\twhole_loss: 0.004300 \n",
            "Train Epoch: 153 [192/486 (40%)]\tlearningLoss: 0.001211\twhole_loss: 0.025368 \n",
            "Train Epoch: 153 [208/486 (43%)]\tlearningLoss: 0.001960\twhole_loss: 0.131050 \n",
            "Train Epoch: 153 [224/486 (46%)]\tlearningLoss: 0.002081\twhole_loss: 0.021077 \n",
            "Train Epoch: 153 [240/486 (49%)]\tlearningLoss: 0.002095\twhole_loss: 0.002463 \n",
            "Train Epoch: 153 [256/486 (53%)]\tlearningLoss: 0.002149\twhole_loss: 0.009524 \n",
            "Train Epoch: 153 [272/486 (56%)]\tlearningLoss: 0.002159\twhole_loss: 0.001652 \n",
            "Train Epoch: 153 [288/486 (59%)]\tlearningLoss: 0.002168\twhole_loss: 0.001680 \n",
            "Train Epoch: 153 [304/486 (63%)]\tlearningLoss: 0.002287\twhole_loss: 0.020772 \n",
            "Train Epoch: 153 [320/486 (66%)]\tlearningLoss: 0.002309\twhole_loss: 0.003830 \n",
            "Train Epoch: 153 [336/486 (69%)]\tlearningLoss: 0.002352\twhole_loss: 0.007640 \n",
            "Train Epoch: 153 [352/486 (72%)]\tlearningLoss: 0.002477\twhole_loss: 0.021875 \n",
            "Train Epoch: 153 [368/486 (76%)]\tlearningLoss: 0.002762\twhole_loss: 0.049739 \n",
            "Train Epoch: 153 [384/486 (79%)]\tlearningLoss: 0.002880\twhole_loss: 0.020766 \n",
            "Train Epoch: 153 [400/486 (82%)]\tlearningLoss: 0.003903\twhole_loss: 0.179044 \n",
            "Train Epoch: 153 [416/486 (86%)]\tlearningLoss: 0.003968\twhole_loss: 0.011256 \n",
            "Train Epoch: 153 [432/486 (89%)]\tlearningLoss: 0.004002\twhole_loss: 0.006017 \n",
            "Train Epoch: 153 [448/486 (92%)]\tlearningLoss: 0.004010\twhole_loss: 0.001304 \n",
            "Train Epoch: 153 [464/486 (95%)]\tlearningLoss: 0.004038\twhole_loss: 0.004980 \n",
            "Train Epoch: 153 [180/486 (37%)]\tlearningLoss: 0.004057\twhole_loss: 0.003260 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.6921, Accuracy: 66/75(88%)\n",
            "\n",
            "Train Epoch: 154 [0/486 (0%)]\tlearningLoss: 0.000004\twhole_loss: 0.000638 \n",
            "Train Epoch: 154 [16/486 (3%)]\tlearningLoss: 0.000033\twhole_loss: 0.005083 \n",
            "Train Epoch: 154 [32/486 (7%)]\tlearningLoss: 0.000173\twhole_loss: 0.024474 \n",
            "Train Epoch: 154 [48/486 (10%)]\tlearningLoss: 0.000851\twhole_loss: 0.118711 \n",
            "Train Epoch: 154 [64/486 (13%)]\tlearningLoss: 0.000856\twhole_loss: 0.000940 \n",
            "Train Epoch: 154 [80/486 (16%)]\tlearningLoss: 0.000865\twhole_loss: 0.001451 \n",
            "Train Epoch: 154 [96/486 (20%)]\tlearningLoss: 0.000894\twhole_loss: 0.005080 \n",
            "Train Epoch: 154 [112/486 (23%)]\tlearningLoss: 0.000895\twhole_loss: 0.000327 \n",
            "Train Epoch: 154 [128/486 (26%)]\tlearningLoss: 0.000988\twhole_loss: 0.016161 \n",
            "Train Epoch: 154 [144/486 (30%)]\tlearningLoss: 0.001017\twhole_loss: 0.005171 \n",
            "Train Epoch: 154 [160/486 (33%)]\tlearningLoss: 0.001583\twhole_loss: 0.099059 \n",
            "Train Epoch: 154 [176/486 (36%)]\tlearningLoss: 0.001622\twhole_loss: 0.006726 \n",
            "Train Epoch: 154 [192/486 (40%)]\tlearningLoss: 0.001644\twhole_loss: 0.003927 \n",
            "Train Epoch: 154 [208/486 (43%)]\tlearningLoss: 0.001722\twhole_loss: 0.013644 \n",
            "Train Epoch: 154 [224/486 (46%)]\tlearningLoss: 0.001728\twhole_loss: 0.000987 \n",
            "Train Epoch: 154 [240/486 (49%)]\tlearningLoss: 0.001732\twhole_loss: 0.000723 \n",
            "Train Epoch: 154 [256/486 (53%)]\tlearningLoss: 0.001821\twhole_loss: 0.015511 \n",
            "Train Epoch: 154 [272/486 (56%)]\tlearningLoss: 0.001833\twhole_loss: 0.002161 \n",
            "Train Epoch: 154 [288/486 (59%)]\tlearningLoss: 0.003133\twhole_loss: 0.227436 \n",
            "Train Epoch: 154 [304/486 (63%)]\tlearningLoss: 0.003163\twhole_loss: 0.005267 \n",
            "Train Epoch: 154 [320/486 (66%)]\tlearningLoss: 0.003249\twhole_loss: 0.015140 \n",
            "Train Epoch: 154 [336/486 (69%)]\tlearningLoss: 0.003478\twhole_loss: 0.040114 \n",
            "Train Epoch: 154 [352/486 (72%)]\tlearningLoss: 0.003514\twhole_loss: 0.006139 \n",
            "Train Epoch: 154 [368/486 (76%)]\tlearningLoss: 0.003799\twhole_loss: 0.050008 \n",
            "Train Epoch: 154 [384/486 (79%)]\tlearningLoss: 0.003834\twhole_loss: 0.006001 \n",
            "Train Epoch: 154 [400/486 (82%)]\tlearningLoss: 0.004344\twhole_loss: 0.089328 \n",
            "Train Epoch: 154 [416/486 (86%)]\tlearningLoss: 0.005140\twhole_loss: 0.139367 \n",
            "Train Epoch: 154 [432/486 (89%)]\tlearningLoss: 0.005169\twhole_loss: 0.005033 \n",
            "Train Epoch: 154 [448/486 (92%)]\tlearningLoss: 0.005235\twhole_loss: 0.011580 \n",
            "Train Epoch: 154 [464/486 (95%)]\tlearningLoss: 0.005292\twhole_loss: 0.009834 \n",
            "Train Epoch: 154 [180/486 (37%)]\tlearningLoss: 0.005306\twhole_loss: 0.002559 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 2, 1, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.7903, Accuracy: 64/75(85%)\n",
            "\n",
            "Train Epoch: 155 [0/486 (0%)]\tlearningLoss: 0.000009\twhole_loss: 0.001615 \n",
            "Train Epoch: 155 [16/486 (3%)]\tlearningLoss: 0.000052\twhole_loss: 0.007440 \n",
            "Train Epoch: 155 [32/486 (7%)]\tlearningLoss: 0.000087\twhole_loss: 0.006199 \n",
            "Train Epoch: 155 [48/486 (10%)]\tlearningLoss: 0.000291\twhole_loss: 0.035752 \n",
            "Train Epoch: 155 [64/486 (13%)]\tlearningLoss: 0.000320\twhole_loss: 0.004985 \n",
            "Train Epoch: 155 [80/486 (16%)]\tlearningLoss: 0.000331\twhole_loss: 0.001927 \n",
            "Train Epoch: 155 [96/486 (20%)]\tlearningLoss: 0.000670\twhole_loss: 0.059334 \n",
            "Train Epoch: 155 [112/486 (23%)]\tlearningLoss: 0.000689\twhole_loss: 0.003327 \n",
            "Train Epoch: 155 [128/486 (26%)]\tlearningLoss: 0.000751\twhole_loss: 0.010852 \n",
            "Train Epoch: 155 [144/486 (30%)]\tlearningLoss: 0.000811\twhole_loss: 0.010486 \n",
            "Train Epoch: 155 [160/486 (33%)]\tlearningLoss: 0.000815\twhole_loss: 0.000651 \n",
            "Train Epoch: 155 [176/486 (36%)]\tlearningLoss: 0.000824\twhole_loss: 0.001617 \n",
            "Train Epoch: 155 [192/486 (40%)]\tlearningLoss: 0.000982\twhole_loss: 0.027696 \n",
            "Train Epoch: 155 [208/486 (43%)]\tlearningLoss: 0.001322\twhole_loss: 0.059495 \n",
            "Train Epoch: 155 [224/486 (46%)]\tlearningLoss: 0.001326\twhole_loss: 0.000637 \n",
            "Train Epoch: 155 [240/486 (49%)]\tlearningLoss: 0.001354\twhole_loss: 0.005024 \n",
            "Train Epoch: 155 [256/486 (53%)]\tlearningLoss: 0.001502\twhole_loss: 0.025881 \n",
            "Train Epoch: 155 [272/486 (56%)]\tlearningLoss: 0.001560\twhole_loss: 0.010010 \n",
            "Train Epoch: 155 [288/486 (59%)]\tlearningLoss: 0.001616\twhole_loss: 0.009954 \n",
            "Train Epoch: 155 [304/486 (63%)]\tlearningLoss: 0.001618\twhole_loss: 0.000250 \n",
            "Train Epoch: 155 [320/486 (66%)]\tlearningLoss: 0.001661\twhole_loss: 0.007600 \n",
            "Train Epoch: 155 [336/486 (69%)]\tlearningLoss: 0.001767\twhole_loss: 0.018409 \n",
            "Train Epoch: 155 [352/486 (72%)]\tlearningLoss: 0.001816\twhole_loss: 0.008606 \n",
            "Train Epoch: 155 [368/486 (76%)]\tlearningLoss: 0.002014\twhole_loss: 0.034678 \n",
            "Train Epoch: 155 [384/486 (79%)]\tlearningLoss: 0.002020\twhole_loss: 0.001058 \n",
            "Train Epoch: 155 [400/486 (82%)]\tlearningLoss: 0.002209\twhole_loss: 0.033109 \n",
            "Train Epoch: 155 [416/486 (86%)]\tlearningLoss: 0.002219\twhole_loss: 0.001722 \n",
            "Train Epoch: 155 [432/486 (89%)]\tlearningLoss: 0.002241\twhole_loss: 0.003783 \n",
            "Train Epoch: 155 [448/486 (92%)]\tlearningLoss: 0.002253\twhole_loss: 0.002171 \n",
            "Train Epoch: 155 [464/486 (95%)]\tlearningLoss: 0.002260\twhole_loss: 0.001266 \n",
            "Train Epoch: 155 [180/486 (37%)]\tlearningLoss: 0.002271\twhole_loss: 0.001804 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.9633, Accuracy: 66/75(88%)\n",
            "\n",
            "Train Epoch: 156 [0/486 (0%)]\tlearningLoss: 0.000041\twhole_loss: 0.007219 \n",
            "Train Epoch: 156 [16/486 (3%)]\tlearningLoss: 0.000104\twhole_loss: 0.011010 \n",
            "Train Epoch: 156 [32/486 (7%)]\tlearningLoss: 0.000108\twhole_loss: 0.000663 \n",
            "Train Epoch: 156 [48/486 (10%)]\tlearningLoss: 0.000120\twhole_loss: 0.002167 \n",
            "Train Epoch: 156 [64/486 (13%)]\tlearningLoss: 0.000176\twhole_loss: 0.009780 \n",
            "Train Epoch: 156 [80/486 (16%)]\tlearningLoss: 0.000183\twhole_loss: 0.001161 \n",
            "Train Epoch: 156 [96/486 (20%)]\tlearningLoss: 0.000184\twhole_loss: 0.000250 \n",
            "Train Epoch: 156 [112/486 (23%)]\tlearningLoss: 0.000191\twhole_loss: 0.001114 \n",
            "Train Epoch: 156 [128/486 (26%)]\tlearningLoss: 0.000255\twhole_loss: 0.011271 \n",
            "Train Epoch: 156 [144/486 (30%)]\tlearningLoss: 0.000286\twhole_loss: 0.005499 \n",
            "Train Epoch: 156 [160/486 (33%)]\tlearningLoss: 0.000290\twhole_loss: 0.000543 \n",
            "Train Epoch: 156 [176/486 (36%)]\tlearningLoss: 0.000290\twhole_loss: 0.000149 \n",
            "Train Epoch: 156 [192/486 (40%)]\tlearningLoss: 0.000297\twhole_loss: 0.001201 \n",
            "Train Epoch: 156 [208/486 (43%)]\tlearningLoss: 0.000308\twhole_loss: 0.001948 \n",
            "Train Epoch: 156 [224/486 (46%)]\tlearningLoss: 0.000312\twhole_loss: 0.000645 \n",
            "Train Epoch: 156 [240/486 (49%)]\tlearningLoss: 0.000314\twhole_loss: 0.000284 \n",
            "Train Epoch: 156 [256/486 (53%)]\tlearningLoss: 0.000364\twhole_loss: 0.008861 \n",
            "Train Epoch: 156 [272/486 (56%)]\tlearningLoss: 0.000369\twhole_loss: 0.000827 \n",
            "Train Epoch: 156 [288/486 (59%)]\tlearningLoss: 0.000400\twhole_loss: 0.005465 \n",
            "Train Epoch: 156 [304/486 (63%)]\tlearningLoss: 0.000401\twhole_loss: 0.000127 \n",
            "Train Epoch: 156 [320/486 (66%)]\tlearningLoss: 0.000480\twhole_loss: 0.013781 \n",
            "Train Epoch: 156 [336/486 (69%)]\tlearningLoss: 0.000570\twhole_loss: 0.015845 \n",
            "Train Epoch: 156 [352/486 (72%)]\tlearningLoss: 0.000597\twhole_loss: 0.004697 \n",
            "Train Epoch: 156 [368/486 (76%)]\tlearningLoss: 0.000605\twhole_loss: 0.001443 \n",
            "Train Epoch: 156 [384/486 (79%)]\tlearningLoss: 0.000625\twhole_loss: 0.003374 \n",
            "Train Epoch: 156 [400/486 (82%)]\tlearningLoss: 0.000635\twhole_loss: 0.001803 \n",
            "Train Epoch: 156 [416/486 (86%)]\tlearningLoss: 0.000804\twhole_loss: 0.029489 \n",
            "Train Epoch: 156 [432/486 (89%)]\tlearningLoss: 0.000804\twhole_loss: 0.000103 \n",
            "Train Epoch: 156 [448/486 (92%)]\tlearningLoss: 0.000812\twhole_loss: 0.001444 \n",
            "Train Epoch: 156 [464/486 (95%)]\tlearningLoss: 0.000817\twhole_loss: 0.000885 \n",
            "Train Epoch: 156 [180/486 (37%)]\tlearningLoss: 0.000818\twhole_loss: 0.000147 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 2, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.6017, Accuracy: 64/75(85%)\n",
            "\n",
            "Train Epoch: 157 [0/486 (0%)]\tlearningLoss: 0.000235\twhole_loss: 0.041158 \n",
            "Train Epoch: 157 [16/486 (3%)]\tlearningLoss: 0.000236\twhole_loss: 0.000107 \n",
            "Train Epoch: 157 [32/486 (7%)]\tlearningLoss: 0.000244\twhole_loss: 0.001439 \n",
            "Train Epoch: 157 [48/486 (10%)]\tlearningLoss: 0.000253\twhole_loss: 0.001603 \n",
            "Train Epoch: 157 [64/486 (13%)]\tlearningLoss: 0.000255\twhole_loss: 0.000238 \n",
            "Train Epoch: 157 [80/486 (16%)]\tlearningLoss: 0.000284\twhole_loss: 0.005069 \n",
            "Train Epoch: 157 [96/486 (20%)]\tlearningLoss: 0.000309\twhole_loss: 0.004409 \n",
            "Train Epoch: 157 [112/486 (23%)]\tlearningLoss: 0.000347\twhole_loss: 0.006669 \n",
            "Train Epoch: 157 [128/486 (26%)]\tlearningLoss: 0.000364\twhole_loss: 0.003055 \n",
            "Train Epoch: 157 [144/486 (30%)]\tlearningLoss: 0.000366\twhole_loss: 0.000281 \n",
            "Train Epoch: 157 [160/486 (33%)]\tlearningLoss: 0.000368\twhole_loss: 0.000370 \n",
            "Train Epoch: 157 [176/486 (36%)]\tlearningLoss: 0.000379\twhole_loss: 0.001865 \n",
            "Train Epoch: 157 [192/486 (40%)]\tlearningLoss: 0.000488\twhole_loss: 0.019103 \n",
            "Train Epoch: 157 [208/486 (43%)]\tlearningLoss: 0.000499\twhole_loss: 0.002033 \n",
            "Train Epoch: 157 [224/486 (46%)]\tlearningLoss: 0.000556\twhole_loss: 0.009944 \n",
            "Train Epoch: 157 [240/486 (49%)]\tlearningLoss: 0.000713\twhole_loss: 0.027356 \n",
            "Train Epoch: 157 [256/486 (53%)]\tlearningLoss: 0.000716\twhole_loss: 0.000671 \n",
            "Train Epoch: 157 [272/486 (56%)]\tlearningLoss: 0.000746\twhole_loss: 0.005162 \n",
            "Train Epoch: 157 [288/486 (59%)]\tlearningLoss: 0.000784\twhole_loss: 0.006590 \n",
            "Train Epoch: 157 [304/486 (63%)]\tlearningLoss: 0.000816\twhole_loss: 0.005646 \n",
            "Train Epoch: 157 [320/486 (66%)]\tlearningLoss: 0.000817\twhole_loss: 0.000199 \n",
            "Train Epoch: 157 [336/486 (69%)]\tlearningLoss: 0.000819\twhole_loss: 0.000324 \n",
            "Train Epoch: 157 [352/486 (72%)]\tlearningLoss: 0.000847\twhole_loss: 0.004937 \n",
            "Train Epoch: 157 [368/486 (76%)]\tlearningLoss: 0.000862\twhole_loss: 0.002692 \n",
            "Train Epoch: 157 [384/486 (79%)]\tlearningLoss: 0.000900\twhole_loss: 0.006656 \n",
            "Train Epoch: 157 [400/486 (82%)]\tlearningLoss: 0.001004\twhole_loss: 0.018038 \n",
            "Train Epoch: 157 [416/486 (86%)]\tlearningLoss: 0.001118\twhole_loss: 0.020045 \n",
            "Train Epoch: 157 [432/486 (89%)]\tlearningLoss: 0.001323\twhole_loss: 0.035864 \n",
            "Train Epoch: 157 [448/486 (92%)]\tlearningLoss: 0.001324\twhole_loss: 0.000212 \n",
            "Train Epoch: 157 [464/486 (95%)]\tlearningLoss: 0.001374\twhole_loss: 0.008739 \n",
            "Train Epoch: 157 [180/486 (37%)]\tlearningLoss: 0.001377\twhole_loss: 0.000448 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 2, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.2965, Accuracy: 66/75(88%)\n",
            "\n",
            "Train Epoch: 158 [0/486 (0%)]\tlearningLoss: 0.000008\twhole_loss: 0.001462 \n",
            "Train Epoch: 158 [16/486 (3%)]\tlearningLoss: 0.000110\twhole_loss: 0.017869 \n",
            "Train Epoch: 158 [32/486 (7%)]\tlearningLoss: 0.000154\twhole_loss: 0.007666 \n",
            "Train Epoch: 158 [48/486 (10%)]\tlearningLoss: 0.000157\twhole_loss: 0.000521 \n",
            "Train Epoch: 158 [64/486 (13%)]\tlearningLoss: 0.000189\twhole_loss: 0.005625 \n",
            "Train Epoch: 158 [80/486 (16%)]\tlearningLoss: 0.000191\twhole_loss: 0.000211 \n",
            "Train Epoch: 158 [96/486 (20%)]\tlearningLoss: 0.000192\twhole_loss: 0.000196 \n",
            "Train Epoch: 158 [112/486 (23%)]\tlearningLoss: 0.000206\twhole_loss: 0.002466 \n",
            "Train Epoch: 158 [128/486 (26%)]\tlearningLoss: 0.000209\twhole_loss: 0.000591 \n",
            "Train Epoch: 158 [144/486 (30%)]\tlearningLoss: 0.000240\twhole_loss: 0.005336 \n",
            "Train Epoch: 158 [160/486 (33%)]\tlearningLoss: 0.000240\twhole_loss: 0.000079 \n",
            "Train Epoch: 158 [176/486 (36%)]\tlearningLoss: 0.000274\twhole_loss: 0.005894 \n",
            "Train Epoch: 158 [192/486 (40%)]\tlearningLoss: 0.000283\twhole_loss: 0.001524 \n",
            "Train Epoch: 158 [208/486 (43%)]\tlearningLoss: 0.000297\twhole_loss: 0.002515 \n",
            "Train Epoch: 158 [224/486 (46%)]\tlearningLoss: 0.000337\twhole_loss: 0.007094 \n",
            "Train Epoch: 158 [240/486 (49%)]\tlearningLoss: 0.000360\twhole_loss: 0.004003 \n",
            "Train Epoch: 158 [256/486 (53%)]\tlearningLoss: 0.000367\twhole_loss: 0.001259 \n",
            "Train Epoch: 158 [272/486 (56%)]\tlearningLoss: 0.000844\twhole_loss: 0.083460 \n",
            "Train Epoch: 158 [288/486 (59%)]\tlearningLoss: 0.000851\twhole_loss: 0.001072 \n",
            "Train Epoch: 158 [304/486 (63%)]\tlearningLoss: 0.000853\twhole_loss: 0.000394 \n",
            "Train Epoch: 158 [320/486 (66%)]\tlearningLoss: 0.000858\twhole_loss: 0.000965 \n",
            "Train Epoch: 158 [336/486 (69%)]\tlearningLoss: 0.001103\twhole_loss: 0.042882 \n",
            "Train Epoch: 158 [352/486 (72%)]\tlearningLoss: 0.001192\twhole_loss: 0.015551 \n",
            "Train Epoch: 158 [368/486 (76%)]\tlearningLoss: 0.001554\twhole_loss: 0.063324 \n",
            "Train Epoch: 158 [384/486 (79%)]\tlearningLoss: 0.001605\twhole_loss: 0.008890 \n",
            "Train Epoch: 158 [400/486 (82%)]\tlearningLoss: 0.003624\twhole_loss: 0.353411 \n",
            "Train Epoch: 158 [416/486 (86%)]\tlearningLoss: 0.003624\twhole_loss: 0.000016 \n",
            "Train Epoch: 158 [432/486 (89%)]\tlearningLoss: 0.003680\twhole_loss: 0.009643 \n",
            "Train Epoch: 158 [448/486 (92%)]\tlearningLoss: 0.003680\twhole_loss: 0.000026 \n",
            "Train Epoch: 158 [464/486 (95%)]\tlearningLoss: 0.003697\twhole_loss: 0.003114 \n",
            "Train Epoch: 158 [180/486 (37%)]\tlearningLoss: 0.003699\twhole_loss: 0.000299 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.0969, Accuracy: 67/75(89%)\n",
            "\n",
            "Train Epoch: 159 [0/486 (0%)]\tlearningLoss: 0.000027\twhole_loss: 0.004666 \n",
            "Train Epoch: 159 [16/486 (3%)]\tlearningLoss: 0.000237\twhole_loss: 0.036851 \n",
            "Train Epoch: 159 [32/486 (7%)]\tlearningLoss: 0.000238\twhole_loss: 0.000158 \n",
            "Train Epoch: 159 [48/486 (10%)]\tlearningLoss: 0.000240\twhole_loss: 0.000295 \n",
            "Train Epoch: 159 [64/486 (13%)]\tlearningLoss: 0.000242\twhole_loss: 0.000356 \n",
            "Train Epoch: 159 [80/486 (16%)]\tlearningLoss: 0.000248\twhole_loss: 0.001090 \n",
            "Train Epoch: 159 [96/486 (20%)]\tlearningLoss: 0.000250\twhole_loss: 0.000260 \n",
            "Train Epoch: 159 [112/486 (23%)]\tlearningLoss: 0.001713\twhole_loss: 0.256051 \n",
            "Train Epoch: 159 [128/486 (26%)]\tlearningLoss: 0.001906\twhole_loss: 0.033825 \n",
            "Train Epoch: 159 [144/486 (30%)]\tlearningLoss: 0.002896\twhole_loss: 0.173277 \n",
            "Train Epoch: 159 [160/486 (33%)]\tlearningLoss: 0.002910\twhole_loss: 0.002335 \n",
            "Train Epoch: 159 [176/486 (36%)]\tlearningLoss: 0.002915\twhole_loss: 0.001029 \n",
            "Train Epoch: 159 [192/486 (40%)]\tlearningLoss: 0.002939\twhole_loss: 0.004136 \n",
            "Train Epoch: 159 [208/486 (43%)]\tlearningLoss: 0.002940\twhole_loss: 0.000102 \n",
            "Train Epoch: 159 [224/486 (46%)]\tlearningLoss: 0.003581\twhole_loss: 0.112228 \n",
            "Train Epoch: 159 [240/486 (49%)]\tlearningLoss: 0.003697\twhole_loss: 0.020304 \n",
            "Train Epoch: 159 [256/486 (53%)]\tlearningLoss: 0.005811\twhole_loss: 0.369935 \n",
            "Train Epoch: 159 [272/486 (56%)]\tlearningLoss: 0.005847\twhole_loss: 0.006241 \n",
            "Train Epoch: 159 [288/486 (59%)]\tlearningLoss: 0.006109\twhole_loss: 0.045962 \n",
            "Train Epoch: 159 [304/486 (63%)]\tlearningLoss: 0.006237\twhole_loss: 0.022317 \n",
            "Train Epoch: 159 [320/486 (66%)]\tlearningLoss: 0.006294\twhole_loss: 0.009965 \n",
            "Train Epoch: 159 [336/486 (69%)]\tlearningLoss: 0.006731\twhole_loss: 0.076559 \n",
            "Train Epoch: 159 [352/486 (72%)]\tlearningLoss: 0.008439\twhole_loss: 0.298905 \n",
            "Train Epoch: 159 [368/486 (76%)]\tlearningLoss: 0.008465\twhole_loss: 0.004463 \n",
            "Train Epoch: 159 [384/486 (79%)]\tlearningLoss: 0.008719\twhole_loss: 0.044536 \n",
            "Train Epoch: 159 [400/486 (82%)]\tlearningLoss: 0.008767\twhole_loss: 0.008352 \n",
            "Train Epoch: 159 [416/486 (86%)]\tlearningLoss: 0.008952\twhole_loss: 0.032468 \n",
            "Train Epoch: 159 [432/486 (89%)]\tlearningLoss: 0.008969\twhole_loss: 0.002968 \n",
            "Train Epoch: 159 [448/486 (92%)]\tlearningLoss: 0.008972\twhole_loss: 0.000468 \n",
            "Train Epoch: 159 [464/486 (95%)]\tlearningLoss: 0.009126\twhole_loss: 0.026997 \n",
            "Train Epoch: 159 [180/486 (37%)]\tlearningLoss: 0.009129\twhole_loss: 0.000441 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.0921, Accuracy: 69/75(92%)\n",
            "\n",
            "Train Epoch: 160 [0/486 (0%)]\tlearningLoss: 0.001902\twhole_loss: 0.332796 \n",
            "Train Epoch: 160 [16/486 (3%)]\tlearningLoss: 0.001932\twhole_loss: 0.005237 \n",
            "Train Epoch: 160 [32/486 (7%)]\tlearningLoss: 0.001966\twhole_loss: 0.006061 \n",
            "Train Epoch: 160 [48/486 (10%)]\tlearningLoss: 0.003676\twhole_loss: 0.299119 \n",
            "Train Epoch: 160 [64/486 (13%)]\tlearningLoss: 0.003811\twhole_loss: 0.023715 \n",
            "Train Epoch: 160 [80/486 (16%)]\tlearningLoss: 0.004386\twhole_loss: 0.100574 \n",
            "Train Epoch: 160 [96/486 (20%)]\tlearningLoss: 0.004412\twhole_loss: 0.004589 \n",
            "Train Epoch: 160 [112/486 (23%)]\tlearningLoss: 0.004460\twhole_loss: 0.008422 \n",
            "Train Epoch: 160 [128/486 (26%)]\tlearningLoss: 0.004484\twhole_loss: 0.004207 \n",
            "Train Epoch: 160 [144/486 (30%)]\tlearningLoss: 0.005138\twhole_loss: 0.114424 \n",
            "Train Epoch: 160 [160/486 (33%)]\tlearningLoss: 0.006983\twhole_loss: 0.322904 \n",
            "Train Epoch: 160 [176/486 (36%)]\tlearningLoss: 0.006984\twhole_loss: 0.000207 \n",
            "Train Epoch: 160 [192/486 (40%)]\tlearningLoss: 0.007466\twhole_loss: 0.084376 \n",
            "Train Epoch: 160 [208/486 (43%)]\tlearningLoss: 0.007595\twhole_loss: 0.022562 \n",
            "Train Epoch: 160 [224/486 (46%)]\tlearningLoss: 0.007636\twhole_loss: 0.007106 \n",
            "Train Epoch: 160 [240/486 (49%)]\tlearningLoss: 0.007743\twhole_loss: 0.018699 \n",
            "Train Epoch: 160 [256/486 (53%)]\tlearningLoss: 0.010480\twhole_loss: 0.479060 \n",
            "Train Epoch: 160 [272/486 (56%)]\tlearningLoss: 0.010543\twhole_loss: 0.010971 \n",
            "Train Epoch: 160 [288/486 (59%)]\tlearningLoss: 0.010666\twhole_loss: 0.021457 \n",
            "Train Epoch: 160 [304/486 (63%)]\tlearningLoss: 0.011009\twhole_loss: 0.060079 \n",
            "Train Epoch: 160 [320/486 (66%)]\tlearningLoss: 0.012567\twhole_loss: 0.272622 \n",
            "Train Epoch: 160 [336/486 (69%)]\tlearningLoss: 0.013924\twhole_loss: 0.237578 \n",
            "Train Epoch: 160 [352/486 (72%)]\tlearningLoss: 0.015163\twhole_loss: 0.216732 \n",
            "Train Epoch: 160 [368/486 (76%)]\tlearningLoss: 0.015180\twhole_loss: 0.003028 \n",
            "Train Epoch: 160 [384/486 (79%)]\tlearningLoss: 0.015480\twhole_loss: 0.052481 \n",
            "Train Epoch: 160 [400/486 (82%)]\tlearningLoss: 0.015579\twhole_loss: 0.017353 \n",
            "Train Epoch: 160 [416/486 (86%)]\tlearningLoss: 0.015581\twhole_loss: 0.000398 \n",
            "Train Epoch: 160 [432/486 (89%)]\tlearningLoss: 0.015719\twhole_loss: 0.023998 \n",
            "Train Epoch: 160 [448/486 (92%)]\tlearningLoss: 0.015725\twhole_loss: 0.001181 \n",
            "Train Epoch: 160 [464/486 (95%)]\tlearningLoss: 0.015938\twhole_loss: 0.037178 \n",
            "Train Epoch: 160 [180/486 (37%)]\tlearningLoss: 0.015938\twhole_loss: 0.000028 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.1653, Accuracy: 66/75(88%)\n",
            "\n",
            "Train Epoch: 161 [0/486 (0%)]\tlearningLoss: 0.000564\twhole_loss: 0.098624 \n",
            "Train Epoch: 161 [16/486 (3%)]\tlearningLoss: 0.000579\twhole_loss: 0.002783 \n",
            "Train Epoch: 161 [32/486 (7%)]\tlearningLoss: 0.000581\twhole_loss: 0.000314 \n",
            "Train Epoch: 161 [48/486 (10%)]\tlearningLoss: 0.001127\twhole_loss: 0.095567 \n",
            "Train Epoch: 161 [64/486 (13%)]\tlearningLoss: 0.001231\twhole_loss: 0.018194 \n",
            "Train Epoch: 161 [80/486 (16%)]\tlearningLoss: 0.001266\twhole_loss: 0.006051 \n",
            "Train Epoch: 161 [96/486 (20%)]\tlearningLoss: 0.002386\twhole_loss: 0.195943 \n",
            "Train Epoch: 161 [112/486 (23%)]\tlearningLoss: 0.002505\twhole_loss: 0.020979 \n",
            "Train Epoch: 161 [128/486 (26%)]\tlearningLoss: 0.002616\twhole_loss: 0.019296 \n",
            "Train Epoch: 161 [144/486 (30%)]\tlearningLoss: 0.005511\twhole_loss: 0.506748 \n",
            "Train Epoch: 161 [160/486 (33%)]\tlearningLoss: 0.006912\twhole_loss: 0.245062 \n",
            "Train Epoch: 161 [176/486 (36%)]\tlearningLoss: 0.006940\twhole_loss: 0.004918 \n",
            "Train Epoch: 161 [192/486 (40%)]\tlearningLoss: 0.006959\twhole_loss: 0.003314 \n",
            "Train Epoch: 161 [208/486 (43%)]\tlearningLoss: 0.007098\twhole_loss: 0.024299 \n",
            "Train Epoch: 161 [224/486 (46%)]\tlearningLoss: 0.008761\twhole_loss: 0.291080 \n",
            "Train Epoch: 161 [240/486 (49%)]\tlearningLoss: 0.011004\twhole_loss: 0.392601 \n",
            "Train Epoch: 161 [256/486 (53%)]\tlearningLoss: 0.011994\twhole_loss: 0.173177 \n",
            "Train Epoch: 161 [272/486 (56%)]\tlearningLoss: 0.013324\twhole_loss: 0.232768 \n",
            "Train Epoch: 161 [288/486 (59%)]\tlearningLoss: 0.013860\twhole_loss: 0.093759 \n",
            "Train Epoch: 161 [304/486 (63%)]\tlearningLoss: 0.013890\twhole_loss: 0.005195 \n",
            "Train Epoch: 161 [320/486 (66%)]\tlearningLoss: 0.015037\twhole_loss: 0.200734 \n",
            "Train Epoch: 161 [336/486 (69%)]\tlearningLoss: 0.015114\twhole_loss: 0.013472 \n",
            "Train Epoch: 161 [352/486 (72%)]\tlearningLoss: 0.015193\twhole_loss: 0.013891 \n",
            "Train Epoch: 161 [368/486 (76%)]\tlearningLoss: 0.015239\twhole_loss: 0.008103 \n",
            "Train Epoch: 161 [384/486 (79%)]\tlearningLoss: 0.016266\twhole_loss: 0.179763 \n",
            "Train Epoch: 161 [400/486 (82%)]\tlearningLoss: 0.017489\twhole_loss: 0.213932 \n",
            "Train Epoch: 161 [416/486 (86%)]\tlearningLoss: 0.018815\twhole_loss: 0.232070 \n",
            "Train Epoch: 161 [432/486 (89%)]\tlearningLoss: 0.020806\twhole_loss: 0.348438 \n",
            "Train Epoch: 161 [448/486 (92%)]\tlearningLoss: 0.020836\twhole_loss: 0.005246 \n",
            "Train Epoch: 161 [464/486 (95%)]\tlearningLoss: 0.021238\twhole_loss: 0.070319 \n",
            "Train Epoch: 161 [180/486 (37%)]\tlearningLoss: 0.021320\twhole_loss: 0.014358 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 2], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.6878, Accuracy: 67/75(89%)\n",
            "\n",
            "Train Epoch: 162 [0/486 (0%)]\tlearningLoss: 0.001116\twhole_loss: 0.195231 \n",
            "Train Epoch: 162 [16/486 (3%)]\tlearningLoss: 0.001217\twhole_loss: 0.017705 \n",
            "Train Epoch: 162 [32/486 (7%)]\tlearningLoss: 0.002530\twhole_loss: 0.229866 \n",
            "Train Epoch: 162 [48/486 (10%)]\tlearningLoss: 0.002806\twhole_loss: 0.048256 \n",
            "Train Epoch: 162 [64/486 (13%)]\tlearningLoss: 0.002974\twhole_loss: 0.029406 \n",
            "Train Epoch: 162 [80/486 (16%)]\tlearningLoss: 0.004745\twhole_loss: 0.309933 \n",
            "Train Epoch: 162 [96/486 (20%)]\tlearningLoss: 0.004750\twhole_loss: 0.000931 \n",
            "Train Epoch: 162 [112/486 (23%)]\tlearningLoss: 0.005544\twhole_loss: 0.138824 \n",
            "Train Epoch: 162 [128/486 (26%)]\tlearningLoss: 0.005763\twhole_loss: 0.038439 \n",
            "Train Epoch: 162 [144/486 (30%)]\tlearningLoss: 0.009251\twhole_loss: 0.610395 \n",
            "Train Epoch: 162 [160/486 (33%)]\tlearningLoss: 0.009491\twhole_loss: 0.041909 \n",
            "Train Epoch: 162 [176/486 (36%)]\tlearningLoss: 0.009657\twhole_loss: 0.029116 \n",
            "Train Epoch: 162 [192/486 (40%)]\tlearningLoss: 0.009739\twhole_loss: 0.014339 \n",
            "Train Epoch: 162 [208/486 (43%)]\tlearningLoss: 0.009872\twhole_loss: 0.023223 \n",
            "Train Epoch: 162 [224/486 (46%)]\tlearningLoss: 0.010540\twhole_loss: 0.116866 \n",
            "Train Epoch: 162 [240/486 (49%)]\tlearningLoss: 0.012665\twhole_loss: 0.371898 \n",
            "Train Epoch: 162 [256/486 (53%)]\tlearningLoss: 0.012940\twhole_loss: 0.048080 \n",
            "Train Epoch: 162 [272/486 (56%)]\tlearningLoss: 0.013777\twhole_loss: 0.146572 \n",
            "Train Epoch: 162 [288/486 (59%)]\tlearningLoss: 0.013950\twhole_loss: 0.030336 \n",
            "Train Epoch: 162 [304/486 (63%)]\tlearningLoss: 0.015104\twhole_loss: 0.201941 \n",
            "Train Epoch: 162 [320/486 (66%)]\tlearningLoss: 0.017426\twhole_loss: 0.406244 \n",
            "Train Epoch: 162 [336/486 (69%)]\tlearningLoss: 0.017703\twhole_loss: 0.048482 \n",
            "Train Epoch: 162 [352/486 (72%)]\tlearningLoss: 0.017801\twhole_loss: 0.017148 \n",
            "Train Epoch: 162 [368/486 (76%)]\tlearningLoss: 0.017961\twhole_loss: 0.028006 \n",
            "Train Epoch: 162 [384/486 (79%)]\tlearningLoss: 0.018004\twhole_loss: 0.007533 \n",
            "Train Epoch: 162 [400/486 (82%)]\tlearningLoss: 0.018888\twhole_loss: 0.154780 \n",
            "Train Epoch: 162 [416/486 (86%)]\tlearningLoss: 0.019467\twhole_loss: 0.101320 \n",
            "Train Epoch: 162 [432/486 (89%)]\tlearningLoss: 0.019669\twhole_loss: 0.035318 \n",
            "Train Epoch: 162 [448/486 (92%)]\tlearningLoss: 0.020070\twhole_loss: 0.070166 \n",
            "Train Epoch: 162 [464/486 (95%)]\tlearningLoss: 0.020412\twhole_loss: 0.059869 \n",
            "Train Epoch: 162 [180/486 (37%)]\tlearningLoss: 0.020413\twhole_loss: 0.000077 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 0, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.9834, Accuracy: 67/75(89%)\n",
            "\n",
            "Train Epoch: 163 [0/486 (0%)]\tlearningLoss: 0.000300\twhole_loss: 0.052445 \n",
            "Train Epoch: 163 [16/486 (3%)]\tlearningLoss: 0.000309\twhole_loss: 0.001603 \n",
            "Train Epoch: 163 [32/486 (7%)]\tlearningLoss: 0.001577\twhole_loss: 0.221951 \n",
            "Train Epoch: 163 [48/486 (10%)]\tlearningLoss: 0.001588\twhole_loss: 0.001849 \n",
            "Train Epoch: 163 [64/486 (13%)]\tlearningLoss: 0.001784\twhole_loss: 0.034300 \n",
            "Train Epoch: 163 [80/486 (16%)]\tlearningLoss: 0.001840\twhole_loss: 0.009891 \n",
            "Train Epoch: 163 [96/486 (20%)]\tlearningLoss: 0.001989\twhole_loss: 0.026058 \n",
            "Train Epoch: 163 [112/486 (23%)]\tlearningLoss: 0.002113\twhole_loss: 0.021625 \n",
            "Train Epoch: 163 [128/486 (26%)]\tlearningLoss: 0.003179\twhole_loss: 0.186661 \n",
            "Train Epoch: 163 [144/486 (30%)]\tlearningLoss: 0.004458\twhole_loss: 0.223809 \n",
            "Train Epoch: 163 [160/486 (33%)]\tlearningLoss: 0.004463\twhole_loss: 0.000747 \n",
            "Train Epoch: 163 [176/486 (36%)]\tlearningLoss: 0.004790\twhole_loss: 0.057298 \n",
            "Train Epoch: 163 [192/486 (40%)]\tlearningLoss: 0.004857\twhole_loss: 0.011789 \n",
            "Train Epoch: 163 [208/486 (43%)]\tlearningLoss: 0.005050\twhole_loss: 0.033783 \n",
            "Train Epoch: 163 [224/486 (46%)]\tlearningLoss: 0.005114\twhole_loss: 0.011197 \n",
            "Train Epoch: 163 [240/486 (49%)]\tlearningLoss: 0.005418\twhole_loss: 0.053065 \n",
            "Train Epoch: 163 [256/486 (53%)]\tlearningLoss: 0.005871\twhole_loss: 0.079315 \n",
            "Train Epoch: 163 [272/486 (56%)]\tlearningLoss: 0.006586\twhole_loss: 0.125187 \n",
            "Train Epoch: 163 [288/486 (59%)]\tlearningLoss: 0.006896\twhole_loss: 0.054283 \n",
            "Train Epoch: 163 [304/486 (63%)]\tlearningLoss: 0.007775\twhole_loss: 0.153722 \n",
            "Train Epoch: 163 [320/486 (66%)]\tlearningLoss: 0.007872\twhole_loss: 0.017030 \n",
            "Train Epoch: 163 [336/486 (69%)]\tlearningLoss: 0.008080\twhole_loss: 0.036325 \n",
            "Train Epoch: 163 [352/486 (72%)]\tlearningLoss: 0.008756\twhole_loss: 0.118380 \n",
            "Train Epoch: 163 [368/486 (76%)]\tlearningLoss: 0.008798\twhole_loss: 0.007395 \n",
            "Train Epoch: 163 [384/486 (79%)]\tlearningLoss: 0.009155\twhole_loss: 0.062370 \n",
            "Train Epoch: 163 [400/486 (82%)]\tlearningLoss: 0.009573\twhole_loss: 0.073184 \n",
            "Train Epoch: 163 [416/486 (86%)]\tlearningLoss: 0.010972\twhole_loss: 0.244765 \n",
            "Train Epoch: 163 [432/486 (89%)]\tlearningLoss: 0.011337\twhole_loss: 0.063971 \n",
            "Train Epoch: 163 [448/486 (92%)]\tlearningLoss: 0.011382\twhole_loss: 0.007849 \n",
            "Train Epoch: 163 [464/486 (95%)]\tlearningLoss: 0.011555\twhole_loss: 0.030295 \n",
            "Train Epoch: 163 [180/486 (37%)]\tlearningLoss: 0.011595\twhole_loss: 0.006953 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.7336, Accuracy: 69/75(92%)\n",
            "\n",
            "Train Epoch: 164 [0/486 (0%)]\tlearningLoss: 0.000076\twhole_loss: 0.013220 \n",
            "Train Epoch: 164 [16/486 (3%)]\tlearningLoss: 0.000803\twhole_loss: 0.127350 \n",
            "Train Epoch: 164 [32/486 (7%)]\tlearningLoss: 0.001225\twhole_loss: 0.073798 \n",
            "Train Epoch: 164 [48/486 (10%)]\tlearningLoss: 0.001307\twhole_loss: 0.014318 \n",
            "Train Epoch: 164 [64/486 (13%)]\tlearningLoss: 0.001541\twhole_loss: 0.041003 \n",
            "Train Epoch: 164 [80/486 (16%)]\tlearningLoss: 0.001642\twhole_loss: 0.017739 \n",
            "Train Epoch: 164 [96/486 (20%)]\tlearningLoss: 0.001696\twhole_loss: 0.009447 \n",
            "Train Epoch: 164 [112/486 (23%)]\tlearningLoss: 0.001698\twhole_loss: 0.000202 \n",
            "Train Epoch: 164 [128/486 (26%)]\tlearningLoss: 0.001940\twhole_loss: 0.042438 \n",
            "Train Epoch: 164 [144/486 (30%)]\tlearningLoss: 0.002001\twhole_loss: 0.010687 \n",
            "Train Epoch: 164 [160/486 (33%)]\tlearningLoss: 0.002085\twhole_loss: 0.014755 \n",
            "Train Epoch: 164 [176/486 (36%)]\tlearningLoss: 0.002113\twhole_loss: 0.004736 \n",
            "Train Epoch: 164 [192/486 (40%)]\tlearningLoss: 0.002148\twhole_loss: 0.006194 \n",
            "Train Epoch: 164 [208/486 (43%)]\tlearningLoss: 0.002205\twhole_loss: 0.009998 \n",
            "Train Epoch: 164 [224/486 (46%)]\tlearningLoss: 0.003367\twhole_loss: 0.203315 \n",
            "Train Epoch: 164 [240/486 (49%)]\tlearningLoss: 0.003427\twhole_loss: 0.010483 \n",
            "Train Epoch: 164 [256/486 (53%)]\tlearningLoss: 0.003482\twhole_loss: 0.009648 \n",
            "Train Epoch: 164 [272/486 (56%)]\tlearningLoss: 0.003878\twhole_loss: 0.069237 \n",
            "Train Epoch: 164 [288/486 (59%)]\tlearningLoss: 0.004429\twhole_loss: 0.096481 \n",
            "Train Epoch: 164 [304/486 (63%)]\tlearningLoss: 0.004436\twhole_loss: 0.001196 \n",
            "Train Epoch: 164 [320/486 (66%)]\tlearningLoss: 0.004678\twhole_loss: 0.042379 \n",
            "Train Epoch: 164 [336/486 (69%)]\tlearningLoss: 0.004811\twhole_loss: 0.023215 \n",
            "Train Epoch: 164 [352/486 (72%)]\tlearningLoss: 0.004896\twhole_loss: 0.014992 \n",
            "Train Epoch: 164 [368/486 (76%)]\tlearningLoss: 0.004984\twhole_loss: 0.015302 \n",
            "Train Epoch: 164 [384/486 (79%)]\tlearningLoss: 0.005048\twhole_loss: 0.011187 \n",
            "Train Epoch: 164 [400/486 (82%)]\tlearningLoss: 0.005049\twhole_loss: 0.000341 \n",
            "Train Epoch: 164 [416/486 (86%)]\tlearningLoss: 0.005107\twhole_loss: 0.010054 \n",
            "Train Epoch: 164 [432/486 (89%)]\tlearningLoss: 0.005255\twhole_loss: 0.025888 \n",
            "Train Epoch: 164 [448/486 (92%)]\tlearningLoss: 0.005293\twhole_loss: 0.006587 \n",
            "Train Epoch: 164 [464/486 (95%)]\tlearningLoss: 0.005612\twhole_loss: 0.055947 \n",
            "Train Epoch: 164 [180/486 (37%)]\tlearningLoss: 0.005630\twhole_loss: 0.003116 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 2, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.0316, Accuracy: 67/75(89%)\n",
            "\n",
            "Train Epoch: 165 [0/486 (0%)]\tlearningLoss: 0.000017\twhole_loss: 0.002946 \n",
            "Train Epoch: 165 [16/486 (3%)]\tlearningLoss: 0.000036\twhole_loss: 0.003335 \n",
            "Train Epoch: 165 [32/486 (7%)]\tlearningLoss: 0.000088\twhole_loss: 0.009088 \n",
            "Train Epoch: 165 [48/486 (10%)]\tlearningLoss: 0.000106\twhole_loss: 0.003248 \n",
            "Train Epoch: 165 [64/486 (13%)]\tlearningLoss: 0.000168\twhole_loss: 0.010759 \n",
            "Train Epoch: 165 [80/486 (16%)]\tlearningLoss: 0.000169\twhole_loss: 0.000259 \n",
            "Train Epoch: 165 [96/486 (20%)]\tlearningLoss: 0.000223\twhole_loss: 0.009393 \n",
            "Train Epoch: 165 [112/486 (23%)]\tlearningLoss: 0.000441\twhole_loss: 0.038102 \n",
            "Train Epoch: 165 [128/486 (26%)]\tlearningLoss: 0.000472\twhole_loss: 0.005502 \n",
            "Train Epoch: 165 [144/486 (30%)]\tlearningLoss: 0.000477\twhole_loss: 0.000876 \n",
            "Train Epoch: 165 [160/486 (33%)]\tlearningLoss: 0.000911\twhole_loss: 0.075873 \n",
            "Train Epoch: 165 [176/486 (36%)]\tlearningLoss: 0.001266\twhole_loss: 0.062242 \n",
            "Train Epoch: 165 [192/486 (40%)]\tlearningLoss: 0.001542\twhole_loss: 0.048250 \n",
            "Train Epoch: 165 [208/486 (43%)]\tlearningLoss: 0.001578\twhole_loss: 0.006303 \n",
            "Train Epoch: 165 [224/486 (46%)]\tlearningLoss: 0.002093\twhole_loss: 0.090160 \n",
            "Train Epoch: 165 [240/486 (49%)]\tlearningLoss: 0.002132\twhole_loss: 0.006827 \n",
            "Train Epoch: 165 [256/486 (53%)]\tlearningLoss: 0.002134\twhole_loss: 0.000371 \n",
            "Train Epoch: 165 [272/486 (56%)]\tlearningLoss: 0.002144\twhole_loss: 0.001640 \n",
            "Train Epoch: 165 [288/486 (59%)]\tlearningLoss: 0.002221\twhole_loss: 0.013417 \n",
            "Train Epoch: 165 [304/486 (63%)]\tlearningLoss: 0.002384\twhole_loss: 0.028571 \n",
            "Train Epoch: 165 [320/486 (66%)]\tlearningLoss: 0.002386\twhole_loss: 0.000422 \n",
            "Train Epoch: 165 [336/486 (69%)]\tlearningLoss: 0.002405\twhole_loss: 0.003227 \n",
            "Train Epoch: 165 [352/486 (72%)]\tlearningLoss: 0.002514\twhole_loss: 0.019210 \n",
            "Train Epoch: 165 [368/486 (76%)]\tlearningLoss: 0.002556\twhole_loss: 0.007196 \n",
            "Train Epoch: 165 [384/486 (79%)]\tlearningLoss: 0.002565\twhole_loss: 0.001653 \n",
            "Train Epoch: 165 [400/486 (82%)]\tlearningLoss: 0.002610\twhole_loss: 0.007904 \n",
            "Train Epoch: 165 [416/486 (86%)]\tlearningLoss: 0.002623\twhole_loss: 0.002183 \n",
            "Train Epoch: 165 [432/486 (89%)]\tlearningLoss: 0.002626\twhole_loss: 0.000625 \n",
            "Train Epoch: 165 [448/486 (92%)]\tlearningLoss: 0.002628\twhole_loss: 0.000381 \n",
            "Train Epoch: 165 [464/486 (95%)]\tlearningLoss: 0.005087\twhole_loss: 0.430202 \n",
            "Train Epoch: 165 [180/486 (37%)]\tlearningLoss: 0.005110\twhole_loss: 0.004107 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.6305, Accuracy: 68/75(91%)\n",
            "\n",
            "Train Epoch: 166 [0/486 (0%)]\tlearningLoss: 0.000148\twhole_loss: 0.025859 \n",
            "Train Epoch: 166 [16/486 (3%)]\tlearningLoss: 0.000180\twhole_loss: 0.005724 \n",
            "Train Epoch: 166 [32/486 (7%)]\tlearningLoss: 0.000242\twhole_loss: 0.010723 \n",
            "Train Epoch: 166 [48/486 (10%)]\tlearningLoss: 0.000293\twhole_loss: 0.008959 \n",
            "Train Epoch: 166 [64/486 (13%)]\tlearningLoss: 0.000299\twhole_loss: 0.001041 \n",
            "Train Epoch: 166 [80/486 (16%)]\tlearningLoss: 0.000494\twhole_loss: 0.034123 \n",
            "Train Epoch: 166 [96/486 (20%)]\tlearningLoss: 0.000973\twhole_loss: 0.083872 \n",
            "Train Epoch: 166 [112/486 (23%)]\tlearningLoss: 0.000994\twhole_loss: 0.003708 \n",
            "Train Epoch: 166 [128/486 (26%)]\tlearningLoss: 0.001024\twhole_loss: 0.005198 \n",
            "Train Epoch: 166 [144/486 (30%)]\tlearningLoss: 0.001145\twhole_loss: 0.021164 \n",
            "Train Epoch: 166 [160/486 (33%)]\tlearningLoss: 0.001340\twhole_loss: 0.034151 \n",
            "Train Epoch: 166 [176/486 (36%)]\tlearningLoss: 0.001365\twhole_loss: 0.004328 \n",
            "Train Epoch: 166 [192/486 (40%)]\tlearningLoss: 0.001485\twhole_loss: 0.020965 \n",
            "Train Epoch: 166 [208/486 (43%)]\tlearningLoss: 0.001654\twhole_loss: 0.029684 \n",
            "Train Epoch: 166 [224/486 (46%)]\tlearningLoss: 0.001717\twhole_loss: 0.010945 \n",
            "Train Epoch: 166 [240/486 (49%)]\tlearningLoss: 0.001723\twhole_loss: 0.001161 \n",
            "Train Epoch: 166 [256/486 (53%)]\tlearningLoss: 0.003581\twhole_loss: 0.325058 \n",
            "Train Epoch: 166 [272/486 (56%)]\tlearningLoss: 0.003594\twhole_loss: 0.002260 \n",
            "Train Epoch: 166 [288/486 (59%)]\tlearningLoss: 0.003635\twhole_loss: 0.007193 \n",
            "Train Epoch: 166 [304/486 (63%)]\tlearningLoss: 0.004122\twhole_loss: 0.085249 \n",
            "Train Epoch: 166 [320/486 (66%)]\tlearningLoss: 0.004125\twhole_loss: 0.000569 \n",
            "Train Epoch: 166 [336/486 (69%)]\tlearningLoss: 0.004210\twhole_loss: 0.014811 \n",
            "Train Epoch: 166 [352/486 (72%)]\tlearningLoss: 0.004244\twhole_loss: 0.006007 \n",
            "Train Epoch: 166 [368/486 (76%)]\tlearningLoss: 0.004272\twhole_loss: 0.004914 \n",
            "Train Epoch: 166 [384/486 (79%)]\tlearningLoss: 0.004475\twhole_loss: 0.035463 \n",
            "Train Epoch: 166 [400/486 (82%)]\tlearningLoss: 0.004489\twhole_loss: 0.002370 \n",
            "Train Epoch: 166 [416/486 (86%)]\tlearningLoss: 0.004491\twhole_loss: 0.000397 \n",
            "Train Epoch: 166 [432/486 (89%)]\tlearningLoss: 0.004500\twhole_loss: 0.001521 \n",
            "Train Epoch: 166 [448/486 (92%)]\tlearningLoss: 0.004588\twhole_loss: 0.015417 \n",
            "Train Epoch: 166 [464/486 (95%)]\tlearningLoss: 0.004842\twhole_loss: 0.044550 \n",
            "Train Epoch: 166 [180/486 (37%)]\tlearningLoss: 0.004875\twhole_loss: 0.005762 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 2, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.4486, Accuracy: 69/75(92%)\n",
            "\n",
            "Train Epoch: 167 [0/486 (0%)]\tlearningLoss: 0.000000\twhole_loss: 0.000018 \n",
            "Train Epoch: 167 [16/486 (3%)]\tlearningLoss: 0.000054\twhole_loss: 0.009463 \n",
            "Train Epoch: 167 [32/486 (7%)]\tlearningLoss: 0.000388\twhole_loss: 0.058395 \n",
            "Train Epoch: 167 [48/486 (10%)]\tlearningLoss: 0.000572\twhole_loss: 0.032144 \n",
            "Train Epoch: 167 [64/486 (13%)]\tlearningLoss: 0.000576\twhole_loss: 0.000788 \n",
            "Train Epoch: 167 [80/486 (16%)]\tlearningLoss: 0.001545\twhole_loss: 0.169493 \n",
            "Train Epoch: 167 [96/486 (20%)]\tlearningLoss: 0.001563\twhole_loss: 0.003183 \n",
            "Train Epoch: 167 [112/486 (23%)]\tlearningLoss: 0.001582\twhole_loss: 0.003452 \n",
            "Train Epoch: 167 [128/486 (26%)]\tlearningLoss: 0.001652\twhole_loss: 0.012157 \n",
            "Train Epoch: 167 [144/486 (30%)]\tlearningLoss: 0.001902\twhole_loss: 0.043703 \n",
            "Train Epoch: 167 [160/486 (33%)]\tlearningLoss: 0.001906\twhole_loss: 0.000841 \n",
            "Train Epoch: 167 [176/486 (36%)]\tlearningLoss: 0.001916\twhole_loss: 0.001585 \n",
            "Train Epoch: 167 [192/486 (40%)]\tlearningLoss: 0.001931\twhole_loss: 0.002742 \n",
            "Train Epoch: 167 [208/486 (43%)]\tlearningLoss: 0.001946\twhole_loss: 0.002562 \n",
            "Train Epoch: 167 [224/486 (46%)]\tlearningLoss: 0.001956\twhole_loss: 0.001763 \n",
            "Train Epoch: 167 [240/486 (49%)]\tlearningLoss: 0.002281\twhole_loss: 0.056847 \n",
            "Train Epoch: 167 [256/486 (53%)]\tlearningLoss: 0.002285\twhole_loss: 0.000776 \n",
            "Train Epoch: 167 [272/486 (56%)]\tlearningLoss: 0.002297\twhole_loss: 0.002018 \n",
            "Train Epoch: 167 [288/486 (59%)]\tlearningLoss: 0.002383\twhole_loss: 0.015093 \n",
            "Train Epoch: 167 [304/486 (63%)]\tlearningLoss: 0.002393\twhole_loss: 0.001694 \n",
            "Train Epoch: 167 [320/486 (66%)]\tlearningLoss: 0.002487\twhole_loss: 0.016585 \n",
            "Train Epoch: 167 [336/486 (69%)]\tlearningLoss: 0.002488\twhole_loss: 0.000022 \n",
            "Train Epoch: 167 [352/486 (72%)]\tlearningLoss: 0.002624\twhole_loss: 0.023904 \n",
            "Train Epoch: 167 [368/486 (76%)]\tlearningLoss: 0.002627\twhole_loss: 0.000413 \n",
            "Train Epoch: 167 [384/486 (79%)]\tlearningLoss: 0.002664\twhole_loss: 0.006517 \n",
            "Train Epoch: 167 [400/486 (82%)]\tlearningLoss: 0.002673\twhole_loss: 0.001551 \n",
            "Train Epoch: 167 [416/486 (86%)]\tlearningLoss: 0.002693\twhole_loss: 0.003485 \n",
            "Train Epoch: 167 [432/486 (89%)]\tlearningLoss: 0.002693\twhole_loss: 0.000155 \n",
            "Train Epoch: 167 [448/486 (92%)]\tlearningLoss: 0.002706\twhole_loss: 0.002158 \n",
            "Train Epoch: 167 [464/486 (95%)]\tlearningLoss: 0.002706\twhole_loss: 0.000033 \n",
            "Train Epoch: 167 [180/486 (37%)]\tlearningLoss: 0.002706\twhole_loss: 0.000000 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.5707, Accuracy: 68/75(91%)\n",
            "\n",
            "Train Epoch: 168 [0/486 (0%)]\tlearningLoss: 0.000004\twhole_loss: 0.000722 \n",
            "Train Epoch: 168 [16/486 (3%)]\tlearningLoss: 0.000013\twhole_loss: 0.001637 \n",
            "Train Epoch: 168 [32/486 (7%)]\tlearningLoss: 0.000014\twhole_loss: 0.000171 \n",
            "Train Epoch: 168 [48/486 (10%)]\tlearningLoss: 0.000015\twhole_loss: 0.000052 \n",
            "Train Epoch: 168 [64/486 (13%)]\tlearningLoss: 0.000992\twhole_loss: 0.170974 \n",
            "Train Epoch: 168 [80/486 (16%)]\tlearningLoss: 0.001042\twhole_loss: 0.008764 \n",
            "Train Epoch: 168 [96/486 (20%)]\tlearningLoss: 0.001061\twhole_loss: 0.003435 \n",
            "Train Epoch: 168 [112/486 (23%)]\tlearningLoss: 0.001096\twhole_loss: 0.006104 \n",
            "Train Epoch: 168 [128/486 (26%)]\tlearningLoss: 0.001151\twhole_loss: 0.009526 \n",
            "Train Epoch: 168 [144/486 (30%)]\tlearningLoss: 0.001157\twhole_loss: 0.001025 \n",
            "Train Epoch: 168 [160/486 (33%)]\tlearningLoss: 0.001190\twhole_loss: 0.005840 \n",
            "Train Epoch: 168 [176/486 (36%)]\tlearningLoss: 0.001198\twhole_loss: 0.001353 \n",
            "Train Epoch: 168 [192/486 (40%)]\tlearningLoss: 0.001288\twhole_loss: 0.015760 \n",
            "Train Epoch: 168 [208/486 (43%)]\tlearningLoss: 0.001558\twhole_loss: 0.047286 \n",
            "Train Epoch: 168 [224/486 (46%)]\tlearningLoss: 0.001644\twhole_loss: 0.015087 \n",
            "Train Epoch: 168 [240/486 (49%)]\tlearningLoss: 0.001645\twhole_loss: 0.000096 \n",
            "Train Epoch: 168 [256/486 (53%)]\tlearningLoss: 0.001657\twhole_loss: 0.002106 \n",
            "Train Epoch: 168 [272/486 (56%)]\tlearningLoss: 0.001675\twhole_loss: 0.003245 \n",
            "Train Epoch: 168 [288/486 (59%)]\tlearningLoss: 0.001727\twhole_loss: 0.008973 \n",
            "Train Epoch: 168 [304/486 (63%)]\tlearningLoss: 0.001728\twhole_loss: 0.000199 \n",
            "Train Epoch: 168 [320/486 (66%)]\tlearningLoss: 0.001900\twhole_loss: 0.030162 \n",
            "Train Epoch: 168 [336/486 (69%)]\tlearningLoss: 0.001949\twhole_loss: 0.008471 \n",
            "Train Epoch: 168 [352/486 (72%)]\tlearningLoss: 0.001950\twhole_loss: 0.000340 \n",
            "Train Epoch: 168 [368/486 (76%)]\tlearningLoss: 0.001952\twhole_loss: 0.000316 \n",
            "Train Epoch: 168 [384/486 (79%)]\tlearningLoss: 0.001952\twhole_loss: 0.000006 \n",
            "Train Epoch: 168 [400/486 (82%)]\tlearningLoss: 0.001984\twhole_loss: 0.005567 \n",
            "Train Epoch: 168 [416/486 (86%)]\tlearningLoss: 0.002010\twhole_loss: 0.004608 \n",
            "Train Epoch: 168 [432/486 (89%)]\tlearningLoss: 0.002202\twhole_loss: 0.033534 \n",
            "Train Epoch: 168 [448/486 (92%)]\tlearningLoss: 0.002216\twhole_loss: 0.002418 \n",
            "Train Epoch: 168 [464/486 (95%)]\tlearningLoss: 0.002371\twhole_loss: 0.027112 \n",
            "Train Epoch: 168 [180/486 (37%)]\tlearningLoss: 0.002372\twhole_loss: 0.000255 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.0150, Accuracy: 68/75(91%)\n",
            "\n",
            "Train Epoch: 169 [0/486 (0%)]\tlearningLoss: 0.000073\twhole_loss: 0.012739 \n",
            "Train Epoch: 169 [16/486 (3%)]\tlearningLoss: 0.000084\twhole_loss: 0.002038 \n",
            "Train Epoch: 169 [32/486 (7%)]\tlearningLoss: 0.000114\twhole_loss: 0.005174 \n",
            "Train Epoch: 169 [48/486 (10%)]\tlearningLoss: 0.000125\twhole_loss: 0.002003 \n",
            "Train Epoch: 169 [64/486 (13%)]\tlearningLoss: 0.000144\twhole_loss: 0.003223 \n",
            "Train Epoch: 169 [80/486 (16%)]\tlearningLoss: 0.000145\twhole_loss: 0.000202 \n",
            "Train Epoch: 169 [96/486 (20%)]\tlearningLoss: 0.000148\twhole_loss: 0.000505 \n",
            "Train Epoch: 169 [112/486 (23%)]\tlearningLoss: 0.000202\twhole_loss: 0.009527 \n",
            "Train Epoch: 169 [128/486 (26%)]\tlearningLoss: 0.000208\twhole_loss: 0.000943 \n",
            "Train Epoch: 169 [144/486 (30%)]\tlearningLoss: 0.000220\twhole_loss: 0.002090 \n",
            "Train Epoch: 169 [160/486 (33%)]\tlearningLoss: 0.000222\twhole_loss: 0.000330 \n",
            "Train Epoch: 169 [176/486 (36%)]\tlearningLoss: 0.000226\twhole_loss: 0.000768 \n",
            "Train Epoch: 169 [192/486 (40%)]\tlearningLoss: 0.000386\twhole_loss: 0.028018 \n",
            "Train Epoch: 169 [208/486 (43%)]\tlearningLoss: 0.000389\twhole_loss: 0.000556 \n",
            "Train Epoch: 169 [224/486 (46%)]\tlearningLoss: 0.000481\twhole_loss: 0.016131 \n",
            "Train Epoch: 169 [240/486 (49%)]\tlearningLoss: 0.000497\twhole_loss: 0.002718 \n",
            "Train Epoch: 169 [256/486 (53%)]\tlearningLoss: 0.000500\twhole_loss: 0.000549 \n",
            "Train Epoch: 169 [272/486 (56%)]\tlearningLoss: 0.000509\twhole_loss: 0.001480 \n",
            "Train Epoch: 169 [288/486 (59%)]\tlearningLoss: 0.000510\twhole_loss: 0.000255 \n",
            "Train Epoch: 169 [304/486 (63%)]\tlearningLoss: 0.000584\twhole_loss: 0.012896 \n",
            "Train Epoch: 169 [320/486 (66%)]\tlearningLoss: 0.000585\twhole_loss: 0.000266 \n",
            "Train Epoch: 169 [336/486 (69%)]\tlearningLoss: 0.001040\twhole_loss: 0.079643 \n",
            "Train Epoch: 169 [352/486 (72%)]\tlearningLoss: 0.001068\twhole_loss: 0.004898 \n",
            "Train Epoch: 169 [368/486 (76%)]\tlearningLoss: 0.001068\twhole_loss: 0.000001 \n",
            "Train Epoch: 169 [384/486 (79%)]\tlearningLoss: 0.001070\twhole_loss: 0.000266 \n",
            "Train Epoch: 169 [400/486 (82%)]\tlearningLoss: 0.001258\twhole_loss: 0.033008 \n",
            "Train Epoch: 169 [416/486 (86%)]\tlearningLoss: 0.001273\twhole_loss: 0.002589 \n",
            "Train Epoch: 169 [432/486 (89%)]\tlearningLoss: 0.001283\twhole_loss: 0.001659 \n",
            "Train Epoch: 169 [448/486 (92%)]\tlearningLoss: 0.001315\twhole_loss: 0.005573 \n",
            "Train Epoch: 169 [464/486 (95%)]\tlearningLoss: 0.001370\twhole_loss: 0.009749 \n",
            "Train Epoch: 169 [180/486 (37%)]\tlearningLoss: 0.001372\twhole_loss: 0.000389 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.5918, Accuracy: 68/75(91%)\n",
            "\n",
            "Train Epoch: 170 [0/486 (0%)]\tlearningLoss: 0.000049\twhole_loss: 0.008576 \n",
            "Train Epoch: 170 [16/486 (3%)]\tlearningLoss: 0.000063\twhole_loss: 0.002369 \n",
            "Train Epoch: 170 [32/486 (7%)]\tlearningLoss: 0.000092\twhole_loss: 0.005140 \n",
            "Train Epoch: 170 [48/486 (10%)]\tlearningLoss: 0.000221\twhole_loss: 0.022663 \n",
            "Train Epoch: 170 [64/486 (13%)]\tlearningLoss: 0.000245\twhole_loss: 0.004141 \n",
            "Train Epoch: 170 [80/486 (16%)]\tlearningLoss: 0.000250\twhole_loss: 0.000898 \n",
            "Train Epoch: 170 [96/486 (20%)]\tlearningLoss: 0.000590\twhole_loss: 0.059492 \n",
            "Train Epoch: 170 [112/486 (23%)]\tlearningLoss: 0.000598\twhole_loss: 0.001444 \n",
            "Train Epoch: 170 [128/486 (26%)]\tlearningLoss: 0.000621\twhole_loss: 0.004034 \n",
            "Train Epoch: 170 [144/486 (30%)]\tlearningLoss: 0.000623\twhole_loss: 0.000239 \n",
            "Train Epoch: 170 [160/486 (33%)]\tlearningLoss: 0.000624\twhole_loss: 0.000180 \n",
            "Train Epoch: 170 [176/486 (36%)]\tlearningLoss: 0.000630\twhole_loss: 0.001022 \n",
            "Train Epoch: 170 [192/486 (40%)]\tlearningLoss: 0.000910\twhole_loss: 0.048969 \n",
            "Train Epoch: 170 [208/486 (43%)]\tlearningLoss: 0.000912\twhole_loss: 0.000462 \n",
            "Train Epoch: 170 [224/486 (46%)]\tlearningLoss: 0.000912\twhole_loss: 0.000013 \n",
            "Train Epoch: 170 [240/486 (49%)]\tlearningLoss: 0.000919\twhole_loss: 0.001121 \n",
            "Train Epoch: 170 [256/486 (53%)]\tlearningLoss: 0.000937\twhole_loss: 0.003178 \n",
            "Train Epoch: 170 [272/486 (56%)]\tlearningLoss: 0.000939\twhole_loss: 0.000373 \n",
            "Train Epoch: 170 [288/486 (59%)]\tlearningLoss: 0.000939\twhole_loss: 0.000005 \n",
            "Train Epoch: 170 [304/486 (63%)]\tlearningLoss: 0.000940\twhole_loss: 0.000125 \n",
            "Train Epoch: 170 [320/486 (66%)]\tlearningLoss: 0.000943\twhole_loss: 0.000548 \n",
            "Train Epoch: 170 [336/486 (69%)]\tlearningLoss: 0.000980\twhole_loss: 0.006489 \n",
            "Train Epoch: 170 [352/486 (72%)]\tlearningLoss: 0.002951\twhole_loss: 0.344859 \n",
            "Train Epoch: 170 [368/486 (76%)]\tlearningLoss: 0.002961\twhole_loss: 0.001798 \n",
            "Train Epoch: 170 [384/486 (79%)]\tlearningLoss: 0.003077\twhole_loss: 0.020353 \n",
            "Train Epoch: 170 [400/486 (82%)]\tlearningLoss: 0.003093\twhole_loss: 0.002800 \n",
            "Train Epoch: 170 [416/486 (86%)]\tlearningLoss: 0.003112\twhole_loss: 0.003351 \n",
            "Train Epoch: 170 [432/486 (89%)]\tlearningLoss: 0.003216\twhole_loss: 0.018243 \n",
            "Train Epoch: 170 [448/486 (92%)]\tlearningLoss: 0.003627\twhole_loss: 0.071782 \n",
            "Train Epoch: 170 [464/486 (95%)]\tlearningLoss: 0.003739\twhole_loss: 0.019641 \n",
            "Train Epoch: 170 [180/486 (37%)]\tlearningLoss: 0.003746\twhole_loss: 0.001257 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.0724, Accuracy: 70/75(93%)\n",
            "\n",
            "Train Epoch: 171 [0/486 (0%)]\tlearningLoss: 0.000011\twhole_loss: 0.001937 \n",
            "Train Epoch: 171 [16/486 (3%)]\tlearningLoss: 0.000013\twhole_loss: 0.000382 \n",
            "Train Epoch: 171 [32/486 (7%)]\tlearningLoss: 0.000154\twhole_loss: 0.024681 \n",
            "Train Epoch: 171 [48/486 (10%)]\tlearningLoss: 0.000171\twhole_loss: 0.002936 \n",
            "Train Epoch: 171 [64/486 (13%)]\tlearningLoss: 0.000380\twhole_loss: 0.036578 \n",
            "Train Epoch: 171 [80/486 (16%)]\tlearningLoss: 0.000399\twhole_loss: 0.003314 \n",
            "Train Epoch: 171 [96/486 (20%)]\tlearningLoss: 0.000462\twhole_loss: 0.010961 \n",
            "Train Epoch: 171 [112/486 (23%)]\tlearningLoss: 0.000598\twhole_loss: 0.023883 \n",
            "Train Epoch: 171 [128/486 (26%)]\tlearningLoss: 0.000622\twhole_loss: 0.004109 \n",
            "Train Epoch: 171 [144/486 (30%)]\tlearningLoss: 0.000625\twhole_loss: 0.000623 \n",
            "Train Epoch: 171 [160/486 (33%)]\tlearningLoss: 0.000658\twhole_loss: 0.005700 \n",
            "Train Epoch: 171 [176/486 (36%)]\tlearningLoss: 0.001013\twhole_loss: 0.062100 \n",
            "Train Epoch: 171 [192/486 (40%)]\tlearningLoss: 0.001014\twhole_loss: 0.000210 \n",
            "Train Epoch: 171 [208/486 (43%)]\tlearningLoss: 0.001021\twhole_loss: 0.001212 \n",
            "Train Epoch: 171 [224/486 (46%)]\tlearningLoss: 0.002199\twhole_loss: 0.206274 \n",
            "Train Epoch: 171 [240/486 (49%)]\tlearningLoss: 0.002200\twhole_loss: 0.000169 \n",
            "Train Epoch: 171 [256/486 (53%)]\tlearningLoss: 0.002331\twhole_loss: 0.022830 \n",
            "Train Epoch: 171 [272/486 (56%)]\tlearningLoss: 0.002367\twhole_loss: 0.006308 \n",
            "Train Epoch: 171 [288/486 (59%)]\tlearningLoss: 0.002528\twhole_loss: 0.028246 \n",
            "Train Epoch: 171 [304/486 (63%)]\tlearningLoss: 0.002531\twhole_loss: 0.000413 \n",
            "Train Epoch: 171 [320/486 (66%)]\tlearningLoss: 0.002531\twhole_loss: 0.000006 \n",
            "Train Epoch: 171 [336/486 (69%)]\tlearningLoss: 0.002564\twhole_loss: 0.005891 \n",
            "Train Epoch: 171 [352/486 (72%)]\tlearningLoss: 0.002576\twhole_loss: 0.002033 \n",
            "Train Epoch: 171 [368/486 (76%)]\tlearningLoss: 0.002583\twhole_loss: 0.001185 \n",
            "Train Epoch: 171 [384/486 (79%)]\tlearningLoss: 0.002583\twhole_loss: 0.000018 \n",
            "Train Epoch: 171 [400/486 (82%)]\tlearningLoss: 0.002584\twhole_loss: 0.000116 \n",
            "Train Epoch: 171 [416/486 (86%)]\tlearningLoss: 0.002584\twhole_loss: 0.000086 \n",
            "Train Epoch: 171 [432/486 (89%)]\tlearningLoss: 0.002686\twhole_loss: 0.017868 \n",
            "Train Epoch: 171 [448/486 (92%)]\tlearningLoss: 0.002778\twhole_loss: 0.016118 \n",
            "Train Epoch: 171 [464/486 (95%)]\tlearningLoss: 0.002779\twhole_loss: 0.000068 \n",
            "Train Epoch: 171 [180/486 (37%)]\tlearningLoss: 0.002782\twhole_loss: 0.000602 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 2, 1, 1, 2, 0, 2, 2, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:7.1646, Accuracy: 63/75(84%)\n",
            "\n",
            "Train Epoch: 172 [0/486 (0%)]\tlearningLoss: 0.000042\twhole_loss: 0.007388 \n",
            "Train Epoch: 172 [16/486 (3%)]\tlearningLoss: 0.000048\twhole_loss: 0.001014 \n",
            "Train Epoch: 172 [32/486 (7%)]\tlearningLoss: 0.000386\twhole_loss: 0.059214 \n",
            "Train Epoch: 172 [48/486 (10%)]\tlearningLoss: 0.000444\twhole_loss: 0.010119 \n",
            "Train Epoch: 172 [64/486 (13%)]\tlearningLoss: 0.000446\twhole_loss: 0.000306 \n",
            "Train Epoch: 172 [80/486 (16%)]\tlearningLoss: 0.000534\twhole_loss: 0.015447 \n",
            "Train Epoch: 172 [96/486 (20%)]\tlearningLoss: 0.000545\twhole_loss: 0.001902 \n",
            "Train Epoch: 172 [112/486 (23%)]\tlearningLoss: 0.000553\twhole_loss: 0.001399 \n",
            "Train Epoch: 172 [128/486 (26%)]\tlearningLoss: 0.000554\twhole_loss: 0.000221 \n",
            "Train Epoch: 172 [144/486 (30%)]\tlearningLoss: 0.000590\twhole_loss: 0.006273 \n",
            "Train Epoch: 172 [160/486 (33%)]\tlearningLoss: 0.004054\twhole_loss: 0.606144 \n",
            "Train Epoch: 172 [176/486 (36%)]\tlearningLoss: 0.004855\twhole_loss: 0.140166 \n",
            "Train Epoch: 172 [192/486 (40%)]\tlearningLoss: 0.004858\twhole_loss: 0.000549 \n",
            "Train Epoch: 172 [208/486 (43%)]\tlearningLoss: 0.004873\twhole_loss: 0.002676 \n",
            "Train Epoch: 172 [224/486 (46%)]\tlearningLoss: 0.005012\twhole_loss: 0.024269 \n",
            "Train Epoch: 172 [240/486 (49%)]\tlearningLoss: 0.005046\twhole_loss: 0.005900 \n",
            "Train Epoch: 172 [256/486 (53%)]\tlearningLoss: 0.008344\twhole_loss: 0.577177 \n",
            "Train Epoch: 172 [272/486 (56%)]\tlearningLoss: 0.009921\twhole_loss: 0.275967 \n",
            "Train Epoch: 172 [288/486 (59%)]\tlearningLoss: 0.010097\twhole_loss: 0.030853 \n",
            "Train Epoch: 172 [304/486 (63%)]\tlearningLoss: 0.011406\twhole_loss: 0.229088 \n",
            "Train Epoch: 172 [320/486 (66%)]\tlearningLoss: 0.011473\twhole_loss: 0.011633 \n",
            "Train Epoch: 172 [336/486 (69%)]\tlearningLoss: 0.011486\twhole_loss: 0.002368 \n",
            "Train Epoch: 172 [352/486 (72%)]\tlearningLoss: 0.014648\twhole_loss: 0.553307 \n",
            "Train Epoch: 172 [368/486 (76%)]\tlearningLoss: 0.015126\twhole_loss: 0.083585 \n",
            "Train Epoch: 172 [384/486 (79%)]\tlearningLoss: 0.015158\twhole_loss: 0.005643 \n",
            "Train Epoch: 172 [400/486 (82%)]\tlearningLoss: 0.015391\twhole_loss: 0.040854 \n",
            "Train Epoch: 172 [416/486 (86%)]\tlearningLoss: 0.016047\twhole_loss: 0.114841 \n",
            "Train Epoch: 172 [432/486 (89%)]\tlearningLoss: 0.016649\twhole_loss: 0.105189 \n",
            "Train Epoch: 172 [448/486 (92%)]\tlearningLoss: 0.016659\twhole_loss: 0.001896 \n",
            "Train Epoch: 172 [464/486 (95%)]\tlearningLoss: 0.016681\twhole_loss: 0.003816 \n",
            "Train Epoch: 172 [180/486 (37%)]\tlearningLoss: 0.016717\twhole_loss: 0.006188 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 0, 1, 0, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.9047, Accuracy: 63/75(84%)\n",
            "\n",
            "Train Epoch: 173 [0/486 (0%)]\tlearningLoss: 0.000122\twhole_loss: 0.021406 \n",
            "Train Epoch: 173 [16/486 (3%)]\tlearningLoss: 0.000292\twhole_loss: 0.029736 \n",
            "Train Epoch: 173 [32/486 (7%)]\tlearningLoss: 0.001391\twhole_loss: 0.192280 \n",
            "Train Epoch: 173 [48/486 (10%)]\tlearningLoss: 0.001531\twhole_loss: 0.024421 \n",
            "Train Epoch: 173 [64/486 (13%)]\tlearningLoss: 0.003100\twhole_loss: 0.274587 \n",
            "Train Epoch: 173 [80/486 (16%)]\tlearningLoss: 0.003759\twhole_loss: 0.115358 \n",
            "Train Epoch: 173 [96/486 (20%)]\tlearningLoss: 0.004065\twhole_loss: 0.053655 \n",
            "Train Epoch: 173 [112/486 (23%)]\tlearningLoss: 0.004134\twhole_loss: 0.011964 \n",
            "Train Epoch: 173 [128/486 (26%)]\tlearningLoss: 0.005268\twhole_loss: 0.198445 \n",
            "Train Epoch: 173 [144/486 (30%)]\tlearningLoss: 0.005390\twhole_loss: 0.021315 \n",
            "Train Epoch: 173 [160/486 (33%)]\tlearningLoss: 0.005710\twhole_loss: 0.056117 \n",
            "Train Epoch: 173 [176/486 (36%)]\tlearningLoss: 0.005737\twhole_loss: 0.004772 \n",
            "Train Epoch: 173 [192/486 (40%)]\tlearningLoss: 0.008571\twhole_loss: 0.495850 \n",
            "Train Epoch: 173 [208/486 (43%)]\tlearningLoss: 0.008607\twhole_loss: 0.006365 \n",
            "Train Epoch: 173 [224/486 (46%)]\tlearningLoss: 0.008732\twhole_loss: 0.021761 \n",
            "Train Epoch: 173 [240/486 (49%)]\tlearningLoss: 0.008967\twhole_loss: 0.041158 \n",
            "Train Epoch: 173 [256/486 (53%)]\tlearningLoss: 0.009656\twhole_loss: 0.120548 \n",
            "Train Epoch: 173 [272/486 (56%)]\tlearningLoss: 0.009880\twhole_loss: 0.039181 \n",
            "Train Epoch: 173 [288/486 (59%)]\tlearningLoss: 0.009915\twhole_loss: 0.006170 \n",
            "Train Epoch: 173 [304/486 (63%)]\tlearningLoss: 0.010929\twhole_loss: 0.177500 \n",
            "Train Epoch: 173 [320/486 (66%)]\tlearningLoss: 0.011147\twhole_loss: 0.038061 \n",
            "Train Epoch: 173 [336/486 (69%)]\tlearningLoss: 0.011672\twhole_loss: 0.091908 \n",
            "Train Epoch: 173 [352/486 (72%)]\tlearningLoss: 0.011685\twhole_loss: 0.002273 \n",
            "Train Epoch: 173 [368/486 (76%)]\tlearningLoss: 0.011694\twhole_loss: 0.001543 \n",
            "Train Epoch: 173 [384/486 (79%)]\tlearningLoss: 0.012478\twhole_loss: 0.137203 \n",
            "Train Epoch: 173 [400/486 (82%)]\tlearningLoss: 0.013277\twhole_loss: 0.139980 \n",
            "Train Epoch: 173 [416/486 (86%)]\tlearningLoss: 0.013701\twhole_loss: 0.074090 \n",
            "Train Epoch: 173 [432/486 (89%)]\tlearningLoss: 0.015284\twhole_loss: 0.276982 \n",
            "Train Epoch: 173 [448/486 (92%)]\tlearningLoss: 0.015340\twhole_loss: 0.009848 \n",
            "Train Epoch: 173 [464/486 (95%)]\tlearningLoss: 0.015602\twhole_loss: 0.045861 \n",
            "Train Epoch: 173 [180/486 (37%)]\tlearningLoss: 0.015619\twhole_loss: 0.002973 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 2, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.1661, Accuracy: 64/75(85%)\n",
            "\n",
            "Train Epoch: 174 [0/486 (0%)]\tlearningLoss: 0.000017\twhole_loss: 0.002910 \n",
            "Train Epoch: 174 [16/486 (3%)]\tlearningLoss: 0.000049\twhole_loss: 0.005739 \n",
            "Train Epoch: 174 [32/486 (7%)]\tlearningLoss: 0.000078\twhole_loss: 0.004996 \n",
            "Train Epoch: 174 [48/486 (10%)]\tlearningLoss: 0.000087\twhole_loss: 0.001588 \n",
            "Train Epoch: 174 [64/486 (13%)]\tlearningLoss: 0.000123\twhole_loss: 0.006325 \n",
            "Train Epoch: 174 [80/486 (16%)]\tlearningLoss: 0.000160\twhole_loss: 0.006438 \n",
            "Train Epoch: 174 [96/486 (20%)]\tlearningLoss: 0.000257\twhole_loss: 0.017060 \n",
            "Train Epoch: 174 [112/486 (23%)]\tlearningLoss: 0.000475\twhole_loss: 0.037998 \n",
            "Train Epoch: 174 [128/486 (26%)]\tlearningLoss: 0.000528\twhole_loss: 0.009260 \n",
            "Train Epoch: 174 [144/486 (30%)]\tlearningLoss: 0.000633\twhole_loss: 0.018481 \n",
            "Train Epoch: 174 [160/486 (33%)]\tlearningLoss: 0.000634\twhole_loss: 0.000074 \n",
            "Train Epoch: 174 [176/486 (36%)]\tlearningLoss: 0.001073\twhole_loss: 0.076914 \n",
            "Train Epoch: 174 [192/486 (40%)]\tlearningLoss: 0.002467\twhole_loss: 0.244008 \n",
            "Train Epoch: 174 [208/486 (43%)]\tlearningLoss: 0.002514\twhole_loss: 0.008099 \n",
            "Train Epoch: 174 [224/486 (46%)]\tlearningLoss: 0.002737\twhole_loss: 0.039157 \n",
            "Train Epoch: 174 [240/486 (49%)]\tlearningLoss: 0.002761\twhole_loss: 0.004048 \n",
            "Train Epoch: 174 [256/486 (53%)]\tlearningLoss: 0.003083\twhole_loss: 0.056348 \n",
            "Train Epoch: 174 [272/486 (56%)]\tlearningLoss: 0.003639\twhole_loss: 0.097465 \n",
            "Train Epoch: 174 [288/486 (59%)]\tlearningLoss: 0.003656\twhole_loss: 0.002908 \n",
            "Train Epoch: 174 [304/486 (63%)]\tlearningLoss: 0.003970\twhole_loss: 0.054885 \n",
            "Train Epoch: 174 [320/486 (66%)]\tlearningLoss: 0.004014\twhole_loss: 0.007786 \n",
            "Train Epoch: 174 [336/486 (69%)]\tlearningLoss: 0.004172\twhole_loss: 0.027689 \n",
            "Train Epoch: 174 [352/486 (72%)]\tlearningLoss: 0.004210\twhole_loss: 0.006615 \n",
            "Train Epoch: 174 [368/486 (76%)]\tlearningLoss: 0.004504\twhole_loss: 0.051421 \n",
            "Train Epoch: 174 [384/486 (79%)]\tlearningLoss: 0.004575\twhole_loss: 0.012429 \n",
            "Train Epoch: 174 [400/486 (82%)]\tlearningLoss: 0.004597\twhole_loss: 0.003869 \n",
            "Train Epoch: 174 [416/486 (86%)]\tlearningLoss: 0.004611\twhole_loss: 0.002500 \n",
            "Train Epoch: 174 [432/486 (89%)]\tlearningLoss: 0.004629\twhole_loss: 0.003004 \n",
            "Train Epoch: 174 [448/486 (92%)]\tlearningLoss: 0.004641\twhole_loss: 0.002112 \n",
            "Train Epoch: 174 [464/486 (95%)]\tlearningLoss: 0.004662\twhole_loss: 0.003742 \n",
            "Train Epoch: 174 [180/486 (37%)]\tlearningLoss: 0.004662\twhole_loss: 0.000034 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.5484, Accuracy: 68/75(91%)\n",
            "\n",
            "Train Epoch: 175 [0/486 (0%)]\tlearningLoss: 0.000016\twhole_loss: 0.002845 \n",
            "Train Epoch: 175 [16/486 (3%)]\tlearningLoss: 0.000189\twhole_loss: 0.030209 \n",
            "Train Epoch: 175 [32/486 (7%)]\tlearningLoss: 0.000292\twhole_loss: 0.017961 \n",
            "Train Epoch: 175 [48/486 (10%)]\tlearningLoss: 0.000417\twhole_loss: 0.021980 \n",
            "Train Epoch: 175 [64/486 (13%)]\tlearningLoss: 0.000634\twhole_loss: 0.037980 \n",
            "Train Epoch: 175 [80/486 (16%)]\tlearningLoss: 0.000639\twhole_loss: 0.000810 \n",
            "Train Epoch: 175 [96/486 (20%)]\tlearningLoss: 0.000653\twhole_loss: 0.002426 \n",
            "Train Epoch: 175 [112/486 (23%)]\tlearningLoss: 0.000679\twhole_loss: 0.004627 \n",
            "Train Epoch: 175 [128/486 (26%)]\tlearningLoss: 0.000729\twhole_loss: 0.008804 \n",
            "Train Epoch: 175 [144/486 (30%)]\tlearningLoss: 0.000749\twhole_loss: 0.003382 \n",
            "Train Epoch: 175 [160/486 (33%)]\tlearningLoss: 0.000783\twhole_loss: 0.005997 \n",
            "Train Epoch: 175 [176/486 (36%)]\tlearningLoss: 0.000843\twhole_loss: 0.010419 \n",
            "Train Epoch: 175 [192/486 (40%)]\tlearningLoss: 0.001122\twhole_loss: 0.048914 \n",
            "Train Epoch: 175 [208/486 (43%)]\tlearningLoss: 0.001130\twhole_loss: 0.001377 \n",
            "Train Epoch: 175 [224/486 (46%)]\tlearningLoss: 0.001159\twhole_loss: 0.005128 \n",
            "Train Epoch: 175 [240/486 (49%)]\tlearningLoss: 0.001167\twhole_loss: 0.001389 \n",
            "Train Epoch: 175 [256/486 (53%)]\tlearningLoss: 0.001174\twhole_loss: 0.001180 \n",
            "Train Epoch: 175 [272/486 (56%)]\tlearningLoss: 0.001239\twhole_loss: 0.011411 \n",
            "Train Epoch: 175 [288/486 (59%)]\tlearningLoss: 0.001301\twhole_loss: 0.010838 \n",
            "Train Epoch: 175 [304/486 (63%)]\tlearningLoss: 0.002594\twhole_loss: 0.226252 \n",
            "Train Epoch: 175 [320/486 (66%)]\tlearningLoss: 0.002640\twhole_loss: 0.008013 \n",
            "Train Epoch: 175 [336/486 (69%)]\tlearningLoss: 0.002657\twhole_loss: 0.003016 \n",
            "Train Epoch: 175 [352/486 (72%)]\tlearningLoss: 0.002846\twhole_loss: 0.033071 \n",
            "Train Epoch: 175 [368/486 (76%)]\tlearningLoss: 0.002889\twhole_loss: 0.007608 \n",
            "Train Epoch: 175 [384/486 (79%)]\tlearningLoss: 0.003687\twhole_loss: 0.139617 \n",
            "Train Epoch: 175 [400/486 (82%)]\tlearningLoss: 0.003723\twhole_loss: 0.006195 \n",
            "Train Epoch: 175 [416/486 (86%)]\tlearningLoss: 0.004372\twhole_loss: 0.113583 \n",
            "Train Epoch: 175 [432/486 (89%)]\tlearningLoss: 0.004459\twhole_loss: 0.015232 \n",
            "Train Epoch: 175 [448/486 (92%)]\tlearningLoss: 0.004574\twhole_loss: 0.020185 \n",
            "Train Epoch: 175 [464/486 (95%)]\tlearningLoss: 0.005316\twhole_loss: 0.129904 \n",
            "Train Epoch: 175 [180/486 (37%)]\tlearningLoss: 0.005386\twhole_loss: 0.012259 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 2, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.1076, Accuracy: 65/75(87%)\n",
            "\n",
            "Train Epoch: 176 [0/486 (0%)]\tlearningLoss: 0.000177\twhole_loss: 0.031030 \n",
            "Train Epoch: 176 [16/486 (3%)]\tlearningLoss: 0.000210\twhole_loss: 0.005719 \n",
            "Train Epoch: 176 [32/486 (7%)]\tlearningLoss: 0.000222\twhole_loss: 0.002187 \n",
            "Train Epoch: 176 [48/486 (10%)]\tlearningLoss: 0.000326\twhole_loss: 0.018065 \n",
            "Train Epoch: 176 [64/486 (13%)]\tlearningLoss: 0.000431\twhole_loss: 0.018463 \n",
            "Train Epoch: 176 [80/486 (16%)]\tlearningLoss: 0.000473\twhole_loss: 0.007266 \n",
            "Train Epoch: 176 [96/486 (20%)]\tlearningLoss: 0.000499\twhole_loss: 0.004560 \n",
            "Train Epoch: 176 [112/486 (23%)]\tlearningLoss: 0.000521\twhole_loss: 0.003895 \n",
            "Train Epoch: 176 [128/486 (26%)]\tlearningLoss: 0.000570\twhole_loss: 0.008515 \n",
            "Train Epoch: 176 [144/486 (30%)]\tlearningLoss: 0.001007\twhole_loss: 0.076547 \n",
            "Train Epoch: 176 [160/486 (33%)]\tlearningLoss: 0.001266\twhole_loss: 0.045273 \n",
            "Train Epoch: 176 [176/486 (36%)]\tlearningLoss: 0.001293\twhole_loss: 0.004787 \n",
            "Train Epoch: 176 [192/486 (40%)]\tlearningLoss: 0.001305\twhole_loss: 0.002065 \n",
            "Train Epoch: 176 [208/486 (43%)]\tlearningLoss: 0.001360\twhole_loss: 0.009691 \n",
            "Train Epoch: 176 [224/486 (46%)]\tlearningLoss: 0.001464\twhole_loss: 0.018220 \n",
            "Train Epoch: 176 [240/486 (49%)]\tlearningLoss: 0.001488\twhole_loss: 0.004040 \n",
            "Train Epoch: 176 [256/486 (53%)]\tlearningLoss: 0.002130\twhole_loss: 0.112433 \n",
            "Train Epoch: 176 [272/486 (56%)]\tlearningLoss: 0.002130\twhole_loss: 0.000075 \n",
            "Train Epoch: 176 [288/486 (59%)]\tlearningLoss: 0.002184\twhole_loss: 0.009318 \n",
            "Train Epoch: 176 [304/486 (63%)]\tlearningLoss: 0.002312\twhole_loss: 0.022398 \n",
            "Train Epoch: 176 [320/486 (66%)]\tlearningLoss: 0.002502\twhole_loss: 0.033287 \n",
            "Train Epoch: 176 [336/486 (69%)]\tlearningLoss: 0.002631\twhole_loss: 0.022531 \n",
            "Train Epoch: 176 [352/486 (72%)]\tlearningLoss: 0.002633\twhole_loss: 0.000433 \n",
            "Train Epoch: 176 [368/486 (76%)]\tlearningLoss: 0.002672\twhole_loss: 0.006732 \n",
            "Train Epoch: 176 [384/486 (79%)]\tlearningLoss: 0.002708\twhole_loss: 0.006411 \n",
            "Train Epoch: 176 [400/486 (82%)]\tlearningLoss: 0.002716\twhole_loss: 0.001371 \n",
            "Train Epoch: 176 [416/486 (86%)]\tlearningLoss: 0.002921\twhole_loss: 0.035866 \n",
            "Train Epoch: 176 [432/486 (89%)]\tlearningLoss: 0.002968\twhole_loss: 0.008157 \n",
            "Train Epoch: 176 [448/486 (92%)]\tlearningLoss: 0.002969\twhole_loss: 0.000249 \n",
            "Train Epoch: 176 [464/486 (95%)]\tlearningLoss: 0.002984\twhole_loss: 0.002618 \n",
            "Train Epoch: 176 [180/486 (37%)]\tlearningLoss: 0.003259\twhole_loss: 0.048125 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.2503, Accuracy: 70/75(93%)\n",
            "\n",
            "Train Epoch: 177 [0/486 (0%)]\tlearningLoss: 0.000002\twhole_loss: 0.000381 \n",
            "Train Epoch: 177 [16/486 (3%)]\tlearningLoss: 0.000540\twhole_loss: 0.094156 \n",
            "Train Epoch: 177 [32/486 (7%)]\tlearningLoss: 0.001341\twhole_loss: 0.140179 \n",
            "Train Epoch: 177 [48/486 (10%)]\tlearningLoss: 0.001344\twhole_loss: 0.000440 \n",
            "Train Epoch: 177 [64/486 (13%)]\tlearningLoss: 0.001374\twhole_loss: 0.005304 \n",
            "Train Epoch: 177 [80/486 (16%)]\tlearningLoss: 0.001462\twhole_loss: 0.015357 \n",
            "Train Epoch: 177 [96/486 (20%)]\tlearningLoss: 0.001540\twhole_loss: 0.013618 \n",
            "Train Epoch: 177 [112/486 (23%)]\tlearningLoss: 0.002698\twhole_loss: 0.202632 \n",
            "Train Epoch: 177 [128/486 (26%)]\tlearningLoss: 0.002775\twhole_loss: 0.013527 \n",
            "Train Epoch: 177 [144/486 (30%)]\tlearningLoss: 0.002801\twhole_loss: 0.004636 \n",
            "Train Epoch: 177 [160/486 (33%)]\tlearningLoss: 0.002875\twhole_loss: 0.012911 \n",
            "Train Epoch: 177 [176/486 (36%)]\tlearningLoss: 0.006049\twhole_loss: 0.555422 \n",
            "Train Epoch: 177 [192/486 (40%)]\tlearningLoss: 0.006055\twhole_loss: 0.001083 \n",
            "Train Epoch: 177 [208/486 (43%)]\tlearningLoss: 0.006121\twhole_loss: 0.011614 \n",
            "Train Epoch: 177 [224/486 (46%)]\tlearningLoss: 0.006692\twhole_loss: 0.099815 \n",
            "Train Epoch: 177 [240/486 (49%)]\tlearningLoss: 0.006708\twhole_loss: 0.002886 \n",
            "Train Epoch: 177 [256/486 (53%)]\tlearningLoss: 0.006714\twhole_loss: 0.001025 \n",
            "Train Epoch: 177 [272/486 (56%)]\tlearningLoss: 0.006798\twhole_loss: 0.014690 \n",
            "Train Epoch: 177 [288/486 (59%)]\tlearningLoss: 0.006803\twhole_loss: 0.000928 \n",
            "Train Epoch: 177 [304/486 (63%)]\tlearningLoss: 0.007095\twhole_loss: 0.050941 \n",
            "Train Epoch: 177 [320/486 (66%)]\tlearningLoss: 0.010935\twhole_loss: 0.672105 \n",
            "Train Epoch: 177 [336/486 (69%)]\tlearningLoss: 0.011060\twhole_loss: 0.021916 \n",
            "Train Epoch: 177 [352/486 (72%)]\tlearningLoss: 0.013383\twhole_loss: 0.406444 \n",
            "Train Epoch: 177 [368/486 (76%)]\tlearningLoss: 0.013514\twhole_loss: 0.022871 \n",
            "Train Epoch: 177 [384/486 (79%)]\tlearningLoss: 0.016588\twhole_loss: 0.537996 \n",
            "Train Epoch: 177 [400/486 (82%)]\tlearningLoss: 0.018189\twhole_loss: 0.280258 \n",
            "Train Epoch: 177 [416/486 (86%)]\tlearningLoss: 0.019234\twhole_loss: 0.182794 \n",
            "Train Epoch: 177 [432/486 (89%)]\tlearningLoss: 0.021094\twhole_loss: 0.325594 \n",
            "Train Epoch: 177 [448/486 (92%)]\tlearningLoss: 0.021342\twhole_loss: 0.043240 \n",
            "Train Epoch: 177 [464/486 (95%)]\tlearningLoss: 0.022253\twhole_loss: 0.159428 \n",
            "Train Epoch: 177 [180/486 (37%)]\tlearningLoss: 0.022316\twhole_loss: 0.011042 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.1663, Accuracy: 63/75(84%)\n",
            "\n",
            "Train Epoch: 178 [0/486 (0%)]\tlearningLoss: 0.000010\twhole_loss: 0.001792 \n",
            "Train Epoch: 178 [16/486 (3%)]\tlearningLoss: 0.003253\twhole_loss: 0.567460 \n",
            "Train Epoch: 178 [32/486 (7%)]\tlearningLoss: 0.003455\twhole_loss: 0.035457 \n",
            "Train Epoch: 178 [48/486 (10%)]\tlearningLoss: 0.003613\twhole_loss: 0.027553 \n",
            "Train Epoch: 178 [64/486 (13%)]\tlearningLoss: 0.004287\twhole_loss: 0.117999 \n",
            "Train Epoch: 178 [80/486 (16%)]\tlearningLoss: 0.005142\twhole_loss: 0.149570 \n",
            "Train Epoch: 178 [96/486 (20%)]\tlearningLoss: 0.005801\twhole_loss: 0.115317 \n",
            "Train Epoch: 178 [112/486 (23%)]\tlearningLoss: 0.007960\twhole_loss: 0.377790 \n",
            "Train Epoch: 178 [128/486 (26%)]\tlearningLoss: 0.009999\twhole_loss: 0.356906 \n",
            "Train Epoch: 178 [144/486 (30%)]\tlearningLoss: 0.010163\twhole_loss: 0.028689 \n",
            "Train Epoch: 178 [160/486 (33%)]\tlearningLoss: 0.010393\twhole_loss: 0.040325 \n",
            "Train Epoch: 178 [176/486 (36%)]\tlearningLoss: 0.011124\twhole_loss: 0.127859 \n",
            "Train Epoch: 178 [192/486 (40%)]\tlearningLoss: 0.011322\twhole_loss: 0.034599 \n",
            "Train Epoch: 178 [208/486 (43%)]\tlearningLoss: 0.011472\twhole_loss: 0.026340 \n",
            "Train Epoch: 178 [224/486 (46%)]\tlearningLoss: 0.012537\twhole_loss: 0.186251 \n",
            "Train Epoch: 178 [240/486 (49%)]\tlearningLoss: 0.012898\twhole_loss: 0.063199 \n",
            "Train Epoch: 178 [256/486 (53%)]\tlearningLoss: 0.013424\twhole_loss: 0.092121 \n",
            "Train Epoch: 178 [272/486 (56%)]\tlearningLoss: 0.013770\twhole_loss: 0.060506 \n",
            "Train Epoch: 178 [288/486 (59%)]\tlearningLoss: 0.014228\twhole_loss: 0.080194 \n",
            "Train Epoch: 178 [304/486 (63%)]\tlearningLoss: 0.016737\twhole_loss: 0.438963 \n",
            "Train Epoch: 178 [320/486 (66%)]\tlearningLoss: 0.016764\twhole_loss: 0.004880 \n",
            "Train Epoch: 178 [336/486 (69%)]\tlearningLoss: 0.017842\twhole_loss: 0.188663 \n",
            "Train Epoch: 178 [352/486 (72%)]\tlearningLoss: 0.018880\twhole_loss: 0.181580 \n",
            "Train Epoch: 178 [368/486 (76%)]\tlearningLoss: 0.018902\twhole_loss: 0.003824 \n",
            "Train Epoch: 178 [384/486 (79%)]\tlearningLoss: 0.018936\twhole_loss: 0.006000 \n",
            "Train Epoch: 178 [400/486 (82%)]\tlearningLoss: 0.019082\twhole_loss: 0.025435 \n",
            "Train Epoch: 178 [416/486 (86%)]\tlearningLoss: 0.019236\twhole_loss: 0.027027 \n",
            "Train Epoch: 178 [432/486 (89%)]\tlearningLoss: 0.019634\twhole_loss: 0.069613 \n",
            "Train Epoch: 178 [448/486 (92%)]\tlearningLoss: 0.020576\twhole_loss: 0.164900 \n",
            "Train Epoch: 178 [464/486 (95%)]\tlearningLoss: 0.021537\twhole_loss: 0.168125 \n",
            "Train Epoch: 178 [180/486 (37%)]\tlearningLoss: 0.021566\twhole_loss: 0.005117 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 2, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.1267, Accuracy: 66/75(88%)\n",
            "\n",
            "Train Epoch: 179 [0/486 (0%)]\tlearningLoss: 0.000572\twhole_loss: 0.100177 \n",
            "Train Epoch: 179 [16/486 (3%)]\tlearningLoss: 0.000593\twhole_loss: 0.003635 \n",
            "Train Epoch: 179 [32/486 (7%)]\tlearningLoss: 0.001893\twhole_loss: 0.227511 \n",
            "Train Epoch: 179 [48/486 (10%)]\tlearningLoss: 0.002076\twhole_loss: 0.031905 \n",
            "Train Epoch: 179 [64/486 (13%)]\tlearningLoss: 0.002173\twhole_loss: 0.016986 \n",
            "Train Epoch: 179 [80/486 (16%)]\tlearningLoss: 0.002783\twhole_loss: 0.106792 \n",
            "Train Epoch: 179 [96/486 (20%)]\tlearningLoss: 0.003123\twhole_loss: 0.059583 \n",
            "Train Epoch: 179 [112/486 (23%)]\tlearningLoss: 0.004141\twhole_loss: 0.178083 \n",
            "Train Epoch: 179 [128/486 (26%)]\tlearningLoss: 0.004160\twhole_loss: 0.003414 \n",
            "Train Epoch: 179 [144/486 (30%)]\tlearningLoss: 0.004168\twhole_loss: 0.001326 \n",
            "Train Epoch: 179 [160/486 (33%)]\tlearningLoss: 0.004748\twhole_loss: 0.101449 \n",
            "Train Epoch: 179 [176/486 (36%)]\tlearningLoss: 0.004785\twhole_loss: 0.006451 \n",
            "Train Epoch: 179 [192/486 (40%)]\tlearningLoss: 0.005920\twhole_loss: 0.198673 \n",
            "Train Epoch: 179 [208/486 (43%)]\tlearningLoss: 0.006834\twhole_loss: 0.160027 \n",
            "Train Epoch: 179 [224/486 (46%)]\tlearningLoss: 0.007028\twhole_loss: 0.033808 \n",
            "Train Epoch: 179 [240/486 (49%)]\tlearningLoss: 0.007057\twhole_loss: 0.005212 \n",
            "Train Epoch: 179 [256/486 (53%)]\tlearningLoss: 0.007176\twhole_loss: 0.020800 \n",
            "Train Epoch: 179 [272/486 (56%)]\tlearningLoss: 0.008099\twhole_loss: 0.161414 \n",
            "Train Epoch: 179 [288/486 (59%)]\tlearningLoss: 0.008243\twhole_loss: 0.025312 \n",
            "Train Epoch: 179 [304/486 (63%)]\tlearningLoss: 0.009904\twhole_loss: 0.290609 \n",
            "Train Epoch: 179 [320/486 (66%)]\tlearningLoss: 0.009954\twhole_loss: 0.008773 \n",
            "Train Epoch: 179 [336/486 (69%)]\tlearningLoss: 0.009963\twhole_loss: 0.001601 \n",
            "Train Epoch: 179 [352/486 (72%)]\tlearningLoss: 0.010815\twhole_loss: 0.149058 \n",
            "Train Epoch: 179 [368/486 (76%)]\tlearningLoss: 0.012647\twhole_loss: 0.320579 \n",
            "Train Epoch: 179 [384/486 (79%)]\tlearningLoss: 0.013002\twhole_loss: 0.062136 \n",
            "Train Epoch: 179 [400/486 (82%)]\tlearningLoss: 0.013002\twhole_loss: 0.000010 \n",
            "Train Epoch: 179 [416/486 (86%)]\tlearningLoss: 0.013071\twhole_loss: 0.012182 \n",
            "Train Epoch: 179 [432/486 (89%)]\tlearningLoss: 0.013490\twhole_loss: 0.073299 \n",
            "Train Epoch: 179 [448/486 (92%)]\tlearningLoss: 0.015085\twhole_loss: 0.279000 \n",
            "Train Epoch: 179 [464/486 (95%)]\tlearningLoss: 0.015790\twhole_loss: 0.123478 \n",
            "Train Epoch: 179 [180/486 (37%)]\tlearningLoss: 0.015963\twhole_loss: 0.030159 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.6054, Accuracy: 69/75(92%)\n",
            "\n",
            "Train Epoch: 180 [0/486 (0%)]\tlearningLoss: 0.000258\twhole_loss: 0.045235 \n",
            "Train Epoch: 180 [16/486 (3%)]\tlearningLoss: 0.000283\twhole_loss: 0.004290 \n",
            "Train Epoch: 180 [32/486 (7%)]\tlearningLoss: 0.000301\twhole_loss: 0.003111 \n",
            "Train Epoch: 180 [48/486 (10%)]\tlearningLoss: 0.000607\twhole_loss: 0.053656 \n",
            "Train Epoch: 180 [64/486 (13%)]\tlearningLoss: 0.002294\twhole_loss: 0.295155 \n",
            "Train Epoch: 180 [80/486 (16%)]\tlearningLoss: 0.002349\twhole_loss: 0.009585 \n",
            "Train Epoch: 180 [96/486 (20%)]\tlearningLoss: 0.002840\twhole_loss: 0.085964 \n",
            "Train Epoch: 180 [112/486 (23%)]\tlearningLoss: 0.002934\twhole_loss: 0.016375 \n",
            "Train Epoch: 180 [128/486 (26%)]\tlearningLoss: 0.003327\twhole_loss: 0.068907 \n",
            "Train Epoch: 180 [144/486 (30%)]\tlearningLoss: 0.004155\twhole_loss: 0.144774 \n",
            "Train Epoch: 180 [160/486 (33%)]\tlearningLoss: 0.004502\twhole_loss: 0.060816 \n",
            "Train Epoch: 180 [176/486 (36%)]\tlearningLoss: 0.007224\twhole_loss: 0.476274 \n",
            "Train Epoch: 180 [192/486 (40%)]\tlearningLoss: 0.007578\twhole_loss: 0.061927 \n",
            "Train Epoch: 180 [208/486 (43%)]\tlearningLoss: 0.008758\twhole_loss: 0.206564 \n",
            "Train Epoch: 180 [224/486 (46%)]\tlearningLoss: 0.008834\twhole_loss: 0.013334 \n",
            "Train Epoch: 180 [240/486 (49%)]\tlearningLoss: 0.011222\twhole_loss: 0.417823 \n",
            "Train Epoch: 180 [256/486 (53%)]\tlearningLoss: 0.011280\twhole_loss: 0.010154 \n",
            "Train Epoch: 180 [272/486 (56%)]\tlearningLoss: 0.013875\twhole_loss: 0.454117 \n",
            "Train Epoch: 180 [288/486 (59%)]\tlearningLoss: 0.013924\twhole_loss: 0.008726 \n",
            "Train Epoch: 180 [304/486 (63%)]\tlearningLoss: 0.014235\twhole_loss: 0.054386 \n",
            "Train Epoch: 180 [320/486 (66%)]\tlearningLoss: 0.014622\twhole_loss: 0.067609 \n",
            "Train Epoch: 180 [336/486 (69%)]\tlearningLoss: 0.014828\twhole_loss: 0.036182 \n",
            "Train Epoch: 180 [352/486 (72%)]\tlearningLoss: 0.019104\twhole_loss: 0.748309 \n",
            "Train Epoch: 180 [368/486 (76%)]\tlearningLoss: 0.019381\twhole_loss: 0.048454 \n",
            "Train Epoch: 180 [384/486 (79%)]\tlearningLoss: 0.020392\twhole_loss: 0.176873 \n",
            "Train Epoch: 180 [400/486 (82%)]\tlearningLoss: 0.021794\twhole_loss: 0.245297 \n",
            "Train Epoch: 180 [416/486 (86%)]\tlearningLoss: 0.022671\twhole_loss: 0.153502 \n",
            "Train Epoch: 180 [432/486 (89%)]\tlearningLoss: 0.023004\twhole_loss: 0.058362 \n",
            "Train Epoch: 180 [448/486 (92%)]\tlearningLoss: 0.023106\twhole_loss: 0.017822 \n",
            "Train Epoch: 180 [464/486 (95%)]\tlearningLoss: 0.023126\twhole_loss: 0.003514 \n",
            "Train Epoch: 180 [180/486 (37%)]\tlearningLoss: 0.023764\twhole_loss: 0.111650 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 2, 1, 2, 2, 2, 2, 0, 2, 2, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 2], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:6.8107, Accuracy: 59/75(79%)\n",
            "\n",
            "Train Epoch: 181 [0/486 (0%)]\tlearningLoss: 0.000375\twhole_loss: 0.065655 \n",
            "Train Epoch: 181 [16/486 (3%)]\tlearningLoss: 0.000899\twhole_loss: 0.091666 \n",
            "Train Epoch: 181 [32/486 (7%)]\tlearningLoss: 0.003302\twhole_loss: 0.420478 \n",
            "Train Epoch: 181 [48/486 (10%)]\tlearningLoss: 0.003445\twhole_loss: 0.025006 \n",
            "Train Epoch: 181 [64/486 (13%)]\tlearningLoss: 0.003693\twhole_loss: 0.043520 \n",
            "Train Epoch: 181 [80/486 (16%)]\tlearningLoss: 0.006037\twhole_loss: 0.410129 \n",
            "Train Epoch: 181 [96/486 (20%)]\tlearningLoss: 0.006128\twhole_loss: 0.015992 \n",
            "Train Epoch: 181 [112/486 (23%)]\tlearningLoss: 0.006174\twhole_loss: 0.008089 \n",
            "Train Epoch: 181 [128/486 (26%)]\tlearningLoss: 0.006411\twhole_loss: 0.041423 \n",
            "Train Epoch: 181 [144/486 (30%)]\tlearningLoss: 0.006909\twhole_loss: 0.087034 \n",
            "Train Epoch: 181 [160/486 (33%)]\tlearningLoss: 0.007463\twhole_loss: 0.096980 \n",
            "Train Epoch: 181 [176/486 (36%)]\tlearningLoss: 0.007779\twhole_loss: 0.055428 \n",
            "Train Epoch: 181 [192/486 (40%)]\tlearningLoss: 0.008014\twhole_loss: 0.041137 \n",
            "Train Epoch: 181 [208/486 (43%)]\tlearningLoss: 0.008620\twhole_loss: 0.105876 \n",
            "Train Epoch: 181 [224/486 (46%)]\tlearningLoss: 0.009199\twhole_loss: 0.101427 \n",
            "Train Epoch: 181 [240/486 (49%)]\tlearningLoss: 0.009252\twhole_loss: 0.009271 \n",
            "Train Epoch: 181 [256/486 (53%)]\tlearningLoss: 0.009260\twhole_loss: 0.001379 \n",
            "Train Epoch: 181 [272/486 (56%)]\tlearningLoss: 0.009274\twhole_loss: 0.002479 \n",
            "Train Epoch: 181 [288/486 (59%)]\tlearningLoss: 0.009281\twhole_loss: 0.001174 \n",
            "Train Epoch: 181 [304/486 (63%)]\tlearningLoss: 0.009323\twhole_loss: 0.007462 \n",
            "Train Epoch: 181 [320/486 (66%)]\tlearningLoss: 0.010314\twhole_loss: 0.173424 \n",
            "Train Epoch: 181 [336/486 (69%)]\tlearningLoss: 0.010355\twhole_loss: 0.007029 \n",
            "Train Epoch: 181 [352/486 (72%)]\tlearningLoss: 0.010744\twhole_loss: 0.068070 \n",
            "Train Epoch: 181 [368/486 (76%)]\tlearningLoss: 0.010749\twhole_loss: 0.000905 \n",
            "Train Epoch: 181 [384/486 (79%)]\tlearningLoss: 0.011411\twhole_loss: 0.115837 \n",
            "Train Epoch: 181 [400/486 (82%)]\tlearningLoss: 0.011458\twhole_loss: 0.008348 \n",
            "Train Epoch: 181 [416/486 (86%)]\tlearningLoss: 0.011477\twhole_loss: 0.003336 \n",
            "Train Epoch: 181 [432/486 (89%)]\tlearningLoss: 0.011912\twhole_loss: 0.075991 \n",
            "Train Epoch: 181 [448/486 (92%)]\tlearningLoss: 0.011985\twhole_loss: 0.012750 \n",
            "Train Epoch: 181 [464/486 (95%)]\tlearningLoss: 0.012844\twhole_loss: 0.150362 \n",
            "Train Epoch: 181 [180/486 (37%)]\tlearningLoss: 0.012996\twhole_loss: 0.026668 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.3095, Accuracy: 70/75(93%)\n",
            "\n",
            "Train Epoch: 182 [0/486 (0%)]\tlearningLoss: 0.000401\twhole_loss: 0.070096 \n",
            "Train Epoch: 182 [16/486 (3%)]\tlearningLoss: 0.000452\twhole_loss: 0.008992 \n",
            "Train Epoch: 182 [32/486 (7%)]\tlearningLoss: 0.000897\twhole_loss: 0.077901 \n",
            "Train Epoch: 182 [48/486 (10%)]\tlearningLoss: 0.001033\twhole_loss: 0.023746 \n",
            "Train Epoch: 182 [64/486 (13%)]\tlearningLoss: 0.001039\twhole_loss: 0.001111 \n",
            "Train Epoch: 182 [80/486 (16%)]\tlearningLoss: 0.001512\twhole_loss: 0.082789 \n",
            "Train Epoch: 182 [96/486 (20%)]\tlearningLoss: 0.001516\twhole_loss: 0.000706 \n",
            "Train Epoch: 182 [112/486 (23%)]\tlearningLoss: 0.001598\twhole_loss: 0.014290 \n",
            "Train Epoch: 182 [128/486 (26%)]\tlearningLoss: 0.004360\twhole_loss: 0.483337 \n",
            "Train Epoch: 182 [144/486 (30%)]\tlearningLoss: 0.004670\twhole_loss: 0.054331 \n",
            "Train Epoch: 182 [160/486 (33%)]\tlearningLoss: 0.005067\twhole_loss: 0.069361 \n",
            "Train Epoch: 182 [176/486 (36%)]\tlearningLoss: 0.005083\twhole_loss: 0.002862 \n",
            "Train Epoch: 182 [192/486 (40%)]\tlearningLoss: 0.005190\twhole_loss: 0.018760 \n",
            "Train Epoch: 182 [208/486 (43%)]\tlearningLoss: 0.005541\twhole_loss: 0.061371 \n",
            "Train Epoch: 182 [224/486 (46%)]\tlearningLoss: 0.005661\twhole_loss: 0.021041 \n",
            "Train Epoch: 182 [240/486 (49%)]\tlearningLoss: 0.005739\twhole_loss: 0.013683 \n",
            "Train Epoch: 182 [256/486 (53%)]\tlearningLoss: 0.005765\twhole_loss: 0.004469 \n",
            "Train Epoch: 182 [272/486 (56%)]\tlearningLoss: 0.005908\twhole_loss: 0.025111 \n",
            "Train Epoch: 182 [288/486 (59%)]\tlearningLoss: 0.006025\twhole_loss: 0.020334 \n",
            "Train Epoch: 182 [304/486 (63%)]\tlearningLoss: 0.006231\twhole_loss: 0.036218 \n",
            "Train Epoch: 182 [320/486 (66%)]\tlearningLoss: 0.006307\twhole_loss: 0.013283 \n",
            "Train Epoch: 182 [336/486 (69%)]\tlearningLoss: 0.007001\twhole_loss: 0.121430 \n",
            "Train Epoch: 182 [352/486 (72%)]\tlearningLoss: 0.007278\twhole_loss: 0.048405 \n",
            "Train Epoch: 182 [368/486 (76%)]\tlearningLoss: 0.007381\twhole_loss: 0.018113 \n",
            "Train Epoch: 182 [384/486 (79%)]\tlearningLoss: 0.007478\twhole_loss: 0.016840 \n",
            "Train Epoch: 182 [400/486 (82%)]\tlearningLoss: 0.007599\twhole_loss: 0.021248 \n",
            "Train Epoch: 182 [416/486 (86%)]\tlearningLoss: 0.007698\twhole_loss: 0.017387 \n",
            "Train Epoch: 182 [432/486 (89%)]\tlearningLoss: 0.007826\twhole_loss: 0.022335 \n",
            "Train Epoch: 182 [448/486 (92%)]\tlearningLoss: 0.007949\twhole_loss: 0.021539 \n",
            "Train Epoch: 182 [464/486 (95%)]\tlearningLoss: 0.008106\twhole_loss: 0.027480 \n",
            "Train Epoch: 182 [180/486 (37%)]\tlearningLoss: 0.008116\twhole_loss: 0.001817 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 2], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.6611, Accuracy: 66/75(88%)\n",
            "\n",
            "Train Epoch: 183 [0/486 (0%)]\tlearningLoss: 0.000142\twhole_loss: 0.024882 \n",
            "Train Epoch: 183 [16/486 (3%)]\tlearningLoss: 0.000332\twhole_loss: 0.033165 \n",
            "Train Epoch: 183 [32/486 (7%)]\tlearningLoss: 0.000338\twhole_loss: 0.001058 \n",
            "Train Epoch: 183 [48/486 (10%)]\tlearningLoss: 0.000603\twhole_loss: 0.046411 \n",
            "Train Epoch: 183 [64/486 (13%)]\tlearningLoss: 0.001114\twhole_loss: 0.089468 \n",
            "Train Epoch: 183 [80/486 (16%)]\tlearningLoss: 0.001363\twhole_loss: 0.043539 \n",
            "Train Epoch: 183 [96/486 (20%)]\tlearningLoss: 0.001570\twhole_loss: 0.036202 \n",
            "Train Epoch: 183 [112/486 (23%)]\tlearningLoss: 0.001609\twhole_loss: 0.006794 \n",
            "Train Epoch: 183 [128/486 (26%)]\tlearningLoss: 0.001736\twhole_loss: 0.022262 \n",
            "Train Epoch: 183 [144/486 (30%)]\tlearningLoss: 0.001778\twhole_loss: 0.007320 \n",
            "Train Epoch: 183 [160/486 (33%)]\tlearningLoss: 0.001817\twhole_loss: 0.006859 \n",
            "Train Epoch: 183 [176/486 (36%)]\tlearningLoss: 0.001820\twhole_loss: 0.000477 \n",
            "Train Epoch: 183 [192/486 (40%)]\tlearningLoss: 0.001834\twhole_loss: 0.002583 \n",
            "Train Epoch: 183 [208/486 (43%)]\tlearningLoss: 0.001868\twhole_loss: 0.005840 \n",
            "Train Epoch: 183 [224/486 (46%)]\tlearningLoss: 0.001937\twhole_loss: 0.012173 \n",
            "Train Epoch: 183 [240/486 (49%)]\tlearningLoss: 0.002145\twhole_loss: 0.036298 \n",
            "Train Epoch: 183 [256/486 (53%)]\tlearningLoss: 0.002322\twhole_loss: 0.030962 \n",
            "Train Epoch: 183 [272/486 (56%)]\tlearningLoss: 0.002342\twhole_loss: 0.003582 \n",
            "Train Epoch: 183 [288/486 (59%)]\tlearningLoss: 0.002528\twhole_loss: 0.032533 \n",
            "Train Epoch: 183 [304/486 (63%)]\tlearningLoss: 0.002633\twhole_loss: 0.018427 \n",
            "Train Epoch: 183 [320/486 (66%)]\tlearningLoss: 0.002651\twhole_loss: 0.003153 \n",
            "Train Epoch: 183 [336/486 (69%)]\tlearningLoss: 0.002686\twhole_loss: 0.006085 \n",
            "Train Epoch: 183 [352/486 (72%)]\tlearningLoss: 0.003054\twhole_loss: 0.064419 \n",
            "Train Epoch: 183 [368/486 (76%)]\tlearningLoss: 0.003078\twhole_loss: 0.004079 \n",
            "Train Epoch: 183 [384/486 (79%)]\tlearningLoss: 0.003148\twhole_loss: 0.012382 \n",
            "Train Epoch: 183 [400/486 (82%)]\tlearningLoss: 0.003160\twhole_loss: 0.002009 \n",
            "Train Epoch: 183 [416/486 (86%)]\tlearningLoss: 0.003209\twhole_loss: 0.008531 \n",
            "Train Epoch: 183 [432/486 (89%)]\tlearningLoss: 0.003282\twhole_loss: 0.012823 \n",
            "Train Epoch: 183 [448/486 (92%)]\tlearningLoss: 0.003283\twhole_loss: 0.000275 \n",
            "Train Epoch: 183 [464/486 (95%)]\tlearningLoss: 0.003283\twhole_loss: 0.000002 \n",
            "Train Epoch: 183 [180/486 (37%)]\tlearningLoss: 0.003739\twhole_loss: 0.079686 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 2, 2, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.3254, Accuracy: 66/75(88%)\n",
            "\n",
            "Train Epoch: 184 [0/486 (0%)]\tlearningLoss: 0.000242\twhole_loss: 0.042354 \n",
            "Train Epoch: 184 [16/486 (3%)]\tlearningLoss: 0.000411\twhole_loss: 0.029496 \n",
            "Train Epoch: 184 [32/486 (7%)]\tlearningLoss: 0.000460\twhole_loss: 0.008568 \n",
            "Train Epoch: 184 [48/486 (10%)]\tlearningLoss: 0.000470\twhole_loss: 0.001909 \n",
            "Train Epoch: 184 [64/486 (13%)]\tlearningLoss: 0.000521\twhole_loss: 0.008897 \n",
            "Train Epoch: 184 [80/486 (16%)]\tlearningLoss: 0.000539\twhole_loss: 0.003177 \n",
            "Train Epoch: 184 [96/486 (20%)]\tlearningLoss: 0.000540\twhole_loss: 0.000154 \n",
            "Train Epoch: 184 [112/486 (23%)]\tlearningLoss: 0.000544\twhole_loss: 0.000624 \n",
            "Train Epoch: 184 [128/486 (26%)]\tlearningLoss: 0.000670\twhole_loss: 0.022094 \n",
            "Train Epoch: 184 [144/486 (30%)]\tlearningLoss: 0.000744\twhole_loss: 0.012876 \n",
            "Train Epoch: 184 [160/486 (33%)]\tlearningLoss: 0.001961\twhole_loss: 0.213057 \n",
            "Train Epoch: 184 [176/486 (36%)]\tlearningLoss: 0.002027\twhole_loss: 0.011495 \n",
            "Train Epoch: 184 [192/486 (40%)]\tlearningLoss: 0.002027\twhole_loss: 0.000067 \n",
            "Train Epoch: 184 [208/486 (43%)]\tlearningLoss: 0.002031\twhole_loss: 0.000673 \n",
            "Train Epoch: 184 [224/486 (46%)]\tlearningLoss: 0.002035\twhole_loss: 0.000738 \n",
            "Train Epoch: 184 [240/486 (49%)]\tlearningLoss: 0.002144\twhole_loss: 0.018996 \n",
            "Train Epoch: 184 [256/486 (53%)]\tlearningLoss: 0.002153\twhole_loss: 0.001639 \n",
            "Train Epoch: 184 [272/486 (56%)]\tlearningLoss: 0.002175\twhole_loss: 0.003792 \n",
            "Train Epoch: 184 [288/486 (59%)]\tlearningLoss: 0.002245\twhole_loss: 0.012324 \n",
            "Train Epoch: 184 [304/486 (63%)]\tlearningLoss: 0.002252\twhole_loss: 0.001108 \n",
            "Train Epoch: 184 [320/486 (66%)]\tlearningLoss: 0.002254\twhole_loss: 0.000442 \n",
            "Train Epoch: 184 [336/486 (69%)]\tlearningLoss: 0.002322\twhole_loss: 0.011922 \n",
            "Train Epoch: 184 [352/486 (72%)]\tlearningLoss: 0.003282\twhole_loss: 0.167949 \n",
            "Train Epoch: 184 [368/486 (76%)]\tlearningLoss: 0.003284\twhole_loss: 0.000437 \n",
            "Train Epoch: 184 [384/486 (79%)]\tlearningLoss: 0.003416\twhole_loss: 0.022987 \n",
            "Train Epoch: 184 [400/486 (82%)]\tlearningLoss: 0.003425\twhole_loss: 0.001636 \n",
            "Train Epoch: 184 [416/486 (86%)]\tlearningLoss: 0.006598\twhole_loss: 0.555228 \n",
            "Train Epoch: 184 [432/486 (89%)]\tlearningLoss: 0.006666\twhole_loss: 0.011891 \n",
            "Train Epoch: 184 [448/486 (92%)]\tlearningLoss: 0.006692\twhole_loss: 0.004549 \n",
            "Train Epoch: 184 [464/486 (95%)]\tlearningLoss: 0.006726\twhole_loss: 0.005985 \n",
            "Train Epoch: 184 [180/486 (37%)]\tlearningLoss: 0.006726\twhole_loss: 0.000056 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.2416, Accuracy: 65/75(87%)\n",
            "\n",
            "Train Epoch: 185 [0/486 (0%)]\tlearningLoss: 0.000008\twhole_loss: 0.001354 \n",
            "Train Epoch: 185 [16/486 (3%)]\tlearningLoss: 0.000042\twhole_loss: 0.006046 \n",
            "Train Epoch: 185 [32/486 (7%)]\tlearningLoss: 0.000072\twhole_loss: 0.005209 \n",
            "Train Epoch: 185 [48/486 (10%)]\tlearningLoss: 0.000133\twhole_loss: 0.010659 \n",
            "Train Epoch: 185 [64/486 (13%)]\tlearningLoss: 0.000141\twhole_loss: 0.001481 \n",
            "Train Epoch: 185 [80/486 (16%)]\tlearningLoss: 0.003890\twhole_loss: 0.655970 \n",
            "Train Epoch: 185 [96/486 (20%)]\tlearningLoss: 0.004341\twhole_loss: 0.079031 \n",
            "Train Epoch: 185 [112/486 (23%)]\tlearningLoss: 0.004379\twhole_loss: 0.006562 \n",
            "Train Epoch: 185 [128/486 (26%)]\tlearningLoss: 0.005291\twhole_loss: 0.159613 \n",
            "Train Epoch: 185 [144/486 (30%)]\tlearningLoss: 0.005352\twhole_loss: 0.010604 \n",
            "Train Epoch: 185 [160/486 (33%)]\tlearningLoss: 0.005757\twhole_loss: 0.070947 \n",
            "Train Epoch: 185 [176/486 (36%)]\tlearningLoss: 0.005950\twhole_loss: 0.033774 \n",
            "Train Epoch: 185 [192/486 (40%)]\tlearningLoss: 0.007527\twhole_loss: 0.275903 \n",
            "Train Epoch: 185 [208/486 (43%)]\tlearningLoss: 0.008507\twhole_loss: 0.171646 \n",
            "Train Epoch: 185 [224/486 (46%)]\tlearningLoss: 0.008708\twhole_loss: 0.035110 \n",
            "Train Epoch: 185 [240/486 (49%)]\tlearningLoss: 0.009392\twhole_loss: 0.119728 \n",
            "Train Epoch: 185 [256/486 (53%)]\tlearningLoss: 0.009597\twhole_loss: 0.035896 \n",
            "Train Epoch: 185 [272/486 (56%)]\tlearningLoss: 0.009638\twhole_loss: 0.007059 \n",
            "Train Epoch: 185 [288/486 (59%)]\tlearningLoss: 0.009690\twhole_loss: 0.009075 \n",
            "Train Epoch: 185 [304/486 (63%)]\tlearningLoss: 0.009716\twhole_loss: 0.004667 \n",
            "Train Epoch: 185 [320/486 (66%)]\tlearningLoss: 0.009731\twhole_loss: 0.002532 \n",
            "Train Epoch: 185 [336/486 (69%)]\tlearningLoss: 0.009754\twhole_loss: 0.004018 \n",
            "Train Epoch: 185 [352/486 (72%)]\tlearningLoss: 0.010370\twhole_loss: 0.107796 \n",
            "Train Epoch: 185 [368/486 (76%)]\tlearningLoss: 0.010681\twhole_loss: 0.054495 \n",
            "Train Epoch: 185 [384/486 (79%)]\tlearningLoss: 0.010690\twhole_loss: 0.001630 \n",
            "Train Epoch: 185 [400/486 (82%)]\tlearningLoss: 0.011976\twhole_loss: 0.224960 \n",
            "Train Epoch: 185 [416/486 (86%)]\tlearningLoss: 0.012011\twhole_loss: 0.006183 \n",
            "Train Epoch: 185 [432/486 (89%)]\tlearningLoss: 0.012095\twhole_loss: 0.014715 \n",
            "Train Epoch: 185 [448/486 (92%)]\tlearningLoss: 0.012720\twhole_loss: 0.109399 \n",
            "Train Epoch: 185 [464/486 (95%)]\tlearningLoss: 0.014585\twhole_loss: 0.326312 \n",
            "Train Epoch: 185 [180/486 (37%)]\tlearningLoss: 0.014889\twhole_loss: 0.053143 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.7598, Accuracy: 67/75(89%)\n",
            "\n",
            "Train Epoch: 186 [0/486 (0%)]\tlearningLoss: 0.000294\twhole_loss: 0.051378 \n",
            "Train Epoch: 186 [16/486 (3%)]\tlearningLoss: 0.000836\twhole_loss: 0.094966 \n",
            "Train Epoch: 186 [32/486 (7%)]\tlearningLoss: 0.001114\twhole_loss: 0.048600 \n",
            "Train Epoch: 186 [48/486 (10%)]\tlearningLoss: 0.001286\twhole_loss: 0.030156 \n",
            "Train Epoch: 186 [64/486 (13%)]\tlearningLoss: 0.001323\twhole_loss: 0.006467 \n",
            "Train Epoch: 186 [80/486 (16%)]\tlearningLoss: 0.001515\twhole_loss: 0.033535 \n",
            "Train Epoch: 186 [96/486 (20%)]\tlearningLoss: 0.001516\twhole_loss: 0.000231 \n",
            "Train Epoch: 186 [112/486 (23%)]\tlearningLoss: 0.002057\twhole_loss: 0.094594 \n",
            "Train Epoch: 186 [128/486 (26%)]\tlearningLoss: 0.002209\twhole_loss: 0.026730 \n",
            "Train Epoch: 186 [144/486 (30%)]\tlearningLoss: 0.002235\twhole_loss: 0.004399 \n",
            "Train Epoch: 186 [160/486 (33%)]\tlearningLoss: 0.002313\twhole_loss: 0.013747 \n",
            "Train Epoch: 186 [176/486 (36%)]\tlearningLoss: 0.002340\twhole_loss: 0.004678 \n",
            "Train Epoch: 186 [192/486 (40%)]\tlearningLoss: 0.006616\twhole_loss: 0.748245 \n",
            "Train Epoch: 186 [208/486 (43%)]\tlearningLoss: 0.006654\twhole_loss: 0.006785 \n",
            "Train Epoch: 186 [224/486 (46%)]\tlearningLoss: 0.006775\twhole_loss: 0.021122 \n",
            "Train Epoch: 186 [240/486 (49%)]\tlearningLoss: 0.007436\twhole_loss: 0.115722 \n",
            "Train Epoch: 186 [256/486 (53%)]\tlearningLoss: 0.008826\twhole_loss: 0.243277 \n",
            "Train Epoch: 186 [272/486 (56%)]\tlearningLoss: 0.008930\twhole_loss: 0.018092 \n",
            "Train Epoch: 186 [288/486 (59%)]\tlearningLoss: 0.009073\twhole_loss: 0.025066 \n",
            "Train Epoch: 186 [304/486 (63%)]\tlearningLoss: 0.009140\twhole_loss: 0.011629 \n",
            "Train Epoch: 186 [320/486 (66%)]\tlearningLoss: 0.009169\twhole_loss: 0.005235 \n",
            "Train Epoch: 186 [336/486 (69%)]\tlearningLoss: 0.009180\twhole_loss: 0.001845 \n",
            "Train Epoch: 186 [352/486 (72%)]\tlearningLoss: 0.009216\twhole_loss: 0.006266 \n",
            "Train Epoch: 186 [368/486 (76%)]\tlearningLoss: 0.009265\twhole_loss: 0.008527 \n",
            "Train Epoch: 186 [384/486 (79%)]\tlearningLoss: 0.009292\twhole_loss: 0.004812 \n",
            "Train Epoch: 186 [400/486 (82%)]\tlearningLoss: 0.009298\twhole_loss: 0.001124 \n",
            "Train Epoch: 186 [416/486 (86%)]\tlearningLoss: 0.009766\twhole_loss: 0.081775 \n",
            "Train Epoch: 186 [432/486 (89%)]\tlearningLoss: 0.011147\twhole_loss: 0.241683 \n",
            "Train Epoch: 186 [448/486 (92%)]\tlearningLoss: 0.011202\twhole_loss: 0.009628 \n",
            "Train Epoch: 186 [464/486 (95%)]\tlearningLoss: 0.011332\twhole_loss: 0.022762 \n",
            "Train Epoch: 186 [180/486 (37%)]\tlearningLoss: 0.012027\twhole_loss: 0.121572 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 2, 2, 1, 1, 2, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.0747, Accuracy: 66/75(88%)\n",
            "\n",
            "Train Epoch: 187 [0/486 (0%)]\tlearningLoss: 0.000662\twhole_loss: 0.115922 \n",
            "Train Epoch: 187 [16/486 (3%)]\tlearningLoss: 0.000672\twhole_loss: 0.001650 \n",
            "Train Epoch: 187 [32/486 (7%)]\tlearningLoss: 0.001257\twhole_loss: 0.102327 \n",
            "Train Epoch: 187 [48/486 (10%)]\tlearningLoss: 0.001538\twhole_loss: 0.049307 \n",
            "Train Epoch: 187 [64/486 (13%)]\tlearningLoss: 0.001577\twhole_loss: 0.006795 \n",
            "Train Epoch: 187 [80/486 (16%)]\tlearningLoss: 0.001662\twhole_loss: 0.014774 \n",
            "Train Epoch: 187 [96/486 (20%)]\tlearningLoss: 0.001920\twhole_loss: 0.045251 \n",
            "Train Epoch: 187 [112/486 (23%)]\tlearningLoss: 0.002567\twhole_loss: 0.113229 \n",
            "Train Epoch: 187 [128/486 (26%)]\tlearningLoss: 0.003397\twhole_loss: 0.145279 \n",
            "Train Epoch: 187 [144/486 (30%)]\tlearningLoss: 0.003915\twhole_loss: 0.090554 \n",
            "Train Epoch: 187 [160/486 (33%)]\tlearningLoss: 0.004051\twhole_loss: 0.023820 \n",
            "Train Epoch: 187 [176/486 (36%)]\tlearningLoss: 0.004244\twhole_loss: 0.033796 \n",
            "Train Epoch: 187 [192/486 (40%)]\tlearningLoss: 0.007017\twhole_loss: 0.485317 \n",
            "Train Epoch: 187 [208/486 (43%)]\tlearningLoss: 0.007651\twhole_loss: 0.110972 \n",
            "Train Epoch: 187 [224/486 (46%)]\tlearningLoss: 0.007996\twhole_loss: 0.060253 \n",
            "Train Epoch: 187 [240/486 (49%)]\tlearningLoss: 0.008002\twhole_loss: 0.001088 \n",
            "Train Epoch: 187 [256/486 (53%)]\tlearningLoss: 0.008070\twhole_loss: 0.011932 \n",
            "Train Epoch: 187 [272/486 (56%)]\tlearningLoss: 0.008200\twhole_loss: 0.022652 \n",
            "Train Epoch: 187 [288/486 (59%)]\tlearningLoss: 0.008585\twhole_loss: 0.067369 \n",
            "Train Epoch: 187 [304/486 (63%)]\tlearningLoss: 0.008602\twhole_loss: 0.003026 \n",
            "Train Epoch: 187 [320/486 (66%)]\tlearningLoss: 0.009325\twhole_loss: 0.126519 \n",
            "Train Epoch: 187 [336/486 (69%)]\tlearningLoss: 0.009590\twhole_loss: 0.046352 \n",
            "Train Epoch: 187 [352/486 (72%)]\tlearningLoss: 0.009853\twhole_loss: 0.046050 \n",
            "Train Epoch: 187 [368/486 (76%)]\tlearningLoss: 0.010452\twhole_loss: 0.104952 \n",
            "Train Epoch: 187 [384/486 (79%)]\tlearningLoss: 0.010841\twhole_loss: 0.068003 \n",
            "Train Epoch: 187 [400/486 (82%)]\tlearningLoss: 0.010973\twhole_loss: 0.023110 \n",
            "Train Epoch: 187 [416/486 (86%)]\tlearningLoss: 0.011008\twhole_loss: 0.006064 \n",
            "Train Epoch: 187 [432/486 (89%)]\tlearningLoss: 0.011114\twhole_loss: 0.018642 \n",
            "Train Epoch: 187 [448/486 (92%)]\tlearningLoss: 0.011830\twhole_loss: 0.125158 \n",
            "Train Epoch: 187 [464/486 (95%)]\tlearningLoss: 0.011907\twhole_loss: 0.013552 \n",
            "Train Epoch: 187 [180/486 (37%)]\tlearningLoss: 0.011930\twhole_loss: 0.004013 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 2, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.5738, Accuracy: 68/75(91%)\n",
            "\n",
            "Train Epoch: 188 [0/486 (0%)]\tlearningLoss: 0.000470\twhole_loss: 0.082265 \n",
            "Train Epoch: 188 [16/486 (3%)]\tlearningLoss: 0.000663\twhole_loss: 0.033727 \n",
            "Train Epoch: 188 [32/486 (7%)]\tlearningLoss: 0.000871\twhole_loss: 0.036375 \n",
            "Train Epoch: 188 [48/486 (10%)]\tlearningLoss: 0.003591\twhole_loss: 0.476051 \n",
            "Train Epoch: 188 [64/486 (13%)]\tlearningLoss: 0.003602\twhole_loss: 0.001934 \n",
            "Train Epoch: 188 [80/486 (16%)]\tlearningLoss: 0.003696\twhole_loss: 0.016371 \n",
            "Train Epoch: 188 [96/486 (20%)]\tlearningLoss: 0.003883\twhole_loss: 0.032883 \n",
            "Train Epoch: 188 [112/486 (23%)]\tlearningLoss: 0.004897\twhole_loss: 0.177445 \n",
            "Train Epoch: 188 [128/486 (26%)]\tlearningLoss: 0.005501\twhole_loss: 0.105690 \n",
            "Train Epoch: 188 [144/486 (30%)]\tlearningLoss: 0.006642\twhole_loss: 0.199650 \n",
            "Train Epoch: 188 [160/486 (33%)]\tlearningLoss: 0.006752\twhole_loss: 0.019190 \n",
            "Train Epoch: 188 [176/486 (36%)]\tlearningLoss: 0.006839\twhole_loss: 0.015325 \n",
            "Train Epoch: 188 [192/486 (40%)]\tlearningLoss: 0.007070\twhole_loss: 0.040368 \n",
            "Train Epoch: 188 [208/486 (43%)]\tlearningLoss: 0.007198\twhole_loss: 0.022377 \n",
            "Train Epoch: 188 [224/486 (46%)]\tlearningLoss: 0.008378\twhole_loss: 0.206531 \n",
            "Train Epoch: 188 [240/486 (49%)]\tlearningLoss: 0.008468\twhole_loss: 0.015683 \n",
            "Train Epoch: 188 [256/486 (53%)]\tlearningLoss: 0.008583\twhole_loss: 0.020212 \n",
            "Train Epoch: 188 [272/486 (56%)]\tlearningLoss: 0.009091\twhole_loss: 0.088832 \n",
            "Train Epoch: 188 [288/486 (59%)]\tlearningLoss: 0.009273\twhole_loss: 0.031850 \n",
            "Train Epoch: 188 [304/486 (63%)]\tlearningLoss: 0.009701\twhole_loss: 0.074831 \n",
            "Train Epoch: 188 [320/486 (66%)]\tlearningLoss: 0.009841\twhole_loss: 0.024503 \n",
            "Train Epoch: 188 [336/486 (69%)]\tlearningLoss: 0.010175\twhole_loss: 0.058517 \n",
            "Train Epoch: 188 [352/486 (72%)]\tlearningLoss: 0.011572\twhole_loss: 0.244551 \n",
            "Train Epoch: 188 [368/486 (76%)]\tlearningLoss: 0.011852\twhole_loss: 0.049011 \n",
            "Train Epoch: 188 [384/486 (79%)]\tlearningLoss: 0.012412\twhole_loss: 0.097898 \n",
            "Train Epoch: 188 [400/486 (82%)]\tlearningLoss: 0.014031\twhole_loss: 0.283396 \n",
            "Train Epoch: 188 [416/486 (86%)]\tlearningLoss: 0.014094\twhole_loss: 0.010988 \n",
            "Train Epoch: 188 [432/486 (89%)]\tlearningLoss: 0.014331\twhole_loss: 0.041520 \n",
            "Train Epoch: 188 [448/486 (92%)]\tlearningLoss: 0.014371\twhole_loss: 0.006891 \n",
            "Train Epoch: 188 [464/486 (95%)]\tlearningLoss: 0.014444\twhole_loss: 0.012778 \n",
            "Train Epoch: 188 [180/486 (37%)]\tlearningLoss: 0.014470\twhole_loss: 0.004692 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 2, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 2], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:5.2742, Accuracy: 65/75(87%)\n",
            "\n",
            "Train Epoch: 189 [0/486 (0%)]\tlearningLoss: 0.000227\twhole_loss: 0.039753 \n",
            "Train Epoch: 189 [16/486 (3%)]\tlearningLoss: 0.000550\twhole_loss: 0.056557 \n",
            "Train Epoch: 189 [32/486 (7%)]\tlearningLoss: 0.000591\twhole_loss: 0.007174 \n",
            "Train Epoch: 189 [48/486 (10%)]\tlearningLoss: 0.000787\twhole_loss: 0.034255 \n",
            "Train Epoch: 189 [64/486 (13%)]\tlearningLoss: 0.001003\twhole_loss: 0.037791 \n",
            "Train Epoch: 189 [80/486 (16%)]\tlearningLoss: 0.001012\twhole_loss: 0.001628 \n",
            "Train Epoch: 189 [96/486 (20%)]\tlearningLoss: 0.001647\twhole_loss: 0.111075 \n",
            "Train Epoch: 189 [112/486 (23%)]\tlearningLoss: 0.001803\twhole_loss: 0.027323 \n",
            "Train Epoch: 189 [128/486 (26%)]\tlearningLoss: 0.002133\twhole_loss: 0.057756 \n",
            "Train Epoch: 189 [144/486 (30%)]\tlearningLoss: 0.004438\twhole_loss: 0.403405 \n",
            "Train Epoch: 189 [160/486 (33%)]\tlearningLoss: 0.004862\twhole_loss: 0.074136 \n",
            "Train Epoch: 189 [176/486 (36%)]\tlearningLoss: 0.004869\twhole_loss: 0.001232 \n",
            "Train Epoch: 189 [192/486 (40%)]\tlearningLoss: 0.004909\twhole_loss: 0.007067 \n",
            "Train Epoch: 189 [208/486 (43%)]\tlearningLoss: 0.004937\twhole_loss: 0.004844 \n",
            "Train Epoch: 189 [224/486 (46%)]\tlearningLoss: 0.004957\twhole_loss: 0.003420 \n",
            "Train Epoch: 189 [240/486 (49%)]\tlearningLoss: 0.004958\twhole_loss: 0.000281 \n",
            "Train Epoch: 189 [256/486 (53%)]\tlearningLoss: 0.006269\twhole_loss: 0.229316 \n",
            "Train Epoch: 189 [272/486 (56%)]\tlearningLoss: 0.006904\twhole_loss: 0.111250 \n",
            "Train Epoch: 189 [288/486 (59%)]\tlearningLoss: 0.007109\twhole_loss: 0.035851 \n",
            "Train Epoch: 189 [304/486 (63%)]\tlearningLoss: 0.007177\twhole_loss: 0.011823 \n",
            "Train Epoch: 189 [320/486 (66%)]\tlearningLoss: 0.007475\twhole_loss: 0.052188 \n",
            "Train Epoch: 189 [336/486 (69%)]\tlearningLoss: 0.007573\twhole_loss: 0.017223 \n",
            "Train Epoch: 189 [352/486 (72%)]\tlearningLoss: 0.007581\twhole_loss: 0.001342 \n",
            "Train Epoch: 189 [368/486 (76%)]\tlearningLoss: 0.007736\twhole_loss: 0.027031 \n",
            "Train Epoch: 189 [384/486 (79%)]\tlearningLoss: 0.008146\twhole_loss: 0.071874 \n",
            "Train Epoch: 189 [400/486 (82%)]\tlearningLoss: 0.008207\twhole_loss: 0.010572 \n",
            "Train Epoch: 189 [416/486 (86%)]\tlearningLoss: 0.008242\twhole_loss: 0.006146 \n",
            "Train Epoch: 189 [432/486 (89%)]\tlearningLoss: 0.008636\twhole_loss: 0.069000 \n",
            "Train Epoch: 189 [448/486 (92%)]\tlearningLoss: 0.008926\twhole_loss: 0.050733 \n",
            "Train Epoch: 189 [464/486 (95%)]\tlearningLoss: 0.009305\twhole_loss: 0.066246 \n",
            "Train Epoch: 189 [180/486 (37%)]\tlearningLoss: 0.009734\twhole_loss: 0.075201 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 2, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 2, 0, 0, 0, 2, 2, 0, 2], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.5934, Accuracy: 64/75(85%)\n",
            "\n",
            "Train Epoch: 190 [0/486 (0%)]\tlearningLoss: 0.000026\twhole_loss: 0.004574 \n",
            "Train Epoch: 190 [16/486 (3%)]\tlearningLoss: 0.000235\twhole_loss: 0.036626 \n",
            "Train Epoch: 190 [32/486 (7%)]\tlearningLoss: 0.000239\twhole_loss: 0.000598 \n",
            "Train Epoch: 190 [48/486 (10%)]\tlearningLoss: 0.000239\twhole_loss: 0.000045 \n",
            "Train Epoch: 190 [64/486 (13%)]\tlearningLoss: 0.000240\twhole_loss: 0.000074 \n",
            "Train Epoch: 190 [80/486 (16%)]\tlearningLoss: 0.000268\twhole_loss: 0.004914 \n",
            "Train Epoch: 190 [96/486 (20%)]\tlearningLoss: 0.000286\twhole_loss: 0.003264 \n",
            "Train Epoch: 190 [112/486 (23%)]\tlearningLoss: 0.001329\twhole_loss: 0.182487 \n",
            "Train Epoch: 190 [128/486 (26%)]\tlearningLoss: 0.001402\twhole_loss: 0.012700 \n",
            "Train Epoch: 190 [144/486 (30%)]\tlearningLoss: 0.001414\twhole_loss: 0.002255 \n",
            "Train Epoch: 190 [160/486 (33%)]\tlearningLoss: 0.001416\twhole_loss: 0.000281 \n",
            "Train Epoch: 190 [176/486 (36%)]\tlearningLoss: 0.001444\twhole_loss: 0.004812 \n",
            "Train Epoch: 190 [192/486 (40%)]\tlearningLoss: 0.001589\twhole_loss: 0.025384 \n",
            "Train Epoch: 190 [208/486 (43%)]\tlearningLoss: 0.002442\twhole_loss: 0.149380 \n",
            "Train Epoch: 190 [224/486 (46%)]\tlearningLoss: 0.002476\twhole_loss: 0.005900 \n",
            "Train Epoch: 190 [240/486 (49%)]\tlearningLoss: 0.002591\twhole_loss: 0.020208 \n",
            "Train Epoch: 190 [256/486 (53%)]\tlearningLoss: 0.002682\twhole_loss: 0.015931 \n",
            "Train Epoch: 190 [272/486 (56%)]\tlearningLoss: 0.003865\twhole_loss: 0.206880 \n",
            "Train Epoch: 190 [288/486 (59%)]\tlearningLoss: 0.003967\twhole_loss: 0.017885 \n",
            "Train Epoch: 190 [304/486 (63%)]\tlearningLoss: 0.004066\twhole_loss: 0.017409 \n",
            "Train Epoch: 190 [320/486 (66%)]\tlearningLoss: 0.004269\twhole_loss: 0.035459 \n",
            "Train Epoch: 190 [336/486 (69%)]\tlearningLoss: 0.004896\twhole_loss: 0.109750 \n",
            "Train Epoch: 190 [352/486 (72%)]\tlearningLoss: 0.006091\twhole_loss: 0.209083 \n",
            "Train Epoch: 190 [368/486 (76%)]\tlearningLoss: 0.006306\twhole_loss: 0.037600 \n",
            "Train Epoch: 190 [384/486 (79%)]\tlearningLoss: 0.006596\twhole_loss: 0.050791 \n",
            "Train Epoch: 190 [400/486 (82%)]\tlearningLoss: 0.006793\twhole_loss: 0.034410 \n",
            "Train Epoch: 190 [416/486 (86%)]\tlearningLoss: 0.007138\twhole_loss: 0.060377 \n",
            "Train Epoch: 190 [432/486 (89%)]\tlearningLoss: 0.007267\twhole_loss: 0.022599 \n",
            "Train Epoch: 190 [448/486 (92%)]\tlearningLoss: 0.007313\twhole_loss: 0.008144 \n",
            "Train Epoch: 190 [464/486 (95%)]\tlearningLoss: 0.007403\twhole_loss: 0.015724 \n",
            "Train Epoch: 190 [180/486 (37%)]\tlearningLoss: 0.007569\twhole_loss: 0.029093 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.1545, Accuracy: 68/75(91%)\n",
            "\n",
            "Train Epoch: 191 [0/486 (0%)]\tlearningLoss: 0.000670\twhole_loss: 0.117248 \n",
            "Train Epoch: 191 [16/486 (3%)]\tlearningLoss: 0.000709\twhole_loss: 0.006778 \n",
            "Train Epoch: 191 [32/486 (7%)]\tlearningLoss: 0.000723\twhole_loss: 0.002423 \n",
            "Train Epoch: 191 [48/486 (10%)]\tlearningLoss: 0.000773\twhole_loss: 0.008876 \n",
            "Train Epoch: 191 [64/486 (13%)]\tlearningLoss: 0.000782\twhole_loss: 0.001466 \n",
            "Train Epoch: 191 [80/486 (16%)]\tlearningLoss: 0.001241\twhole_loss: 0.080437 \n",
            "Train Epoch: 191 [96/486 (20%)]\tlearningLoss: 0.001778\twhole_loss: 0.093838 \n",
            "Train Epoch: 191 [112/486 (23%)]\tlearningLoss: 0.001816\twhole_loss: 0.006795 \n",
            "Train Epoch: 191 [128/486 (26%)]\tlearningLoss: 0.001851\twhole_loss: 0.006149 \n",
            "Train Epoch: 191 [144/486 (30%)]\tlearningLoss: 0.001860\twhole_loss: 0.001445 \n",
            "Train Epoch: 191 [160/486 (33%)]\tlearningLoss: 0.001936\twhole_loss: 0.013371 \n",
            "Train Epoch: 191 [176/486 (36%)]\tlearningLoss: 0.001995\twhole_loss: 0.010293 \n",
            "Train Epoch: 191 [192/486 (40%)]\tlearningLoss: 0.001998\twhole_loss: 0.000575 \n",
            "Train Epoch: 191 [208/486 (43%)]\tlearningLoss: 0.002008\twhole_loss: 0.001715 \n",
            "Train Epoch: 191 [224/486 (46%)]\tlearningLoss: 0.002177\twhole_loss: 0.029615 \n",
            "Train Epoch: 191 [240/486 (49%)]\tlearningLoss: 0.002197\twhole_loss: 0.003371 \n",
            "Train Epoch: 191 [256/486 (53%)]\tlearningLoss: 0.002753\twhole_loss: 0.097407 \n",
            "Train Epoch: 191 [272/486 (56%)]\tlearningLoss: 0.002777\twhole_loss: 0.004188 \n",
            "Train Epoch: 191 [288/486 (59%)]\tlearningLoss: 0.002782\twhole_loss: 0.000818 \n",
            "Train Epoch: 191 [304/486 (63%)]\tlearningLoss: 0.003168\twhole_loss: 0.067520 \n",
            "Train Epoch: 191 [320/486 (66%)]\tlearningLoss: 0.003169\twhole_loss: 0.000270 \n",
            "Train Epoch: 191 [336/486 (69%)]\tlearningLoss: 0.003222\twhole_loss: 0.009293 \n",
            "Train Epoch: 191 [352/486 (72%)]\tlearningLoss: 0.003255\twhole_loss: 0.005654 \n",
            "Train Epoch: 191 [368/486 (76%)]\tlearningLoss: 0.003301\twhole_loss: 0.008144 \n",
            "Train Epoch: 191 [384/486 (79%)]\tlearningLoss: 0.003303\twhole_loss: 0.000402 \n",
            "Train Epoch: 191 [400/486 (82%)]\tlearningLoss: 0.003306\twhole_loss: 0.000490 \n",
            "Train Epoch: 191 [416/486 (86%)]\tlearningLoss: 0.003306\twhole_loss: 0.000047 \n",
            "Train Epoch: 191 [432/486 (89%)]\tlearningLoss: 0.003330\twhole_loss: 0.004044 \n",
            "Train Epoch: 191 [448/486 (92%)]\tlearningLoss: 0.003340\twhole_loss: 0.001901 \n",
            "Train Epoch: 191 [464/486 (95%)]\tlearningLoss: 0.003422\twhole_loss: 0.014267 \n",
            "Train Epoch: 191 [180/486 (37%)]\tlearningLoss: 0.005101\twhole_loss: 0.293788 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 2, 2, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:5.1667, Accuracy: 65/75(87%)\n",
            "\n",
            "Train Epoch: 192 [0/486 (0%)]\tlearningLoss: 0.000040\twhole_loss: 0.006978 \n",
            "Train Epoch: 192 [16/486 (3%)]\tlearningLoss: 0.000044\twhole_loss: 0.000670 \n",
            "Train Epoch: 192 [32/486 (7%)]\tlearningLoss: 0.000227\twhole_loss: 0.031990 \n",
            "Train Epoch: 192 [48/486 (10%)]\tlearningLoss: 0.000287\twhole_loss: 0.010533 \n",
            "Train Epoch: 192 [64/486 (13%)]\tlearningLoss: 0.000595\twhole_loss: 0.054026 \n",
            "Train Epoch: 192 [80/486 (16%)]\tlearningLoss: 0.001154\twhole_loss: 0.097672 \n",
            "Train Epoch: 192 [96/486 (20%)]\tlearningLoss: 0.001217\twhole_loss: 0.011020 \n",
            "Train Epoch: 192 [112/486 (23%)]\tlearningLoss: 0.001419\twhole_loss: 0.035520 \n",
            "Train Epoch: 192 [128/486 (26%)]\tlearningLoss: 0.001456\twhole_loss: 0.006420 \n",
            "Train Epoch: 192 [144/486 (30%)]\tlearningLoss: 0.001458\twhole_loss: 0.000353 \n",
            "Train Epoch: 192 [160/486 (33%)]\tlearningLoss: 0.001579\twhole_loss: 0.021183 \n",
            "Train Epoch: 192 [176/486 (36%)]\tlearningLoss: 0.003247\twhole_loss: 0.291809 \n",
            "Train Epoch: 192 [192/486 (40%)]\tlearningLoss: 0.003251\twhole_loss: 0.000730 \n",
            "Train Epoch: 192 [208/486 (43%)]\tlearningLoss: 0.003274\twhole_loss: 0.003976 \n",
            "Train Epoch: 192 [224/486 (46%)]\tlearningLoss: 0.003297\twhole_loss: 0.004126 \n",
            "Train Epoch: 192 [240/486 (49%)]\tlearningLoss: 0.003395\twhole_loss: 0.017145 \n",
            "Train Epoch: 192 [256/486 (53%)]\tlearningLoss: 0.003442\twhole_loss: 0.008271 \n",
            "Train Epoch: 192 [272/486 (56%)]\tlearningLoss: 0.003709\twhole_loss: 0.046675 \n",
            "Train Epoch: 192 [288/486 (59%)]\tlearningLoss: 0.003786\twhole_loss: 0.013540 \n",
            "Train Epoch: 192 [304/486 (63%)]\tlearningLoss: 0.003787\twhole_loss: 0.000033 \n",
            "Train Epoch: 192 [320/486 (66%)]\tlearningLoss: 0.003808\twhole_loss: 0.003751 \n",
            "Train Epoch: 192 [336/486 (69%)]\tlearningLoss: 0.003839\twhole_loss: 0.005318 \n",
            "Train Epoch: 192 [352/486 (72%)]\tlearningLoss: 0.004499\twhole_loss: 0.115567 \n",
            "Train Epoch: 192 [368/486 (76%)]\tlearningLoss: 0.004506\twhole_loss: 0.001242 \n",
            "Train Epoch: 192 [384/486 (79%)]\tlearningLoss: 0.005032\twhole_loss: 0.092124 \n",
            "Train Epoch: 192 [400/486 (82%)]\tlearningLoss: 0.006593\twhole_loss: 0.273030 \n",
            "Train Epoch: 192 [416/486 (86%)]\tlearningLoss: 0.007184\twhole_loss: 0.103551 \n",
            "Train Epoch: 192 [432/486 (89%)]\tlearningLoss: 0.007189\twhole_loss: 0.000764 \n",
            "Train Epoch: 192 [448/486 (92%)]\tlearningLoss: 0.007196\twhole_loss: 0.001222 \n",
            "Train Epoch: 192 [464/486 (95%)]\tlearningLoss: 0.007203\twhole_loss: 0.001271 \n",
            "Train Epoch: 192 [180/486 (37%)]\tlearningLoss: 0.007208\twhole_loss: 0.000840 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.1153, Accuracy: 69/75(92%)\n",
            "\n",
            "Train Epoch: 193 [0/486 (0%)]\tlearningLoss: 0.000000\twhole_loss: 0.000059 \n",
            "Train Epoch: 193 [16/486 (3%)]\tlearningLoss: 0.000114\twhole_loss: 0.019832 \n",
            "Train Epoch: 193 [32/486 (7%)]\tlearningLoss: 0.000542\twhole_loss: 0.074956 \n",
            "Train Epoch: 193 [48/486 (10%)]\tlearningLoss: 0.000547\twhole_loss: 0.000815 \n",
            "Train Epoch: 193 [64/486 (13%)]\tlearningLoss: 0.001412\twhole_loss: 0.151466 \n",
            "Train Epoch: 193 [80/486 (16%)]\tlearningLoss: 0.001763\twhole_loss: 0.061454 \n",
            "Train Epoch: 193 [96/486 (20%)]\tlearningLoss: 0.002722\twhole_loss: 0.167844 \n",
            "Train Epoch: 193 [112/486 (23%)]\tlearningLoss: 0.002998\twhole_loss: 0.048162 \n",
            "Train Epoch: 193 [128/486 (26%)]\tlearningLoss: 0.003001\twhole_loss: 0.000510 \n",
            "Train Epoch: 193 [144/486 (30%)]\tlearningLoss: 0.003002\twhole_loss: 0.000167 \n",
            "Train Epoch: 193 [160/486 (33%)]\tlearningLoss: 0.003374\twhole_loss: 0.065258 \n",
            "Train Epoch: 193 [176/486 (36%)]\tlearningLoss: 0.004135\twhole_loss: 0.133075 \n",
            "Train Epoch: 193 [192/486 (40%)]\tlearningLoss: 0.004819\twhole_loss: 0.119660 \n",
            "Train Epoch: 193 [208/486 (43%)]\tlearningLoss: 0.004976\twhole_loss: 0.027573 \n",
            "Train Epoch: 193 [224/486 (46%)]\tlearningLoss: 0.005645\twhole_loss: 0.117106 \n",
            "Train Epoch: 193 [240/486 (49%)]\tlearningLoss: 0.005703\twhole_loss: 0.010155 \n",
            "Train Epoch: 193 [256/486 (53%)]\tlearningLoss: 0.005804\twhole_loss: 0.017595 \n",
            "Train Epoch: 193 [272/486 (56%)]\tlearningLoss: 0.005828\twhole_loss: 0.004185 \n",
            "Train Epoch: 193 [288/486 (59%)]\tlearningLoss: 0.005854\twhole_loss: 0.004530 \n",
            "Train Epoch: 193 [304/486 (63%)]\tlearningLoss: 0.005943\twhole_loss: 0.015538 \n",
            "Train Epoch: 193 [320/486 (66%)]\tlearningLoss: 0.006026\twhole_loss: 0.014641 \n",
            "Train Epoch: 193 [336/486 (69%)]\tlearningLoss: 0.006129\twhole_loss: 0.017915 \n",
            "Train Epoch: 193 [352/486 (72%)]\tlearningLoss: 0.006139\twhole_loss: 0.001904 \n",
            "Train Epoch: 193 [368/486 (76%)]\tlearningLoss: 0.006445\twhole_loss: 0.053400 \n",
            "Train Epoch: 193 [384/486 (79%)]\tlearningLoss: 0.006453\twhole_loss: 0.001410 \n",
            "Train Epoch: 193 [400/486 (82%)]\tlearningLoss: 0.006508\twhole_loss: 0.009658 \n",
            "Train Epoch: 193 [416/486 (86%)]\tlearningLoss: 0.006841\twhole_loss: 0.058255 \n",
            "Train Epoch: 193 [432/486 (89%)]\tlearningLoss: 0.006944\twhole_loss: 0.018068 \n",
            "Train Epoch: 193 [448/486 (92%)]\tlearningLoss: 0.007308\twhole_loss: 0.063697 \n",
            "Train Epoch: 193 [464/486 (95%)]\tlearningLoss: 0.007354\twhole_loss: 0.008004 \n",
            "Train Epoch: 193 [180/486 (37%)]\tlearningLoss: 0.008255\twhole_loss: 0.157806 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.3593, Accuracy: 60/75(80%)\n",
            "\n",
            "Train Epoch: 194 [0/486 (0%)]\tlearningLoss: 0.000247\twhole_loss: 0.043212 \n",
            "Train Epoch: 194 [16/486 (3%)]\tlearningLoss: 0.000432\twhole_loss: 0.032338 \n",
            "Train Epoch: 194 [32/486 (7%)]\tlearningLoss: 0.000606\twhole_loss: 0.030552 \n",
            "Train Epoch: 194 [48/486 (10%)]\tlearningLoss: 0.001059\twhole_loss: 0.079307 \n",
            "Train Epoch: 194 [64/486 (13%)]\tlearningLoss: 0.001087\twhole_loss: 0.004867 \n",
            "Train Epoch: 194 [80/486 (16%)]\tlearningLoss: 0.001873\twhole_loss: 0.137496 \n",
            "Train Epoch: 194 [96/486 (20%)]\tlearningLoss: 0.001965\twhole_loss: 0.016183 \n",
            "Train Epoch: 194 [112/486 (23%)]\tlearningLoss: 0.001973\twhole_loss: 0.001369 \n",
            "Train Epoch: 194 [128/486 (26%)]\tlearningLoss: 0.002572\twhole_loss: 0.104697 \n",
            "Train Epoch: 194 [144/486 (30%)]\tlearningLoss: 0.004253\twhole_loss: 0.294267 \n",
            "Train Epoch: 194 [160/486 (33%)]\tlearningLoss: 0.004363\twhole_loss: 0.019268 \n",
            "Train Epoch: 194 [176/486 (36%)]\tlearningLoss: 0.004453\twhole_loss: 0.015686 \n",
            "Train Epoch: 194 [192/486 (40%)]\tlearningLoss: 0.004453\twhole_loss: 0.000065 \n",
            "Train Epoch: 194 [208/486 (43%)]\tlearningLoss: 0.004638\twhole_loss: 0.032359 \n",
            "Train Epoch: 194 [224/486 (46%)]\tlearningLoss: 0.004730\twhole_loss: 0.016079 \n",
            "Train Epoch: 194 [240/486 (49%)]\tlearningLoss: 0.005493\twhole_loss: 0.133462 \n",
            "Train Epoch: 194 [256/486 (53%)]\tlearningLoss: 0.005882\twhole_loss: 0.068128 \n",
            "Train Epoch: 194 [272/486 (56%)]\tlearningLoss: 0.006167\twhole_loss: 0.049971 \n",
            "Train Epoch: 194 [288/486 (59%)]\tlearningLoss: 0.006248\twhole_loss: 0.014031 \n",
            "Train Epoch: 194 [304/486 (63%)]\tlearningLoss: 0.006305\twhole_loss: 0.009988 \n",
            "Train Epoch: 194 [320/486 (66%)]\tlearningLoss: 0.008557\twhole_loss: 0.394165 \n",
            "Train Epoch: 194 [336/486 (69%)]\tlearningLoss: 0.008901\twhole_loss: 0.060252 \n",
            "Train Epoch: 194 [352/486 (72%)]\tlearningLoss: 0.009828\twhole_loss: 0.162093 \n",
            "Train Epoch: 194 [368/486 (76%)]\tlearningLoss: 0.009960\twhole_loss: 0.023196 \n",
            "Train Epoch: 194 [384/486 (79%)]\tlearningLoss: 0.010304\twhole_loss: 0.060189 \n",
            "Train Epoch: 194 [400/486 (82%)]\tlearningLoss: 0.010789\twhole_loss: 0.084937 \n",
            "Train Epoch: 194 [416/486 (86%)]\tlearningLoss: 0.010868\twhole_loss: 0.013706 \n",
            "Train Epoch: 194 [432/486 (89%)]\tlearningLoss: 0.010871\twhole_loss: 0.000480 \n",
            "Train Epoch: 194 [448/486 (92%)]\tlearningLoss: 0.011011\twhole_loss: 0.024588 \n",
            "Train Epoch: 194 [464/486 (95%)]\tlearningLoss: 0.011390\twhole_loss: 0.066270 \n",
            "Train Epoch: 194 [180/486 (37%)]\tlearningLoss: 0.016453\twhole_loss: 0.886115 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 2, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.5187, Accuracy: 68/75(91%)\n",
            "\n",
            "Train Epoch: 195 [0/486 (0%)]\tlearningLoss: 0.000085\twhole_loss: 0.014823 \n",
            "Train Epoch: 195 [16/486 (3%)]\tlearningLoss: 0.003987\twhole_loss: 0.682966 \n",
            "Train Epoch: 195 [32/486 (7%)]\tlearningLoss: 0.004854\twhole_loss: 0.151645 \n",
            "Train Epoch: 195 [48/486 (10%)]\tlearningLoss: 0.005411\twhole_loss: 0.097480 \n",
            "Train Epoch: 195 [64/486 (13%)]\tlearningLoss: 0.005426\twhole_loss: 0.002644 \n",
            "Train Epoch: 195 [80/486 (16%)]\tlearningLoss: 0.006026\twhole_loss: 0.105021 \n",
            "Train Epoch: 195 [96/486 (20%)]\tlearningLoss: 0.006316\twhole_loss: 0.050720 \n",
            "Train Epoch: 195 [112/486 (23%)]\tlearningLoss: 0.006591\twhole_loss: 0.048115 \n",
            "Train Epoch: 195 [128/486 (26%)]\tlearningLoss: 0.006664\twhole_loss: 0.012822 \n",
            "Train Epoch: 195 [144/486 (30%)]\tlearningLoss: 0.006779\twhole_loss: 0.020099 \n",
            "Train Epoch: 195 [160/486 (33%)]\tlearningLoss: 0.006852\twhole_loss: 0.012734 \n",
            "Train Epoch: 195 [176/486 (36%)]\tlearningLoss: 0.006934\twhole_loss: 0.014345 \n",
            "Train Epoch: 195 [192/486 (40%)]\tlearningLoss: 0.007059\twhole_loss: 0.021826 \n",
            "Train Epoch: 195 [208/486 (43%)]\tlearningLoss: 0.007121\twhole_loss: 0.010977 \n",
            "Train Epoch: 195 [224/486 (46%)]\tlearningLoss: 0.008050\twhole_loss: 0.162486 \n",
            "Train Epoch: 195 [240/486 (49%)]\tlearningLoss: 0.008164\twhole_loss: 0.020080 \n",
            "Train Epoch: 195 [256/486 (53%)]\tlearningLoss: 0.008867\twhole_loss: 0.122939 \n",
            "Train Epoch: 195 [272/486 (56%)]\tlearningLoss: 0.008916\twhole_loss: 0.008524 \n",
            "Train Epoch: 195 [288/486 (59%)]\tlearningLoss: 0.008921\twhole_loss: 0.000917 \n",
            "Train Epoch: 195 [304/486 (63%)]\tlearningLoss: 0.009204\twhole_loss: 0.049502 \n",
            "Train Epoch: 195 [320/486 (66%)]\tlearningLoss: 0.009264\twhole_loss: 0.010482 \n",
            "Train Epoch: 195 [336/486 (69%)]\tlearningLoss: 0.011275\twhole_loss: 0.352034 \n",
            "Train Epoch: 195 [352/486 (72%)]\tlearningLoss: 0.012336\twhole_loss: 0.185660 \n",
            "Train Epoch: 195 [368/486 (76%)]\tlearningLoss: 0.012455\twhole_loss: 0.020793 \n",
            "Train Epoch: 195 [384/486 (79%)]\tlearningLoss: 0.012719\twhole_loss: 0.046219 \n",
            "Train Epoch: 195 [400/486 (82%)]\tlearningLoss: 0.013567\twhole_loss: 0.148413 \n",
            "Train Epoch: 195 [416/486 (86%)]\tlearningLoss: 0.013715\twhole_loss: 0.025793 \n",
            "Train Epoch: 195 [432/486 (89%)]\tlearningLoss: 0.013916\twhole_loss: 0.035201 \n",
            "Train Epoch: 195 [448/486 (92%)]\tlearningLoss: 0.017290\twhole_loss: 0.590473 \n",
            "Train Epoch: 195 [464/486 (95%)]\tlearningLoss: 0.017910\twhole_loss: 0.108447 \n",
            "Train Epoch: 195 [180/486 (37%)]\tlearningLoss: 0.018069\twhole_loss: 0.027935 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 2, 1, 2, 2, 0, 2, 2, 2, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:6.6309, Accuracy: 61/75(81%)\n",
            "\n",
            "Train Epoch: 196 [0/486 (0%)]\tlearningLoss: 0.000015\twhole_loss: 0.002687 \n",
            "Train Epoch: 196 [16/486 (3%)]\tlearningLoss: 0.000043\twhole_loss: 0.004821 \n",
            "Train Epoch: 196 [32/486 (7%)]\tlearningLoss: 0.000134\twhole_loss: 0.015880 \n",
            "Train Epoch: 196 [48/486 (10%)]\tlearningLoss: 0.000999\twhole_loss: 0.151382 \n",
            "Train Epoch: 196 [64/486 (13%)]\tlearningLoss: 0.002523\twhole_loss: 0.266811 \n",
            "Train Epoch: 196 [80/486 (16%)]\tlearningLoss: 0.003092\twhole_loss: 0.099488 \n",
            "Train Epoch: 196 [96/486 (20%)]\tlearningLoss: 0.006576\twhole_loss: 0.609702 \n",
            "Train Epoch: 196 [112/486 (23%)]\tlearningLoss: 0.008343\twhole_loss: 0.309239 \n",
            "Train Epoch: 196 [128/486 (26%)]\tlearningLoss: 0.008844\twhole_loss: 0.087645 \n",
            "Train Epoch: 196 [144/486 (30%)]\tlearningLoss: 0.009012\twhole_loss: 0.029416 \n",
            "Train Epoch: 196 [160/486 (33%)]\tlearningLoss: 0.009372\twhole_loss: 0.063105 \n",
            "Train Epoch: 196 [176/486 (36%)]\tlearningLoss: 0.010081\twhole_loss: 0.123919 \n",
            "Train Epoch: 196 [192/486 (40%)]\tlearningLoss: 0.011527\twhole_loss: 0.253200 \n",
            "Train Epoch: 196 [208/486 (43%)]\tlearningLoss: 0.013669\twhole_loss: 0.374769 \n",
            "Train Epoch: 196 [224/486 (46%)]\tlearningLoss: 0.013768\twhole_loss: 0.017358 \n",
            "Train Epoch: 196 [240/486 (49%)]\tlearningLoss: 0.015290\twhole_loss: 0.266319 \n",
            "Train Epoch: 196 [256/486 (53%)]\tlearningLoss: 0.016510\twhole_loss: 0.213545 \n",
            "Train Epoch: 196 [272/486 (56%)]\tlearningLoss: 0.017853\twhole_loss: 0.234939 \n",
            "Train Epoch: 196 [288/486 (59%)]\tlearningLoss: 0.020811\twhole_loss: 0.517682 \n",
            "Train Epoch: 196 [304/486 (63%)]\tlearningLoss: 0.022366\twhole_loss: 0.272221 \n",
            "Train Epoch: 196 [320/486 (66%)]\tlearningLoss: 0.023309\twhole_loss: 0.164887 \n",
            "Train Epoch: 196 [336/486 (69%)]\tlearningLoss: 0.024038\twhole_loss: 0.127643 \n",
            "Train Epoch: 196 [352/486 (72%)]\tlearningLoss: 0.024091\twhole_loss: 0.009213 \n",
            "Train Epoch: 196 [368/486 (76%)]\tlearningLoss: 0.025728\twhole_loss: 0.286575 \n",
            "Train Epoch: 196 [384/486 (79%)]\tlearningLoss: 0.028427\twhole_loss: 0.472355 \n",
            "Train Epoch: 196 [400/486 (82%)]\tlearningLoss: 0.028963\twhole_loss: 0.093775 \n",
            "Train Epoch: 196 [416/486 (86%)]\tlearningLoss: 0.029110\twhole_loss: 0.025626 \n",
            "Train Epoch: 196 [432/486 (89%)]\tlearningLoss: 0.029180\twhole_loss: 0.012242 \n",
            "Train Epoch: 196 [448/486 (92%)]\tlearningLoss: 0.029280\twhole_loss: 0.017610 \n",
            "Train Epoch: 196 [464/486 (95%)]\tlearningLoss: 0.033634\twhole_loss: 0.761858 \n",
            "Train Epoch: 196 [180/486 (37%)]\tlearningLoss: 0.035523\twhole_loss: 0.330676 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 2, 2, 2, 1, 1, 2, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.2466, Accuracy: 63/75(84%)\n",
            "\n",
            "Train Epoch: 197 [0/486 (0%)]\tlearningLoss: 0.000413\twhole_loss: 0.072260 \n",
            "Train Epoch: 197 [16/486 (3%)]\tlearningLoss: 0.000690\twhole_loss: 0.048460 \n",
            "Train Epoch: 197 [32/486 (7%)]\tlearningLoss: 0.003830\twhole_loss: 0.549537 \n",
            "Train Epoch: 197 [48/486 (10%)]\tlearningLoss: 0.003997\twhole_loss: 0.029263 \n",
            "Train Epoch: 197 [64/486 (13%)]\tlearningLoss: 0.005189\twhole_loss: 0.208571 \n",
            "Train Epoch: 197 [80/486 (16%)]\tlearningLoss: 0.007586\twhole_loss: 0.419373 \n",
            "Train Epoch: 197 [96/486 (20%)]\tlearningLoss: 0.009752\twhole_loss: 0.379095 \n",
            "Train Epoch: 197 [112/486 (23%)]\tlearningLoss: 0.009792\twhole_loss: 0.007041 \n",
            "Train Epoch: 197 [128/486 (26%)]\tlearningLoss: 0.012493\twhole_loss: 0.472718 \n",
            "Train Epoch: 197 [144/486 (30%)]\tlearningLoss: 0.012732\twhole_loss: 0.041767 \n",
            "Train Epoch: 197 [160/486 (33%)]\tlearningLoss: 0.018343\twhole_loss: 0.981904 \n",
            "Train Epoch: 197 [176/486 (36%)]\tlearningLoss: 0.018353\twhole_loss: 0.001722 \n",
            "Train Epoch: 197 [192/486 (40%)]\tlearningLoss: 0.018428\twhole_loss: 0.013121 \n",
            "Train Epoch: 197 [208/486 (43%)]\tlearningLoss: 0.019905\twhole_loss: 0.258491 \n",
            "Train Epoch: 197 [224/486 (46%)]\tlearningLoss: 0.020501\twhole_loss: 0.104422 \n",
            "Train Epoch: 197 [240/486 (49%)]\tlearningLoss: 0.020806\twhole_loss: 0.053275 \n",
            "Train Epoch: 197 [256/486 (53%)]\tlearningLoss: 0.021351\twhole_loss: 0.095366 \n",
            "Train Epoch: 197 [272/486 (56%)]\tlearningLoss: 0.025905\twhole_loss: 0.797064 \n",
            "Train Epoch: 197 [288/486 (59%)]\tlearningLoss: 0.027923\twhole_loss: 0.353032 \n",
            "Train Epoch: 197 [304/486 (63%)]\tlearningLoss: 0.028401\twhole_loss: 0.083710 \n",
            "Train Epoch: 197 [320/486 (66%)]\tlearningLoss: 0.029679\twhole_loss: 0.223555 \n",
            "Train Epoch: 197 [336/486 (69%)]\tlearningLoss: 0.029726\twhole_loss: 0.008360 \n",
            "Train Epoch: 197 [352/486 (72%)]\tlearningLoss: 0.030174\twhole_loss: 0.078387 \n",
            "Train Epoch: 197 [368/486 (76%)]\tlearningLoss: 0.030721\twhole_loss: 0.095650 \n",
            "Train Epoch: 197 [384/486 (79%)]\tlearningLoss: 0.031196\twhole_loss: 0.083108 \n",
            "Train Epoch: 197 [400/486 (82%)]\tlearningLoss: 0.031461\twhole_loss: 0.046436 \n",
            "Train Epoch: 197 [416/486 (86%)]\tlearningLoss: 0.031570\twhole_loss: 0.019043 \n",
            "Train Epoch: 197 [432/486 (89%)]\tlearningLoss: 0.031654\twhole_loss: 0.014753 \n",
            "Train Epoch: 197 [448/486 (92%)]\tlearningLoss: 0.031908\twhole_loss: 0.044430 \n",
            "Train Epoch: 197 [464/486 (95%)]\tlearningLoss: 0.032300\twhole_loss: 0.068627 \n",
            "Train Epoch: 197 [180/486 (37%)]\tlearningLoss: 0.033928\twhole_loss: 0.284827 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 2, 1, 1, 1, 2, 2, 0, 2, 2, 1, 1, 2, 2, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 0, 2, 0, 0, 0, 2, 2, 0, 2], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.9899, Accuracy: 60/75(80%)\n",
            "\n",
            "Train Epoch: 198 [0/486 (0%)]\tlearningLoss: 0.000297\twhole_loss: 0.051961 \n",
            "Train Epoch: 198 [16/486 (3%)]\tlearningLoss: 0.003420\twhole_loss: 0.546511 \n",
            "Train Epoch: 198 [32/486 (7%)]\tlearningLoss: 0.003441\twhole_loss: 0.003709 \n",
            "Train Epoch: 198 [48/486 (10%)]\tlearningLoss: 0.003709\twhole_loss: 0.046903 \n",
            "Train Epoch: 198 [64/486 (13%)]\tlearningLoss: 0.003996\twhole_loss: 0.050156 \n",
            "Train Epoch: 198 [80/486 (16%)]\tlearningLoss: 0.004256\twhole_loss: 0.045596 \n",
            "Train Epoch: 198 [96/486 (20%)]\tlearningLoss: 0.005084\twhole_loss: 0.144791 \n",
            "Train Epoch: 198 [112/486 (23%)]\tlearningLoss: 0.005344\twhole_loss: 0.045659 \n",
            "Train Epoch: 198 [128/486 (26%)]\tlearningLoss: 0.007615\twhole_loss: 0.397264 \n",
            "Train Epoch: 198 [144/486 (30%)]\tlearningLoss: 0.007870\twhole_loss: 0.044614 \n",
            "Train Epoch: 198 [160/486 (33%)]\tlearningLoss: 0.008844\twhole_loss: 0.170541 \n",
            "Train Epoch: 198 [176/486 (36%)]\tlearningLoss: 0.008926\twhole_loss: 0.014417 \n",
            "Train Epoch: 198 [192/486 (40%)]\tlearningLoss: 0.008964\twhole_loss: 0.006634 \n",
            "Train Epoch: 198 [208/486 (43%)]\tlearningLoss: 0.008974\twhole_loss: 0.001626 \n",
            "Train Epoch: 198 [224/486 (46%)]\tlearningLoss: 0.009526\twhole_loss: 0.096684 \n",
            "Train Epoch: 198 [240/486 (49%)]\tlearningLoss: 0.009554\twhole_loss: 0.004878 \n",
            "Train Epoch: 198 [256/486 (53%)]\tlearningLoss: 0.009684\twhole_loss: 0.022808 \n",
            "Train Epoch: 198 [272/486 (56%)]\tlearningLoss: 0.011425\twhole_loss: 0.304595 \n",
            "Train Epoch: 198 [288/486 (59%)]\tlearningLoss: 0.011692\twhole_loss: 0.046837 \n",
            "Train Epoch: 198 [304/486 (63%)]\tlearningLoss: 0.012140\twhole_loss: 0.078273 \n",
            "Train Epoch: 198 [320/486 (66%)]\tlearningLoss: 0.012178\twhole_loss: 0.006715 \n",
            "Train Epoch: 198 [336/486 (69%)]\tlearningLoss: 0.012195\twhole_loss: 0.002964 \n",
            "Train Epoch: 198 [352/486 (72%)]\tlearningLoss: 0.012215\twhole_loss: 0.003466 \n",
            "Train Epoch: 198 [368/486 (76%)]\tlearningLoss: 0.012519\twhole_loss: 0.053294 \n",
            "Train Epoch: 198 [384/486 (79%)]\tlearningLoss: 0.012862\twhole_loss: 0.059988 \n",
            "Train Epoch: 198 [400/486 (82%)]\tlearningLoss: 0.013061\twhole_loss: 0.034856 \n",
            "Train Epoch: 198 [416/486 (86%)]\tlearningLoss: 0.013144\twhole_loss: 0.014534 \n",
            "Train Epoch: 198 [432/486 (89%)]\tlearningLoss: 0.013636\twhole_loss: 0.085970 \n",
            "Train Epoch: 198 [448/486 (92%)]\tlearningLoss: 0.014096\twhole_loss: 0.080574 \n",
            "Train Epoch: 198 [464/486 (95%)]\tlearningLoss: 0.014591\twhole_loss: 0.086580 \n",
            "Train Epoch: 198 [180/486 (37%)]\tlearningLoss: 0.015206\twhole_loss: 0.107674 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 1, 1, 1, 1, 1, 1, 2, 0, 2, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:2.9780, Accuracy: 68/75(91%)\n",
            "\n",
            "Train Epoch: 199 [0/486 (0%)]\tlearningLoss: 0.000325\twhole_loss: 0.056932 \n",
            "Train Epoch: 199 [16/486 (3%)]\tlearningLoss: 0.000909\twhole_loss: 0.102110 \n",
            "Train Epoch: 199 [32/486 (7%)]\tlearningLoss: 0.001099\twhole_loss: 0.033256 \n",
            "Train Epoch: 199 [48/486 (10%)]\tlearningLoss: 0.001230\twhole_loss: 0.023016 \n",
            "Train Epoch: 199 [64/486 (13%)]\tlearningLoss: 0.001232\twhole_loss: 0.000327 \n",
            "Train Epoch: 199 [80/486 (16%)]\tlearningLoss: 0.001304\twhole_loss: 0.012586 \n",
            "Train Epoch: 199 [96/486 (20%)]\tlearningLoss: 0.002493\twhole_loss: 0.208080 \n",
            "Train Epoch: 199 [112/486 (23%)]\tlearningLoss: 0.002589\twhole_loss: 0.016771 \n",
            "Train Epoch: 199 [128/486 (26%)]\tlearningLoss: 0.002635\twhole_loss: 0.008111 \n",
            "Train Epoch: 199 [144/486 (30%)]\tlearningLoss: 0.002725\twhole_loss: 0.015770 \n",
            "Train Epoch: 199 [160/486 (33%)]\tlearningLoss: 0.002760\twhole_loss: 0.006115 \n",
            "Train Epoch: 199 [176/486 (36%)]\tlearningLoss: 0.002779\twhole_loss: 0.003176 \n",
            "Train Epoch: 199 [192/486 (40%)]\tlearningLoss: 0.002849\twhole_loss: 0.012274 \n",
            "Train Epoch: 199 [208/486 (43%)]\tlearningLoss: 0.002927\twhole_loss: 0.013788 \n",
            "Train Epoch: 199 [224/486 (46%)]\tlearningLoss: 0.003166\twhole_loss: 0.041684 \n",
            "Train Epoch: 199 [240/486 (49%)]\tlearningLoss: 0.003191\twhole_loss: 0.004357 \n",
            "Train Epoch: 199 [256/486 (53%)]\tlearningLoss: 0.003606\twhole_loss: 0.072622 \n",
            "Train Epoch: 199 [272/486 (56%)]\tlearningLoss: 0.003895\twhole_loss: 0.050633 \n",
            "Train Epoch: 199 [288/486 (59%)]\tlearningLoss: 0.003934\twhole_loss: 0.006758 \n",
            "Train Epoch: 199 [304/486 (63%)]\tlearningLoss: 0.004077\twhole_loss: 0.025157 \n",
            "Train Epoch: 199 [320/486 (66%)]\tlearningLoss: 0.004122\twhole_loss: 0.007827 \n",
            "Train Epoch: 199 [336/486 (69%)]\tlearningLoss: 0.004398\twhole_loss: 0.048337 \n",
            "Train Epoch: 199 [352/486 (72%)]\tlearningLoss: 0.004472\twhole_loss: 0.012968 \n",
            "Train Epoch: 199 [368/486 (76%)]\tlearningLoss: 0.004572\twhole_loss: 0.017387 \n",
            "Train Epoch: 199 [384/486 (79%)]\tlearningLoss: 0.005043\twhole_loss: 0.082562 \n",
            "Train Epoch: 199 [400/486 (82%)]\tlearningLoss: 0.005097\twhole_loss: 0.009446 \n",
            "Train Epoch: 199 [416/486 (86%)]\tlearningLoss: 0.005107\twhole_loss: 0.001692 \n",
            "Train Epoch: 199 [432/486 (89%)]\tlearningLoss: 0.005108\twhole_loss: 0.000192 \n",
            "Train Epoch: 199 [448/486 (92%)]\tlearningLoss: 0.005376\twhole_loss: 0.046899 \n",
            "Train Epoch: 199 [464/486 (95%)]\tlearningLoss: 0.005454\twhole_loss: 0.013658 \n",
            "Train Epoch: 199 [180/486 (37%)]\tlearningLoss: 0.005455\twhole_loss: 0.000219 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 2, 1, 1, 1, 1, 2, 0, 2, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:4.3028, Accuracy: 63/75(84%)\n",
            "\n",
            "Train Epoch: 200 [0/486 (0%)]\tlearningLoss: 0.000763\twhole_loss: 0.133483 \n",
            "Train Epoch: 200 [16/486 (3%)]\tlearningLoss: 0.000801\twhole_loss: 0.006773 \n",
            "Train Epoch: 200 [32/486 (7%)]\tlearningLoss: 0.000873\twhole_loss: 0.012526 \n",
            "Train Epoch: 200 [48/486 (10%)]\tlearningLoss: 0.001011\twhole_loss: 0.024061 \n",
            "Train Epoch: 200 [64/486 (13%)]\tlearningLoss: 0.002457\twhole_loss: 0.253049 \n",
            "Train Epoch: 200 [80/486 (16%)]\tlearningLoss: 0.002613\twhole_loss: 0.027449 \n",
            "Train Epoch: 200 [96/486 (20%)]\tlearningLoss: 0.002686\twhole_loss: 0.012726 \n",
            "Train Epoch: 200 [112/486 (23%)]\tlearningLoss: 0.002786\twhole_loss: 0.017505 \n",
            "Train Epoch: 200 [128/486 (26%)]\tlearningLoss: 0.002829\twhole_loss: 0.007501 \n",
            "Train Epoch: 200 [144/486 (30%)]\tlearningLoss: 0.002848\twhole_loss: 0.003252 \n",
            "Train Epoch: 200 [160/486 (33%)]\tlearningLoss: 0.002934\twhole_loss: 0.015126 \n",
            "Train Epoch: 200 [176/486 (36%)]\tlearningLoss: 0.003210\twhole_loss: 0.048338 \n",
            "Train Epoch: 200 [192/486 (40%)]\tlearningLoss: 0.003287\twhole_loss: 0.013483 \n",
            "Train Epoch: 200 [208/486 (43%)]\tlearningLoss: 0.003300\twhole_loss: 0.002248 \n",
            "Train Epoch: 200 [224/486 (46%)]\tlearningLoss: 0.003314\twhole_loss: 0.002422 \n",
            "Train Epoch: 200 [240/486 (49%)]\tlearningLoss: 0.003493\twhole_loss: 0.031286 \n",
            "Train Epoch: 200 [256/486 (53%)]\tlearningLoss: 0.003498\twhole_loss: 0.000895 \n",
            "Train Epoch: 200 [272/486 (56%)]\tlearningLoss: 0.003521\twhole_loss: 0.004093 \n",
            "Train Epoch: 200 [288/486 (59%)]\tlearningLoss: 0.003550\twhole_loss: 0.005012 \n",
            "Train Epoch: 200 [304/486 (63%)]\tlearningLoss: 0.003880\twhole_loss: 0.057697 \n",
            "Train Epoch: 200 [320/486 (66%)]\tlearningLoss: 0.003892\twhole_loss: 0.002214 \n",
            "Train Epoch: 200 [336/486 (69%)]\tlearningLoss: 0.003903\twhole_loss: 0.001817 \n",
            "Train Epoch: 200 [352/486 (72%)]\tlearningLoss: 0.004031\twhole_loss: 0.022489 \n",
            "Train Epoch: 200 [368/486 (76%)]\tlearningLoss: 0.004112\twhole_loss: 0.014115 \n",
            "Train Epoch: 200 [384/486 (79%)]\tlearningLoss: 0.004309\twhole_loss: 0.034573 \n",
            "Train Epoch: 200 [400/486 (82%)]\tlearningLoss: 0.004328\twhole_loss: 0.003248 \n",
            "Train Epoch: 200 [416/486 (86%)]\tlearningLoss: 0.004410\twhole_loss: 0.014356 \n",
            "Train Epoch: 200 [432/486 (89%)]\tlearningLoss: 0.004737\twhole_loss: 0.057213 \n",
            "Train Epoch: 200 [448/486 (92%)]\tlearningLoss: 0.004825\twhole_loss: 0.015490 \n",
            "Train Epoch: 200 [464/486 (95%)]\tlearningLoss: 0.005091\twhole_loss: 0.046524 \n",
            "Train Epoch: 200 [180/486 (37%)]\tlearningLoss: 0.005108\twhole_loss: 0.002995 \n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Predicted:  tensor([1, 0, 1, 1, 1, 1, 1, 2, 0, 2, 2, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "Real:  tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "Real:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "Real:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "Predicted:  tensor([0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Test set : Average loss:3.1014, Accuracy: 67/75(89%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def saveModel():\n",
        "    path = \"./myFirstModel_0411.pth\"\n",
        "    torch.save(model, path)"
      ],
      "metadata": {
        "id": "4KtDr1S7a3Hp"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save_model(model, \"/content/drive/MyDrive/민화 AI인턴/민화 AI인턴 (2)/220410\")\n",
        "saveModel()"
      ],
      "metadata": {
        "id": "J1JwCCzp3XNT"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# # Function to show the images\n",
        "# def imageshow(img):\n",
        "#     img = img / 2 + 0.5     # unnormalize\n",
        "#     npimg = img.numpy()\n",
        "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "# # Function to test the model with a batch of images and show the labels predictions\n",
        "# def testBatch():\n",
        "#     # get batch of images from the test DataLoader  \n",
        "#     images, labels = next(iter(testloader))\n",
        "\n",
        "#     # show all images as one image grid\n",
        "#     imageshow(torchvision.utils.make_grid(images))\n",
        "   \n",
        "#     # Show the real labels on the screen \n",
        "#     print('Real labels: ', ' '.join('%5s' % classes[labels[j]] \n",
        "#                                for j in range(batch_size)))\n",
        "  \n",
        "#     # Let's see what if the model identifiers the  labels of those example\n",
        "#     outputs = model(images)\n",
        "    \n",
        "#     # We got the probability for every 10 labels. The highest (max) probability should be correct label\n",
        "#     _, predicted = torch.max(outputs, 1)\n",
        "    \n",
        "#     # Let's show the predicted labels on the screen to compare with the real ones\n",
        "#     print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] \n",
        "#                               for j in range(batch_size)))"
      ],
      "metadata": {
        "id": "HCFKsiHq5K2H"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "테스트 결과 확인하기"
      ],
      "metadata": {
        "id": "vzjqigSH0Vua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# # Function to show the images\n",
        "# def imageshow(img):\n",
        "#     img = img / 2 + 0.5     # unnormalize\n",
        "#     npimg = img.numpy()\n",
        "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.show()\n",
        "\n",
        "# def testBatch():\n",
        "#   images, labels = next(iter(testloader))\n",
        "#   imageshow(torchvision.utils.make_grid(images))\n",
        "#   print('Real labels: ', ' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))\n"
      ],
      "metadata": {
        "id": "C6phqL-R0XjV"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 텐서보드에 학습데이터 올리기"
      ],
      "metadata": {
        "id": "9TdspUU6w27w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# albumentations_train_loader = torch.utils.data.DataLoader(trainset, batch_size=16,\n",
        "#                                           shuffle=True, num_workers=0)\n",
        "\n",
        "# albumentations_test_loader = torch.utils.data.DataLoader(testset, batch_size=16,\n",
        "#                                          shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "sTb8H7usyI7J"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 타임스탬프\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "# from tensorflow import summary # 텐서보드에 기록하기 위한 함수\n",
        "# import datetime\n",
        "\n",
        "# time_stamp = str(datetime.datetime.now().timestamp())\n",
        "# log_dir = 'runs/resnet' + time_stamp # runs 경로\n",
        "# writer = SummaryWriter(log_dir)\n",
        "\n",
        "# # 텐보 로드\n",
        "# %load_ext tensorboard"
      ],
      "metadata": {
        "id": "Yc8w4ckNytJU"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 데이터 이름을 파라미터로 부여\n",
        "# dataset = 'train_loader'\n",
        "# model_parameter = str(dataset+'Adam_batch_size:{:.0f}, lr:{:.5f}'.format(albumentations_train_loader.batch_size, lr))\n",
        "\n",
        "# # 텐서보드에 올릴 데이터 디렉토리 경로\n",
        "# # train_log_dir = 'logs/tensorboard/train_'+model_parameter   # colab 런타임을 종료하면 없어지기 때문에 아래 경로로 로컬에 저장해놓음\n",
        "# # test_log_dir = 'logs/tensorboard/test_'+model_parameter\n",
        "\n",
        "# train_log_dir = '/content/drive/MyDrive/민화 AI인턴/민화 AI인턴 (2)/220403/ImageData/train'+model_parameter\n",
        "# test_log_dir = '/content/drive/MyDrive/민화 AI인턴/민화 AI인턴 (2)/220403/ImageData/test'+model_parameter\n",
        "\n",
        "# # 위 경로로 데이터를 텐보에 올릴 파일을 만듦\n",
        "# train_summary_writer = summary.create_file_writer(train_log_dir)\n",
        "# test_summary_writer =  summary.create_file_writer(test_log_dir)"
      ],
      "metadata": {
        "id": "1CnLLKAAqDCW"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorboardX\n",
        "# %tensorboard --logdir logs/tensorboard"
      ],
      "metadata": {
        "id": "qkOK2MvBqVXi"
      },
      "execution_count": 43,
      "outputs": []
    }
  ]
}